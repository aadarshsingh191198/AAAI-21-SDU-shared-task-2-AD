acronym_,expansion,id,text
SP,strictly piecewise,TR-30,table*[t]Accuracy on Target SP Stringsets Early Stoppingtab : resultsSPES4.5pttabularcccccccccc2c2Training & 2Test & 3cLSTM & 3cs - RNN & 2RPNI & & & 10 & 30 & 100 & 10 & 30 & 100 & 6SP2 & 21k & 1 & 0.871 ( 0.04 ) & 0.954 ( 0.05 ) & 0.992 ( 0.00 ) & 0.910 ( 0.05 ) & 0.994 ( 0.01 ) & 0.992 ( 0.01 ) & 1.000 & & 2 & 0.960 ( 0.03 ) & 0.989 ( 0.02 ) & 0.998 ( 0.00 ) & 0.976 ( 0.01 ) & 0.998 ( 0.01 ) & 1.000 ( 0.00 ) & 1.000 3 - 10 & 210k & 1 & 0.890 ( 0.07 ) & 0.941 ( 0.04 ) & 0.977 ( 0.02 ) & 0.995 ( 0.01 ) & 0.981 ( 0.05 ) & 0.999 ( 0.00 ) & 1.000 & & 2 & 0.979 ( 0.02 ) & 0.990 ( 0.01 ) & 0.994 ( 0.01 ) & 1.000 ( 0.00 ) & 0.984 ( 0.05 ) & 1.000 ( 0.00 ) & 1.000 3 - 10 & 2100k & 1 & 0.833 ( 0.14 ) & 0.819 ( 0.12 ) & 0.890 ( 0.08 ) & 0.997 ( 0.01 ) & 0.999 ( 0.00 ) & 0.997 ( 0.00 ) & 1.000 & & 2 & 0.838 ( 0.16 ) & 0.805 ( 0.13 ) & 0.872 ( 0.09 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 6SP4 & 21k & 1 & 0.881 ( 0.06 ) & 0.946 ( 0.04 ) & 0.963 ( 0.03 ) & 0.887 ( 0.05 ) & 0.966 ( 0.02 ) & 0.979 ( 0.01 ) & 1.000 & & 2 & 0.950 ( 0.03 ) & 0.960 ( 0.03 ) & 0.983 ( 0.01 ) & 0.883 ( 0.05 ) & 0.975 ( 0.01 ) & 0.979 ( 0.01 ) & 1.000 3 - 10 & 210k & 1 & 0.899 ( 0.11 ) & 0.958 ( 0.07 ) & 0.991 ( 0.01 ) & 0.935 ( 0.08 ) & 0.968 ( 0.04 ) & 0.999 ( 0.00 ) & 1.000 & & 2 & 0.926 ( 0.09 ) & 0.971 ( 0.05 ) & 0.991 ( 0.01 ) & 0.954 ( 0.07 ) & 0.984 ( 0.02 ) & 1.000 ( 0.00 ) & 1.000 3 - 10 & 2100k & 1 & 0.943 ( 0.08 ) & 0.940 ( 0.08 ) & 0.920 ( 0.06 ) & 0.942 ( 0.09 ) & 0.958 ( 0.09 ) & 0.973 ( 0.07 ) & 1.000 & & 2 & 0.928 ( 0.08 ) & 0.930 ( 0.09 ) & 0.911 ( 0.07 ) & 0.951 ( 0.09 ) & 0.962 ( 0.08 ) & 0.974 ( 0.08 ) & 1.000 6SP8 & 21k & 1 & 0.884 ( 0.02 ) & 0.884 ( 0.02 ) & 0.903 ( 0.02 ) & 0.861 ( 0.01 ) & 0.878 ( 0.02 ) & 0.857 ( 0.02 ) & 0.817 & & 2 & 0.733 ( 0.03 ) & 0.643 ( 0.06 ) & 0.688 ( 0.04 ) & 0.730 ( 0.01 ) & 0.681 ( 0.06 ) & 0.625 ( 0.04 ) & 0.587 3 - 10 & 210k & 1 & 0.934 ( 0.05 ) & 0.921 ( 0.05 ) & 0.959 ( 0.03 ) & 0.908 ( 0.02 ) & 0.952 ( 0.03 ) & 0.991 ( 0.00 ) & 0.873 & & 2 & 0.637 ( 0.08 ) & 0.659 ( 0.10 ) & 0.704 ( 0.11 ) & 0.600 ( 0.08 ) & 0.640 ( 0.10 ) & 0.837 ( 0.05 ) & 0.634 3 - 10 & 2100k & 1 & 0.977 ( 0.04 ) & 0.975 ( 0.04 ) & 0.980 ( 0.02 ) & 0.964 ( 0.05 ) & 0.990 ( 0.03 ) & 1.000 ( 0.00 ) & 1.000 & & 2 & 0.881 ( 0.11 ) & 0.865 ( 0.13 ) & 0.864 ( 0.08 ) & 0.890 ( 0.08 ) & 0.942 ( 0.09 ) & 0.984 ( 0.03 ) & 1.000 tabulartable *
CS,charging station,TR-48,"Optimization problemWe consider utility functions that are a weighted sum of three aspects : the charging time , which includes the travel time to the CS , the possible time spent queuing and the effective time for charging ; the charging price , where we use the amount of energy generated from renewable sources as a proxy for the discount in the price of charging ; and finally , the distance from the CS , used as a proxy for battery discharge ( i.e. , if the battery level is very low , one may just want to choose the nearest CS ) ."
MD,mean diffusivity,TR-79,tabularp0.8cmp5.5cmp6 cm 3cAmyloid Load ( PiB Positivity ) Set 1 & PiB Angular L / R & PiB Cingulum Ant L / R & PiB Cingulum Post L / R & PiB Frontal Med Orb L / R & PiB Precuneus L / R & PiB Temporal Sup L / R & PiB Temporal Mid L / R & PiB SupraMarginal L Set 2 & FA Cerebral peduncle R & FA Cerebral peduncle L & MD Corticospinal tract R & MD Corticospinal tract L & Trail - Making Test Part A Score & MD Cerebral peduncle R & PET Cingulum Post R & tabular Group difference across Amyloid Load ( PiB Positivity)tabletable [ ! ]
STL,single task learning,TR-257,tab : schooltabularllllllllModels & STL & ITL & SHAMO & MTFL & GO - MTL & BiFactor & TriFactor 20 & 12.19 ( 0.03 ) & 12.00 ( 0.04 ) & 11.91 ( 0.05 ) & 11.25 ( 0.05 ) & 11.15 ( 0.05 ) & 10.68 ( 0.08 ) & 10.54 ( 0.09 ) 30 & 12.09 ( 0.07 ) & 12.01 ( 0.05 ) & 10.92 ( 0.05 ) & 10.85 ( 0.02 ) & 10.53 ( 0.10 ) & 10.38 ( 0.11 ) & 10.22 ( 0.08 ) 40 & 12.00 ( 0.10 ) & 11.88 ( 0.06 ) & 11.82 ( 0.06 ) & 10.61 ( 0.06 ) & 10.31 ( 0.14 ) & 10.20 ( 0.13 ) & 10.12 ( 0.10 ) tabulartable * We compare the proposed methods BiFactor MTL and TriFactor MTL against the baselines .
SO,smart object,TR-558,"& Consumer & Smart dimensions & SO 1 & SO 2 & SO & Object & & & & network Varying & & & & complexity & & & & of PP & & & & Varying & & & & query & & & & * complexity & & & & Varying & & & & number of & & & & sensing SOs & & & & tabular Experiments : - time overhead , * time and bandwidth overhead tab : expTabletable Experimental resultsIn executing our experiments , we considered three main characteristics that may impact the performance of the proposed solution ."
PG,property graph,TR-600,"However , there are also some differences between them : An RDF graph contains nodes of type resource ( whose label is an IRI ) and nodes of type Literal ( whose label is a value ) , whereas a PG allows a single type of node ; Each node or edge in an RDF graph contains just a single value ( i.e. a label ) , whereas each node or edge in a PG could contain multiple labels and properties respectively;An RDF graph supports multi - value properties , whereas a PG usually just support mono - value properties;An RDF graph allows to have edges between edges , a feature which is n't supported in a PG ( by definition);A node in an RDF graph could be associated with zero or more classes or resources , while a node in a PG usually has a single node type ."
LTE,long term evolution,TR-627,"theoremTheoremlemma[theorem]Lemmadefinition[theorem]DefinitionpropositionPropositioncorollary[theorem]CorollaryremarkRemark*noteNote1 * argmax*argmin op - tical net - works semi - conduc - torPioneering Studies on LTE eMBMS : Towards 5 G Point - to - Multipoint TransmissionsHongzhi Chen1 , De Mi1 , Manuel Fuentes2 , David Vargas 3 , Eduardo Garro4 , Jose Luis Carcel4 , Belkacem Mouhouche2 , Pei Xiao1 and Rahim Tafazolli1 1Institute for Communication Systems , University of Surrey , United Kingdom 2Samsung Electronics RD UK , United Kingdom 3BBC RD , United Kingdom 4Institute of Telecommunications and Multimedia Applications , Universitat Politecnica de Valencia , Spain Email : hongzhi.chen , d.mi , p.xiao , r.tafazolli@surrey.ac.uk , m.fuentes , b.mouhouche@samsung.com , david.vargas@bbc.co.uk , edgarcre , jocarcer@iteam.upv.esThe first 5 G ( 5th generation wireless systems ) New Radio Release-15 was recently completed ."
CT,class table,TR-858,"figure[htbp]flalign*&addExt(class C extends D , CT)= ( C extends D)CT & removeExt(class C extends D , CR ) = CR ' _ S & where CR'= aligned[t]&(T.extends : T',cond ( T C ) ) ( T.extends : T ' , cond ) CR & ( T.m : TT ' , cond(TC ) ) & ( D.m : T T ' , cond ( T = C))(T.m : TT ' , cond ) CR & ( T.m : TT ' , cond ( TC))_opt & ( D.m : T T ' , cond ( T = C))_opt & ( T.m : TT ' , cond)_opt CR & ( T.f : T ' , cond ( TC ) ) ( D.f : T ' , cond ( T = C ) ) & ( T.f : T ' , cond ) CR & S = ( T ' = D if T= C ) ( T.extends : T ' , ) CR alignedflalign*flalign*&addCtor(C , ( D g , C f ) , CT ) = ( CD ; C)CT & removeCtor(C,(D g , C f ) , CR ) = CR'_S & where CR ' = ( TT),cond ( T C ) ) ( TT ) , ) CR & ( CR(TT ) , ) ) & S = ( T = D C if T = C ) ( TT ) , ) CR flalign*flalign*&addFs(C , C_f f , CT ) = C.f : C_f CT & removeF(C , C_f f , CR ) = CR'_S & where CR ' = ( T.f : T',cond ( T C ) ) ( T.f : T ' , ) CR & ( CR(T.f : T ' , ) ) & S = ( T ' = C_f if T= C ) ( T.f : T ' , ) CR & removeFs(C , C_f f , CR ) = CR'_S & where CR ' = CR_f ( C_f f ) C_f fremoveF(CR , C , C_f f ) = CR_f_S_f & S = S_f ( C_f f ) C_f f removeF(CR , C , C_f f ) = CR_f_S_fflalign*flalign * & addMs(C , M , CT ) = C.m : C C ' CT & removeM ( C , C ' m(C x ) return e , CR ) = CR'_S & where CR ' = ( T.m : TT',cond ( T C ) ) ( T.m : TT ' , ) CR & ( CR(T.m : TT ' , ) ) & S = ( T ' = C ' if T= C ) ( T = C if T= C ) ( T.m : TT ' , ) CR flalign*figurefigure[htbp]flalign*&removeMs(C , M , CR ) = CR'_S & where CR ' = CR_m ( C ' m(C x ) return e ) M & removeM(C , C ' m(C e ) return e , CR ) = CR_m_S_m & S = S_m ( C ' m(C x ) return e ) M & removeM(C , C ' m(C x ) return e , CR ) = CR_m_S_m & removeOptM(C , C ' m(C x ) return e , CR ) = CR'_S & where CR ' = ( T.m : TT',cond ( T C))_opt ( T.m : TT ' , ) _ opt CR & ( CR(T.m : TT ' , _ opt ) ) & S = ( T ' = C ' if T= C ) ( T = C if T= C ) ( T.m : TT ' , ) _ opt CR & removeOptMs(C , M , CR ) = ( CR ' ( CRCR'))_S & where CR ' = CR_m ( C ' m(C x ) return e ) M & removeOptM(CR , C , C ' m(C e ) return e ) = CR_m_S_m & S = S_m ( C ' m(C x ) return e ) M & removeOptM(CR , C , C ' m(C x ) return e ) = CR_m_S_m flalign*figure Equivalence of Contextual and Co - Contextual FJ sec : appendixB In here we describe a detailed proof of typing equivalence between FJ and co - contextual FJ ."
FEC,forward error correction,TR-931,"Fn algorithm[!htb ] RuleBlock * block = new RuleBlock ( ) ; block addRule ( new MamdaniRule ( "" if ( SpatialComplexity is HIGH and PacketLossRate is HIGH and FrameType is I ) then RedundancyAmount is HIGH "" , engine ) ) block addRule ( new MamdaniRule ( "" if ( TemporalIntensity is HIGH and PacketLossRate is HIGH and FrameType is I or P ) then RedundancyAmount is HIGH "" , engine ) ) Packet loss x video characteristics rulesalgo : MINT : lossVideoRules algorithmMINT - FEC utilises the same core structure of uavFEC , so once all the fuzzy rules and sets are defined , they are employed in real - time in the fuzzy logic controller ."
NE,nash equilibrium,TR-1105,"The AlgorithmsWe use two multi - population ( each player has its own population of chromosomes representing its alternative choices at any round ) co - evolutionary genetic algorithms , Vriend 's individual learning algorithm [ 15 ] and co - evolutionary programming , a similar algorithm that has been used for the game of prisoner 's dilemma [ 10 ] and , unsuccessfully , for Cournot Duopoly [ 13]. Since those two algorithms do n't , as it will be seen , lead to convergence to the NE in the models under consideration , we introduce two different versions of the algorithms , as well , which are characterized by the use of opponent choices , when the new generation of each player 's chromosome population is created , and therefore can be regarded as "" socialized "" versions of the two algorithms ."
CI,continuous integration,TR-1116,"tab : practices tabularlll Group & Best practice & Implementation candidates ( not exhaustive ) 3*Project management & Version control system & git , mercurial , svn 2 - 3 & Project management tool & GitLab , GitHub , Bitbucket , JIRA 2 - 3 & Workflow & GitLab Flow , GitHub Flow , git flow 3*Coding style & Code formatting style & Mozilla , LLVM , Google , Chromium 2 - 3 & Code formatting tool & clang - format 2 - 3 & Static code analysis & clang - tidy , cppcheck , cpplint 2*Independence & Use open file formats & e.g. , JSON , CSV , HDF5 2 - 3 & Use open - source libraries & e.g. , Eigen , FFTW , GNU Scientific Library 2*Automation & Continuous integration & gitlab - ci , Travis CI , AppVeyor , Microsoft Azure 2 - 3 & Build automation & CMake , GNU make , Bazel , Ninja , MS Build 2*Documentation & Function reference & Doxygen , Sphinx ( with Breathe ) 2 - 3 & "" Big picture "" documentation & Markdown , reStructuredText 2*Testing & Unit test framework & Catch2 , Google Test , Boost Test Library 2 - 3 & Code coverage report & gcov , various commercial tools 2*Deployment & Package binaries & conda , Conan , Debian apt 2 - 3 & Online documentation & GitLab Pages , GitHub Pages , readthedocs.io tabular adjustwidthtableImplementation of the project skeletonsec : implementationBased on the ( general and programming language agnostic ) best practices introduced in the section above , we implement measures for a C++ software library with bindings for the Python language in this section ."
ADN,activity driven networks,TR-1128,"tikzpicture customlegend[legend columns=5,legend style = at=(0.12,1.02),draw = none , column sep=2ex , line width=2 pt , legend entries = DDT , Het GDT , Het ADN , Hom GDT , Hom ADN ] mark = triangle , solid , line legend , color = blue mark = x , solid , color = red mark = triangle , solid , color = magenta mark = o , color = cyan mark = star , color = yellow customlegend tikzpictureadf_a.pdf adf_b.pdf1emadf_c.pdf adf_d.pdfDiffusion dynamics on various contact networks - real contact network ( DDT ) , heterogeneous GDT ( Het GDT ) , homogeneous GDT ( Hom GDT ) , heterogeneous ADN ( Het ADN ) , and homogeneous ADN ( Hom ADN ) : A ) disease prevalence dynamics , B ) accumulative infection over simulation days , C ) variations in the daily prediction comparing to real contact network and D ) variation in the accumulative predictionfig : nethmdffigureDiffusion analysisThe selected four synthetic contact networks ( Hom ADN , Het ADN , Hom GDT and Het GDT ) are generated with 364 K nodes for 32 days and simulations are run with the selected disease parameters ."
SCS,spoken conversational search,TR-1142,table : schemas1tabletable[]tabularllllll 1c & 5c[HTML]DAE8FCAgent 1c & 2c[HTML]DAE8FCRequest & 3c[HTML]DAE8FCAnswer 1c-3*Models & 1c[HTML]DAE8FCOffer & 1c[HTML]DAE8FCUnderstand & 1c[HTML]DAE8FCResults & 1c[HTML]DAE8FCBackchannel & 1c[HTML]DAE8FCEmpty COR & offer & & assert & promise & reject 10 labels & withdraw & & & & CS DBLP : conf / chiir / RadlinskiC17 & a0 & & a1i & & 10 labels & a1p & & a2+i & & & a2+p & & & & & & & & & SCS & Query refine- & Asks to & SERP w/o modification & Confirms & 13 labels & ment offer & repeat & SERP with modification & & & & & SERP with modification + Suggestion & & & & & Scanning document theme & & ODE & list(keywords ) & prompt(keywords ) & bool(data ) & confirm ( ) & 17 labels & prompt(link ) & verify ( ) & count(data ) & success ( ) & & & & top(keywords ) & & & & & link(dataset ) & & DSTC1 & request & & schedule & ack & sorry 32 labels & open - request & are - you - there & morebuses & hold - on & canthelp.cantfindstop & example & bebrief & & impl - conf & canthelp.fromequalsto & & expl - conf & & & canthelp.nobusesattime & & please - repeat & & & canthelp.noconnection & & please - rephrase & & & canthelp.nonextbus & & & & & canthelp.routedoesntrun & & & & & canthelp.systemerror & & & & & canthelp.uncoveredroute & & & & & canthelp.uncoveredstop DSTC2 & request & & inform & impl - conf & canthelp 22 labels & welcomemsg & expl - conf & offer & & canthelp.exception & reqmore & repeat & & & & select & confirm - domain & & & tabularSchema alignments ( Part 2 ) .
MT,machine translation,TR-1316,lcccccc System & + + & + - & - + & - - & + 0 & - 0en - fr - rnn - ff & 3710 & 3023 & 10157 & 18683 & 10 & 95519en - fr - smt - ff & 3362 & 3381 & 32577 & 46714 & 0 & 45068en - fr - trans - ff & 3839 & 2901 & 12398 & 24403 & 3 & 87558en - fr - rnn - back & 3356 & 3372 & 13009 & 17253 & 15 & 94097en - fr - smt - back & 3246 & 3496 & 34111 & 43472 & 1 & 46776en - fr - trans - back & 3482 & 3254 & 14610 & 20962 & 7 & 88787en - es - rnn - ff & 4667 & 3532 & 9929 & 19149 & 41 & 130875en - es - smt - ff & 4276 & 3963 & 39817 & 56169 & 1 & 63967en - es - trans - ff & 4626 & 3601 & 11379 & 25698 & 13 & 122876en - es - rnn - back & 4265 & 3951 & 13716 & 17872 & 24 & 128365en - es - smt - back & 4006 & 4233 & 39636 & 51831 & 1 & 68486en - es - trans - back & 4288 & 3929 & 14295 & 22032 & 23 & 123626 Frequency exacerbation and decay count ( Train set ) lcccccc System & + + & + - & - + & - - & + 0 & - 0en - fr - rnn - ff & 2917 & 2335 & 10653 & 15400 & 11 & 57623en - fr - smt - ff & 2652 & 2610 & 20587 & 26949 & 1 & 36140en - fr - trans - ff & 2997 & 2264 & 12537 & 17430 & 2 & 53709en - fr - rnn - back & 2642 & 2610 & 13513 & 14963 & 11 & 55200en - fr - smt - back & 2577 & 2684 & 22604 & 26608 & 2 & 34464en - fr - trans - back & 2701 & 2554 & 14932 & 17101 & 8 & 51643en - es - rnn - ff & 3541 & 2669 & 10636 & 16425 & 27 & 75113en - es - smt - ff & 3252 & 2982 & 23389 & 29057 & 3 & 49728en - es - trans - ff & 3508 & 2716 & 12069 & 19046 & 13 & 71059en - es - rnn - back & 3241 & 2971 & 14394 & 15847 & 25 & 71933en - es - smt - back & 3163 & 3072 & 24547 & 28389 & 2 & 49238en - es - trans - back & 3256 & 2967 & 15160 & 18606 & 14 & 68408 Frequency exacerbation and decay count ( Test set ) lcccccc System & + + & + - & - + & - - & + 0 & - 0en - fr - rnn - ff & 840.76 & 687.16 & 46.36 & 115.27 & 1.47 & 83.22en - fr - smt - ff & 664.86 & 555.60 & 31.17 & 119.64 & 0.00 & 20.79en - fr - trans - ff & 663.00 & 552.74 & 49.98 & 108.63 & 0.40 & 51.20en - fr - rnn - back & 770.72 & 680.73 & 83.68 & 96.68 & 2.19 & 74.81en - fr - smt - back & 620.67 & 525.26 & 40.36 & 112.35 & 0.13 & 23.29en - fr - trans - back & 639.69 & 568.68 & 75.88 & 90.25 & 1.05 & 55.58en - es - rnn - ff & 733.44 & 535.15 & 42.54 & 117.47 & 4.93 & 118.43en - es - smt - ff & 547.86 & 423.87 & 33.22 & 129.73 & 0.12 & 27.35en - es - trans - ff & 587.22 & 436.02 & 47.61 & 119.98 & 1.37 & 77.46en - es - rnn - back & 677.23 & 564.31 & 94.47 & 101.57 & 2.92 & 102.90en - es - smt - back & 561.03 & 438.09 & 44.31 & 133.35 & 0.12 & 33.78en - es - trans - back & 548.37 & 438.33 & 72.27 & 98.11 & 2.33 & 81.87 Accumulated frequency differences ( Train set ) lcccccc System & + + & + - & - + & - - & + 0 & - 0en - fr - rnn - ff & 827.07 & 655.81 & 68.84 & 133.21 & 2.48 & 104.41en - fr - smt - ff & 790.41 & 640.60 & 60.98 & 156.94 & 0.13 & 53.71en - fr - trans - ff & 662.76 & 533.83 & 73.15 & 123.07 & 0.31 & 78.70en - fr - rnn - back & 751.49 & 655.35 & 112.32 & 114.16 & 2.28 & 92.01en - fr - smt - back & 679.17 & 551.88 & 64.50 & 142.50 & 0.34 & 48.96en - fr - trans - back & 625.59 & 548.18 & 104.26 & 107.39 & 1.41 & 72.88en - es - rnn - ff & 726.16 & 509.28 & 67.76 & 134.45 & 4.16 & 146.04en - es - smt - ff & 679.08 & 503.57 & 70.86 & 169.33 & 0.38 & 76.67en - es - trans - ff & 592.32 & 414.37 & 73.00 & 134.59 & 1.84 & 114.52en - es - rnn - back & 653.89 & 533.03 & 128.86 & 119.04 & 4.22 & 126.46en - es - smt - back & 630.86 & 462.82 & 74.19 & 165.11 & 0.31 & 76.81en - es - trans - back & 538.03 & 415.49 & 103.32 & 118.89 & 2.40 & 104.57 Accumulated frequency differences ( Test set ) Remarks on automatic evaluationThe summary of our results allows us to draw the following conclusions:[leftmargin= * ] Lexical richness All metrics and results presented in Table and Table and for both language pairs indicate that neither of the MT systems reaches the lexical richness of the HT .
MPI,multiple parallel instances,TR-1372,"figure[!htb]subfigure.3 figures / Comparison_IO_compute_scaling_traj_splitting - chain - reader_edited.pdf format = hang Scaling for different components fig : MPIscaling - chain - readersubfiguresubfigure.3 figures / Comparison_tot_time_traj_splitting - chain - reader_edited.pdf Scaling total fig : MPItottime - chain - readersubfiguresubfigure.3 figures / Comparison_Speed_UP_traj_splitting - chain - reader_edited.pdf Speed - up fig : MPIspeedup - chain - readersubfiguresubfigure.45 figures / chain - reader - no - ga - BarPlot - rank - comparison_192_5.pdf format = hang Time comparison of different parts of the calculations per MPI rank using ChainReader with MPI collective communications fig : MPIranks - split - chain - readersubfiguresubfigure.45 figures / chain - reader - ga - BarPlot - rank - comparison_192_3.pdf format = hang Time comparison on different parts of the calculations per MPI rank using ChainReader using Global Arrays fig : MPIranks - split - ga - chain - readersubfigureComparison on the performance of the MDAnalysis ChainReader for the RMSD task on SDSC Comet when the trajectories are split ; for the communication step either collective MPI ( "" MPI "" ) or Global Arrays ( "" ga "" ) was used ."
LC,least confidence,TR-1582,tab : measurescentertabletable*[t]tabularlcccccccccDsets & Chance & FTZ Ent - Ent & FTZ Ent - LC & MNB Ent - Ent & MNB Ent - LC & FTZ Ent - Ent & FTZ Ent - LC & MNB Ent - Ent & MNB Ent - LC SGN & & & & & & & & & DBP & & & & & & & & & YHA & & & & & & & & & YRP & & & & & & & & & YRF & & & & & & & & & AGN & & & & ? &
BF,black females,TR-1632,"[ cloud , below of = gc , node distance = 2 cm ] ( m ) Race Classification ; [ cloud , below of = init , node distance = 2 cm ] ( f ) Race Classification ; [ block3 , below left of = m , node distance = 1.5 cm ] ( of ) BM Age Estimator ; [ block3 , below right of = m , node distance = 1.5 cm ] ( hf ) WM Age Estimator ; [ block3 , below left of = f , node distance = 1.5 cm ] ( om ) BF Age Estimator ; [ block3 , below right of = f , node distance = 1.5 cm ] ( hm ) WF Age Estimator ; [ - > ] ( init ) - ( pp ) ; [ - > ] ( pp ) - ( gc ) ; [ - > ] ( gc ) - node Male ( m ) ; [ - > ] ( gc.south ) - + + ( 0,-.3 cm ) - ( f ) node[near end , above left , yshift=-4pt ] Female ; [ - > ] ( m ) - ( of ) ; [ - > ] ( m ) - ( hf ) ; [ - > ] ( f ) - ( om ) ; [ - > ] ( f ) - ( hm ) ; tikzpicture adjustboxOverview of race - composite age prediction framework ."
GA,global arrays,TR-1755,"MPI - parallel Multi - frame RMSD using Global Arrays alg : GA Input : size : Total number of frames assigned to each rank ga : Initialized Global Arrays xref0 : mobile group in the initial frame which will be considered as reference start stop : that tell which block of trajectory ( frames ) is assigned to each rank topology trajectory : files to read the data structure from Include : BlockRMSD ( ) from Algorithm alg : RMSD algorithmic[1 ] bsize ceil(trajectory.numberframes / size ) ga ga.create(ga.CDBL , [ bsize*size,2 ] , "" RMSD "" ) buf np.zeros([bsize*size,2 ] , dtype = float ) out BlockRMSD(topology , trajectory , xref0 , start = start , stop = stop ) ga.put(ga , out , ( start,0 ) , ( stop,2 ) ) rank = = 0 buf ga.get(ga , lo = None , hi = None ) algorithmicalgorithmMPI and Parallel HDF5sec : methods - hdf5HDF5 is a structured self - describing hierarchical data format which is the standard mechanism for storing large quantities of numerical data in Python ( http://www.hdfgroup.org/HDF5,pythonhdf5 ) ."
CT,class table,TR-1839,"equation translate^*(CTcls ) = cases ( C.extends : D ) & if ( CTcls)= ( C.extends = D ) ( C.f : C_f ) & if ( CTcls ) = ( C.f : C_f ) ( C.m : CC_r ) & if ( CTcls ) = ( C.m : CC_r ) ( C.init(C ) ) & if ( CTcls ) = ( C.init(C ) ) casesequationdefinitiondefinition[Clauses of supertypes of CReq ] equation ( CReq , CR ) _ = cases ( T.extends : T ' ) & for CReq = ( T.extends : T ' ) ( T.init(T ' ) & for ( T.init(T ' ) CReqs(CR ) & CReq = ( T.init(T ) ) T < : T ' ( T'.f : T'_f ) & for ( T'.f : T'_f ) CReqs(CR ) & CReq = ( T.f : T_f ) T < : T ' ( T'.m : T'T'_r ) & for ( T'.m : T'T'_r ) CReqs(CR ) & CReq = ( T.m : TT_r ) T < : T ' casesequationdefinitiondefinition[Clauses of subtypes of CReq ] equation ( CReq , CR ) _ = cases ( T.extends : T ' ) & for CReq = ( T.extends : T ' ) ( T.init(T ' ) & for ( T.init(T ' ) CReqs(CR ) & CReq = ( T.init(T ) ) T ' < : T ( T'.f : T'_f ) & for ( T'.f : T'_f ) CReqs(CR ) & CReq = ( T.f : T_f ) T ' < : T ( T'.m : T'T'_r ) & for ( T'.m : T'T'_r ) CReqs(CR ) & CReq = ( T.m : TT_r)T ' < : T casesequationdefinitiondefinition[Clauses of superclasses of CTcls ] equation ( CTcls , CT)_^ * = cases ( C.extends : D ) & for CTcls = ( C.extends = D ) ( C.init(C ' ) & for ( C.init(C ' ) CT & CTcls = ( C.init(C ) ) C < : C ' ( D.f : D ' ) & for ( D.f : D ' ) CT & CTcls = ( C.f : C ' ) C < : D ( D.m : DD_r ) & for ( D.m : DD_r ) CT & CTcls = ( C.m : CC_r ) C < : D casesequationdefinitiondefinition[Compatible class requirements ] Given two class requirements , compatibility of two class requirements is defined over all cases of clauses : definitiondefinition[Compatibility between a class requirement and a class table clause]Given a class table clause , a class requirement , and a ground solution , such that ground , compatibility is defined over all cases of clauses : definitionlemma[Weakening for context]lem : conxtWeak If tT , and , then x : CtT.lemmaproof Straightforward induction on typing derivations ."
PSO,particle swarm optimization,TR-1938,"Let the position , the velocity and the learning exemplars of the particle be , * * Based on the proposed learning approach ( as shown in Algorithm- ) , the velocity of the particle is updated as follows : Determination of the cardinality of all sets ( and ): * * Evaluation of the cardinality learning sets ( , and ): * * Evaluation of the feature learning sets ( , and ): * * Derivation of the final learning sets ( , and ) * * Velocity update of the particle ( ) as per ( ) * Position UpdateIn the majority of binary PSO variants , the position update process entails the decision to exclude or include each feature ."
FEC,forward error correction,TR-2168,The parameters are defined as follows : FEC - based : accounts for mechanisms that employ FEC ; ARQ - based : mark mechanisms that use ARQ ; QoE - sensitive data : this parameter demonstrates mechanisms that identity and/or considerate the video content to define the EC policy ; Video - aware : check mark is given to mechanisms that use any video characteristics to define the amount of redundancy and/or retransmission ; High - quality video : it is marked if the mechanisms are using videos equal or higher than 720p ( HD ready ) ; Network status : this parameter defines if the mechanisms use the information about the network healthy to define the redundant data ; UEP - enabled : means that different amounts of redundancy are being added to distinct portions of the video .
OP,old persian,TR-2196,"[ 196ff.]Lipp2009 states that OP -st- ( found as a reflex of PIE * -k - t- , * -g - t- ) is due to analogy , while other developments are due to a phonological change predating Middle Persian:[noitemsep]PIE * hreg - to- PIr * rasta- OP rasta- ' right ' MP rast NP rastPIr * musti- ' fist ' MP must , must NP mustPIr * -ista- ( superlative suffix ) MP -ist ; e.g. , Phl balist , MMP barist ' highest ' * barjista- ; Phl xwalist , MMP xwarist ' sweetest ' * huarjista- ( cf ."
AI,artificial intelligence,TR-2291,"A clearer account of research priorities and disagreementsUnpacking the near / long - term distinctionAs commonly used , the terms ' near - term ' and ' long - term ' in fact appear to capture four different dimensions of differing priorities within the AI ES research community : [ ] Capabilities : whether to focus on the impacts and challenges of current AI systems , or those relating to much more advanced AI systems [ ] Impacts : whether to focus mostly on the immediate impacts of AI for society , or whether to consider possible impacts much further into the future ."
EHS,enhanced hybrid simultaneous,TR-2381,"So the derived equation after putting the values is like as below : OP_CEU^x_3 = p_F_CEUp_F_CEU+p_N_CEU(1-e^-_CEU^R_3Ps_CEU p_F ) + ( 1-e^(-_CEU^R_3_CCU _ CCU , CEU(21-+ ) ) ) -p_F_CEUp_F_CEU+p_N_CEU ( 1-e^-_CEU^R_3Ps_CEU p_F)(1-e^(-_CEU^R_3_CCU _ CCU , CEU(21-+)))Energy EfficiencyThe harvested energy at CCU by EHS protocol can be derived as below [ 11 ] , In addition , the transmitted power from CCU to CEU can be expressed as below based on the harvested energy [ 21 ] , Moreover , the EE can be derived as below for the proposed EHS - CNOMA scheme [ 21 ] , So Eq.29 shows that EE is related to the and ."
SA,significance and accuracy,TR-2429,"The specific contributions of this work are presented below with a reference to the section in which they are described : enumerate A graphical user interface for simplifying the deployment of workflows for the RTF , which is coupled with a code generator that allows the flexible use of the RTF on distinct domains [ Section sec : improve ] ; The development and analysis of multi - level reuse algorithms : enumerate A coarse - grain merging algorithm was implemented [ Section sec : stage - merging ] ; A fine - grain Naive Merging Algorithm was proposed and implemented [ Section sec : naive - merging ] ; The fine - grain Smart Cut Merging Algorithm was proposed and implemented [ Section sec : sca ] ; The fine - grain Reuse - Tree Merging Algorithm was proposed and implemented [ Section sec : rtma ] ; enumerate Proposal and implementation of the Task - Balanced Reuse - Tree Merging Algorithm that reduces the issue of loss of parallelism due to load imbalance provoked by the Reuse - Tree Merging Algorithm [ Section sec : TRTMA ] ; The performance gains of the proposed algorithms with a real - world microscopy image analysis application were demonstrated using different SA strategies ( e.g MOAT and VBD ) at different scales ."
CT,computed tomography,TR-2567,figure figure [ ht ] center minipage0.15 ./fig / snapshot0109new2.png minipage minipage0.15 ./fig / snapshot0129new2.png minipage minipage0.15 ./fig / snapshot0139new2.png minipage minipage0.15 ./fig / snapshot0149new2.png minipage minipage0.15 ./fig / snapshot0110new3.png minipage minipage0.15 ./fig / snapshot0130new3.png minipage minipage0.15 ./fig / snapshot0140new3.png minipage minipage0.15 ./fig / snapshot0150new3.png minipage minipage0.15 ./fig / snapshot0106new3.png minipage minipage0.15 ./fig / snapshot0126new3.png minipage minipage0.15 ./fig / snapshot0136new3.png minipage minipage0.15 ./fig / snapshot0146new3.png minipage minipage0.15 ./fig / snapshot0103new3.png minipage minipage0.15 ./fig / snapshot0123new3.png minipage minipage0.15 ./fig / snapshot0133new3.png minipage minipage0.15 ./fig / snapshot0143new3.png minipage center 5fig : vis2_2 Visualizations for five anatomies on the first four holdout CT images .
PCA,principal component analysis,TR-2637,"2003 International Conference on , volume=3 , pages = III-13 , year=2003 , organization = IEEE@inproceedingsphillips1999support , title = Support vector machines applied to face recognition , author = Phillips , P Jonathon , booktitle = Advances in Neural Information Processing Systems , pages=803 - 809 , year=1999@incollectionzhao1998discriminant , title = Discriminant analysis of principal components for face recognition , author = Zhao , Wenyi and Krishnaswamy , Arvindh and Chellappa , Rama and Swets , Daniel L and Weng , John , booktitle = Face Recognition , pages=73 - 85 , year=1998 , publisher = Springer@articletoygar2003face , title = Face recognition using PCA , LDA and ICA approaches on colored images , author = Toygar , Onsen and Adnan , Acan , journal = IU - Journal of Electrical Electronics Engineering , volume=3 , number=1 , pages=735 - 743 , year=2003@inproceedingsmarcialis2002fusion , title = Fusion of LDA and PCA for Face Verification , author = Marcialis , Gian Luca and Roli , Fabio , booktitle = International Workshop on Biometric Authentication , pages=30 - 37 , year=2002 , organization = Springer@articlesatonkarface , title = FACE RECOGNITION USING DIFFERENT DISTANCE MEASURES TECHNIQUES , author = Satonkar , Suhas S and Kurhe , Ajay B and Khanale , Prakash B"
ASA,adaptive segmentation algorithm,TR-2656,"[ ] [ ASA][][Deep learning ] [ ] [ ASA][][Deep learning ] [ ] [ ASA][][Deep learning ] Examples from the test set , where a , c , and d are the ASA mask contours overlaid on manual annotation images ( counted neurons have blue marks ) , b , d , and f are the iterative deep learning predicted masks ( iteration 5 ) contours overlaid on manual annotation imageDiscussionThe evidence from this study suggests that the iterative deep learning based unbiased stereology method presented herein is much faster and more accurate than the state - of - the - art stereology since human involvement was mainly reduced ."
CT,computed tomography,TR-2696,"The total loss can be formulated asequation5eq : hybridlossalignedTP_p(c ) = & _ n=1^Np_n(c)g_n(c ) FN_p(c ) = & _ n=1^N(1-p_n(c))g_n(c ) FP_p(c ) = & _ n=1^Np_n(c)(1-g_n(c ) ) L = & L_Dice + L_Focal = & C - _ c=0^C-1TP_p(c)TP_p(c ) + FN_p(c ) + FP_p(c ) & - 1N_c=0^C-1_n=1^Ng_n(c)(1-p_n(c))^2(p_n(c ) ) , alignedequationwhere , and are the true positives , false negatives and false positives for class calculated by prediction probabilities respectively , is the predicted probability for voxel being class , is the ground truth for voxel being class , is the total number of anatomies plus one ( background ) , is the trade - off between dice loss and focal loss , and are the trade - offs of penalties for false negatives and false positives which are set as 0.5 here , is the total number of voxels in the CT images ."
GA,global arrays,TR-2708,sidewaystable[hp]adjustboxmax width = tabularc c c c c c c c c c c c 10r ( r)5 - 12 Cluster & Gather & File Access & Time & Serial & tabularc Comet : 24 Bridges : 24 SuperMIC : 20 tabular & tabularc Comet : 48 Bridges : 48 SuperMIC : 40 tabular & tabularc Comet : 72 Bridges : 60 SuperMIC : 80 tabular & tabularc Comet : 96 Bridges : 78 tabular & tabularc Comet : 144 Bridges : 84 SuperMIC : 160tabular & Comet : 192 & tabularc Comet : 384 SuperMIC : 320tabular Comet & MPI & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - & - & - Bridges & MPI & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - & - & - SuperMIC & MPI & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & - Comet & GA & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - & - & - Comet & MPI & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - SuperMIC & MPI & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & - Comet & GA & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - SuperMIC & GA & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & - Comet & MPI & PHDF5 & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular Bridges & MPI & PHDF5 & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - SuperMIC & MPI & PHDF5 & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & tabularc tabular tabularadjustboxComparison of the compute and I / O scaling for different test cases and number of processes .
PC,principal component,TR-2711,"@ X rr rr@ & 2 c CNS & 2 c MDC ( lr)2 - 3(lr)4 - 5 & ( PC 0 ) & ( PC 1 ) & ( PC 0 ) & ( PC 1 ) Social circle size , & 0.41 & 0.16 & 0.37 & -0.15 Activity space size , & 0.42 & -0.24 & 0.42 & -0.08 New ties / week , & 0.33 & 0.28 & 0.27 & 0.33 New locations / week , & 0.38 & -0.05 & 0.37 & 0.19 Social circle entropy , & 0.31 & 0.30 & 0.34 & 0.09 Activity space entropy , & 0.38 & -0.16 & 0.30 & -0.07 Social circle stability , & -0.16 & -0.46 & 0.07 & -0.72 Activity space stability , & -0.10 & -0.49 & -0.12 & -0.51 Social circle rank turnover , & -0.20 & 0.28 & -0.33 & 0.10 Activity space rank turnover , & -0.30 & 0.44 & -0.38 & 0.17 Contribution of the original variables to the first principal component ."
SOA,service oriented architecture,TR-2714,"Still in that paper , the authors cite who presented a comparison between HLA and SOA concluding that : HLA has good interoperability , synchronisation and effective and uniform information exchange mechanism between the communicating components ( federates ) , but lacks several features of web services , such as the integration of heterogeneous resources , web - wide accessibility across firewall boundaries;SOA benefits from loose coupling , component reuse and scalability but lacks a uniform data exchange format and time synchronisation mechanisms;The combination of HLA and SOA can extend the capabilities of the two technologies and thus enable integrated simulated and real services ."
SO,smart object,TR-2730,"& Consumer & Smart dimensions & SO 1 & SO 2 & SO & Object & & & & network Varying & & & & complexity & & & & of PP & & & & Varying & & & & query & & & & * complexity & & & & Varying & & & & number of & & & & sensing SOs & & & & tabular Experiments : - time overhead , * time and bandwidth overhead tab : expTabletable Experimental resultsIn executing our experiments , we considered three main characteristics that may impact the performance of the proposed solution ."
PRR,pre - reduced ring,TR-2764,"h]input parameters : - number of elements ( floats ) in reduced data - number of iterations - mode of delay : one - late / rand - late - maximal delay of the process(es ) - tested algorithm , one of ring , Rabenseifner , PRR , SLT - number of processes - process i d - MPI rank : output : - vector of measured average elapsed times for the given algorithmvariables : - 50 of the emulated computation time - start time of measurement - end time of measurement - elapsed time measured in the current process - vector of the measured times of all processes - vector of data to be reduced1 ."
VSM,vector space model,TR-2909,"table*[ht]threeparttableResults for Multi - label Music Genre Classification of Albumstbl : resultstabularlccccccccModality & Target & Settings & Params & Time & AUC & C@1 & C@3 & C@5 Audio & logistic & timbre - mlp & 0.01 M & 1s & 0.792 & 0.04 & 0.14 & 0.22 Audio & logistic & low-3x3 & 0.5 M & 390s & 0.859 & 0.14 & 0.34 & 0.54 Audio & logistic & high-3x3 & 16.5 M & 2280s & 0.840 & 0.20 & 0.43 & 0.69 Audio & logistic & low-4x96 & 0.2 M & 140s & 0.851 & 0.14 & 0.32 & 0.48 Audio & logistic & high-4x96 & 5 M & 260s & 0.862 & 0.12 & 0.33 & 0.48 Audio & logistic & low-4x70 & 0.35 M & 200s & 0.871 & 0.05 & 0.16 & 0.34 Audio & logistic & high-4x70 & 7.5 M & 600s & 0.849 & 0.08 & 0.23 & 0.38 Audio & cosine & low-3x3 & 0.33 M & 400s & 0.864 & 0.26 & 0.47 & 0.65 Audio & cosine & high-3x3 & 15.5 M & 2200s & 0.881 & 0.30 & 0.54 & 0.69 Audio & cosine & low-4x96 & 0.15 M & 135s & 0.860 & 0.19 & 0.40 & 0.52 Audio & cosine & high-4x96 & 4 M & 250s & 0.884 & 0.35 & 0.59 & 0.75 Audio & cosine & low-4x70 & 0.3 M & 190s & 0.868 & 0.26 & 0.51 & 0.68 Audio ( A ) & cosine & high-4x70 & 6.5 M & 590s & 0.888 & 0.35 & 0.60 & 0.74 Text & logistic & VSM & 25 M & 11s & 0.905 & 0.08 & 0.20 & 0.37 Text & logistic & VSM+Sem & 25 M & 11s & 0.916 & 0.10 & 0.25 & 0.44 Text & cosine & VSM & 25 M & 11s & 0.901 & 0.53 & 0.44 & 0.90 Text ( T ) & cosine & VSM+Sem & 25 M & 11s & 0.917 & 0.42 & 0.70 & 0.85 Image ( I ) & logistic & ResNet & 1.7 M & 4009s & 0.743 & 0.06 & 0.15 & 0.27 A + T & logistic & mlp & 1.5 M & 2s & 0.923 & 0.10 & 0.40 & 0.64 A + I & logistic & mlp & 1.5 M & 2s & 0.900 & 0.10 & 0.38 & 0.66 T + I & logistic & mlp & 1.5 M & 2s & 0.921 & 0.10 & 0.37 & 0.63 A + T + I & logistic & mlp & 2 M & 2s & 0.936 & 0.11 & 0.39 & 0.66 A + T & cosine & mlp & 0.3 M & 2s & 0.930 & 0.43 & 0.74 & 0.86 A + I & cosine & mlp & 0.3 M & 2s & 0.896 & 0.32 & 0.57 & 0.76 T + I & cosine & mlp & 0.3 M & 2s & 0.919 & 0.43 & 0.74 & 0.85 A + T + I & cosine & mlp & 0.4 M & 2s & 0.931 & 0.42 & 0.72 & 0.86 tabular tablenotes Number of network hyperparameters , epoch training time , AUC - ROC , and catalog coverage at for different settings and modalities ."
MDC,mobile data challenge,TR-2926,"1.5@ X rrr rrr @ Model M1 : Activity space size , & coeff & p val & LMG Social circle size , & & & 0.98 gender & & 0.8 & 0.01 age group & & 0.3 & 0.01 time coverage & & 0.4 & 0.0 [ , , ] Model M2 : Activity space entropy , & & & Social circle entropy , & & 0.009 & 0.28 gender & & 0.3 & 0.03 age group & & 0.06 & 0.21 time coverage & & 0.002 & 0.48 [ , , ] Model M3 : New locations / week , & & & New ties / week , & & 0.002 & 0.69 gender & & 0.9 & 0.04 age group & & 0.2 & 0.1 time coverage & & 0.06 & 0.17 [ , , ] Model M4 : Activity space stability , & & & Social circle stability , & & 0.1 & 0.82 gender & & 0.6 & 0.15 age group & & 0.8 & 0.03 time coverage & & 1.0 & 0.0 [ , , ] Model M5 : Activity space rank turnover , & & & Social circle rank turnover , & & & 0.97 gender & & 0.8 & 0.02 age group & & 0.1 & 0.01 time coverage & & 0.7 & 0.0 [ , , ] Linear regression models for the MDC dataset ."
MF,matrix factorization,TR-2961,"tab : exp_mseadjustboxmax width=1tabularlcccccccccc2*Dataset & ( a ) & ( b ) & ( c ) & ( d ) & ( e ) & ( f ) & ( g ) & 3cImprovement of SFM verus & MF & MVM & FM-2 & FM-3 & PolyNet-2 & PolyNet-3 & SFM & b & min(c , d ) & min(e , f ) Game & 1.569 0.005 & 0.753 0.007 & 0.764 0.006 & 0.749 0.007 & 0.749 0.004 & 0.748 0.006 & 0.723 0.006 & 4.06 & 3.52 & 3.35 Cloth & 1.624 0.009 & 0.725 0.046 & 0.678 0.004 & 0.679 0.004 & 0.678 0.007 & 0.680 0.005 & 0.659 0.013 & 9.03 & 2.82 & 2.84 Sport & 1.290 0.004 & 0.646 0.019 & 0.638 0.003 & 0.632 0.007 & 0.631 0.005 & 0.632 0.005 & 0.614 0.011 & 5.00 & 2.91 & 2.79 Health & 1.568 0.007 & 0.807 0.012 & 0.779 0.004 & 0.778 0.004 & 0.779 0.005 & 0.776 0.005 & 0.763 0.019 & 5.47 & 2.02 & 1.77 Home & 1.591 0.004 & 0.729 0.067 & 0.714 0.002 & 0.714 0.004 & 0.690 0.003 & 0.692 0.005 & 0.678 0.008 & 6.93 & 5.00 & 1.72 Elec & 1.756 0.002 & 0.792 0.042 & 0.776 0.006 & 0.749 0.007 & 0.760 0.004 & 0.757 0.001 & 0.747 0.006 & 5.69 & 0.27 & 1.33 Yelp & 1.713 0.003 & 1.2575 0.013 & 1.277 0.002 & 1.277 0.002 & 1.272 0.002 & 1.272 0.002 & 1.256 0.010 & 0.09 & 1.58 & 1.19 BX & 4.094 0.025 & 2.844 0.024 & 2.766 0.012 & 2.767 0.014 & 2.654 0.013 & 2.658 0.013 & 2.541 0.025 & 10.66 & 8.16 & 4.27 3lAverage on all datasets & & & & & & 5.87 & 3.29 & 2.41 tabularadjustboxtable *"
SVM,support vector machine,TR-2974,"dev None & 0.4212 & ( + /- 0.06 ) & 0.7572 & ( + /- 0.02 ) Sub & 0.4050 & ( + /- 0.06 ) & 0.6922 & ( + /- 0.03 ) Super & 0.4418 & ( + /- 0.05 ) & 0.7106 & ( + /- 0.02 ) Sub+Super & 0.4807 & ( + /- 0.09 ) & 0.6658 & ( + /- 0.03 ) tabular svm ' sample results tab : svm_sampletableFirst off , even though the sub- and super - sampling techniques yielded similar SDQC distributions , we see a clear advantage for the SVM with more data points , scoring 0.4050 with "" Sub "" and 0.4418 with Super , the latter improving over the original result with ."
AV,acquaintance vaccination,TR-2977,"List of SymbolsList of abbreviationslongtablecc Acronym & Descriptions ADN & Activity driven network modelling AV & Acquaintance vaccination APV & Absolute percentage variation CIP & co - location interaction parameters CN & Common neighbours DST network & Dense SPST network DDT network & Dense SPDT network DDT1 & Vaccinating neighbours in DDT network with direct linksDDT2 & Vaccinating neighbours in DDT network with any linksGDT & Generated SPDT network with 364 K nodes GST & Generated SPST network with 364 K nodes IMV & Individual movement based vaccination strategy IMVE & Individual movement based vaccination strategy with exact information IMVT & Individual movement based vaccination strategy with temporal information LST & SPDT network with the same number of links that of DDT network LST & SPST network with the same number of links that of DST network MLE & Maximum likelihood estimator OSN & Online social network PFU & Plaque - forming unit RV & Random vaccination DV & Degree vaccination RSE & Root squared error SPST & Same place same time transmission SPDT & Same place different time transmission SIR & Susceptible - infected - recovered SDT network & Sparse SPDT network of links having direct and indirect components SST network & Sparse SPST network of links having indirect component onlySPDT graph & graph based on SPDT diffusion longtableList of symbolslongtablecc Symbol & Descriptions A & Set of active copies of nodes in SPDT graph b & active particle decay rates from an area of interaction & activity potential of node & active periods of a node C & Particle concentration in interaction area C , C , C & scenario 1 , 2 and 3 d & Activation degree - number of SPDT links created during an activation E & Intake dose or exposure of infectious particles & Average volume fraction of room air introduced by exhaled breath f & distribution function F & Disease spreading force in the network at the current day of simulation & Average disease spreading force in the network & Graph & Dynamic graph g & Particle generation rate by an infected individual h & Activation frequency I & Number of infected individuals in the system & Number of infected individuals at a simulation day & Number of infected individuals in the system at the current time that disease prevalence & Number of infected individuals up to a simulation day L & links set N & Total number of individuals , nodes , users p & Pulmonary rate of susceptible individual & Infection probability for an intake dose & Probability of creating a link during an activation & Probability of breaking a created link Q & air exchange rate from an area q & Transition probability for changing inactive to active state R & disease reproduction ability of an infected individual r & Particle removal rate from interaction area r & Median of particles removal rates S & Number of susceptible individual T & Simulation period or disease observation period & Activation period or period host user or node stays at the interacted location & Link creation delay or delay neighbour user or node arrives at the interacted location & Stay duration of user or node stays at the interacted location V & Air volume of interaction area , , & waiting periods of a node Y & Labelling sets in graph X & updates z & Number of time step Z & set of nodes in the SPDT graph & power law exponent & Infection rate at the current day of simulation in the network for an infected individual & Indirect transmission period & central tendency & Scaling parameter of activation degree distribution & neighbour proportion & average volume fraction of room air that is exhaled by an susceptible individual & links presence function & state probability & nodes presence function & switching probability form active to inactive states & Infectiousness of infection particles & Fraction of dose or exposure reaches to the target infection site & Duration that virus is generated or infectious period of infected individual & inter - event time for node in activity driven networks & lower limit of active degree distribution & activation potential in ADN networks & particle accumulation rate & Transition probability for changing active to inactive state longtable"
LSA,licensed shared access,TR-3101,"Major abbreviations 8.7cmSlX mygray Abbreviation & mygray Description ACA - A & Alternative Ascending Clock Auction ACA - T & Traditional Ascending Clock Auction CA / CDF & Cryptographic Authority / Cumulative Distribution Function CIA & Confidentiality , Integrity and Availability triad CRN & Cognitive Ratio Network DoS / DDoS & Denial - of - Service / Distributed Denial - of - Service EBV&Encrypted Bit Vector FJ & Friendly Jamming HMAC&Hash Message Authentication Code KKT & Karush - Kuhn - Tucker LSA & Licensed Shared Access MANET & Mobile Ad hoc NETwork NUM & Network Utility Maximization OPE & Order Preserving Encryption PD / PU / SU&Primary Destination / Primary User / Secondary User SINR & Signal - to - Interference - plus - Noise Ratio SSDF & Spectrum Sensing Data Falsification TLC / TTP & Time Lapse Cryptography / Trusted Third Party VCG / GSP & Vickrey - Clarke - Groves / Generalized Second - Price auction Overview of wireless security issuesWireless networks play an extremely important role in many applications ."
FA,fractional anisotropy,TR-3215,tabularp0.8cmp5.5cmp6 cm 3cAmyloid Load ( PiB Positivity ) Set 1 & PiB Angular L / R & PiB Cingulum Ant L / R & PiB Cingulum Post L / R & PiB Frontal Med Orb L / R & PiB Precuneus L / R & PiB Temporal Sup L / R & PiB Temporal Mid L / R & PiB SupraMarginal L Set 2 & FA Cerebral peduncle R & FA Cerebral peduncle L & MD Corticospinal tract R & MD Corticospinal tract L & Trail - Making Test Part A Score & MD Cerebral peduncle R & PET Cingulum Post R & tabular Group difference across Amyloid Load ( PiB Positivity)tabletable [ ! ]
CRF,conditional random field,TR-3235,a ) FCN-32 ; ( b ) FCN-16s ; ( c ) ResNet - DUC ; ( d ) E - Net ; ( e ) SegNet ; ( f ) U - Net ; ( g ) FCN-8s ; ( h ) CWGAN - GP ; ( i ) FC - DenseNet ; ( j ) DSFE - CRF ; ( k ) DSFE - GCN ; ( l ) DSFE - GraphSAGE ; ( m ) DSFE - GGNN ; ( n ) DSFE - GGCN ; ( o ) Ground truth ; ( p ) Optical image .
CNN,convolutional neural network,TR-3241,table[H ] tabularcccccccccc & 3c80 & 3c160 & 3c320 & SRP & GMBF & CNNf15 + 11 & SRP & GMBF & CNNf15 + 11 & SRP & GMBF & CNNf15 + 11 2 * 01 & & & & & & & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & & & & & & & 2*Average & & & & & & & & & & & & & & & & & & tabular Results for the SRP - PHAT strategy ( columns SRP ) ; the one invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with sequences 15 and 11 ( columns CNNf15 + 11 ) tab : baselineResults+ft15 + 11table
PSD,power spectral density,TR-3286,"& Task&Segment & Feature Extraction&Classifier&Performance 4*Biased & 2*Abdullah2010 & 2 * 10 & 2 * 5 & 2*[2]weeks & 2 * 4 & EC & 2*[5]s & 2*AR & 2*ANN&CRR = [ 97.0 ] & & & & & & EO & & & & CRR = [ 96.0]2 - 11 & Riera2008&51&4 & 34 [ 74]days & 2 & EC & [ 4]s & AR , PSD , MuI , COH , CC & FDA & EER = [ 3.4 ] 2 - 11 & Su2010&40&2 & - & 1 & EC & [ 180]s & AR , PSD & KNN , LDA & CRR = [ 97.5 ] 8*Rigorous&Marcel2007&9&3&[3]days & 8 & MI & [ 1]s & PSD & MAP model & HTER = [ 19.3 ] 2 - 11 & Lee2013&4&2 & [ 10]days - [ 5]months & 1 & EC & [ 50]s & PSD & LDA & AC = [ 100 ] 2 - 11 & 2*Rocca2013 & 2 * 9 & 2 * 2 & 2 * 1 - [ 3]weeks & 3 & 2*EC & 2*[1]s & 2*AR & 2*Linear classier & CRR = [ 100 ] & & & & & 5 & & & & & CRR = [ 100 ] 2 - 11 & 2*Armstrong2015 & 15 & 2 & 5 - [ 40]days & 2 * 1 & 2*ERP & 2*[1.1]s & 2*Time - series & 2*CC & CRR = [ 89.0 ] & & 9 & 3 & 134 - [ 188]days & & & & & & CRR = [ 93.0 ] 2 - 11 & 2*Maiorana2016a & 2 * 50 & 2 * 3 & 2*Ave . ["
TVD,total variation diminishing,TR-3307,"After all four variants of SIMPLE - TS algorithms are tested , which are noted as follow : explicit TVD second - order scheme - approximate convective terms with explicit ( Forward Euler ) and TVD second - order schemeexplicit upwind first - order scheme - approximate convective terms with explicit ( Forward Euler ) and upwind first - order schemeimplicit TVD second - order scheme - approximate convective terms with explicit ( Backward Euler ) and TVD second - order schemeimplicit upwind first - order scheme - approximate convective terms with explicit ( Backward Euler ) and upwind first - order schemeExplicit and implicit schemes possess well - known advantages and disadvantages ."
LT,lomonosov 's turnip,TR-3322,"i & BDD - based Network Reliability & HLL & n / a & Hardy2007,Herrmann2010 4*ii & Lomonosov's - Turnip & LT & & Gertsbakh2016 2 - 5 & Sequential Splitting Monte Carlo & ST & , & Vaisman2016 2 - 5 & Generalized Splitting & GS & & Botev2012 2 - 5 & Recursive Variance Reduction & RVR & & Cancela2014 3*iii & Karger 's 2-step Algorithm & K2Simple & & Karger2016 2 - 5 iii & Optimal Monte Carlo Simulation & , & & Dagum2000 , Huber2017 2 - 5 & Counting - based Network Unreliability & & & This paper tabular tableTo the best of our knowledge , methods in Table t : methods are some of the best in their categories as evidenced in the literature ."
PSO,particle swarm optimization,TR-3463,"return NULL ; Agent * a = NULL ; a = ( Agent * ) malloc(sizeof(Agent ) ) ; a->v = NULL ; a->strength = NULL ; / * > > > NEW LINE HERE < < < * / switch ( opt_id ) case _ PSO _ : a->v = ( double * ) malloc(n*sizeof(double ) ) ; break ; ... case _ BSO _ : / * > > > NEW CASE HERE < < < * / a->strength = ( double * ) malloc(n*sizeof(double ) ) ; break ; default : free(a ) ; fprintf(stderr,""optimization identifier @CreateAgent "" ) ; return NULL ; break ; a->x = ( double * ) malloc(n*sizeof(double ) ) ; return a ; In function DestroyAgent ( LibOPT / src / common.c ) , you should deallocate your new variable : / * It deallocates an agentParameters : a : address of the agent to be deallocatedopt_id : identifier of the optimization technique * /void DestroyAgent(Agent * * a , int opt_id ) Agent * tmp = NULL ; tmp = * a ; if(!tmp ) fprintf(stderr,""not allocated @DestroyAgent . "" ) ;"
CNN,convolutional neural network,TR-3646,"update BibT.7exEXMAT - CNN - SOPC : Motionless Analysis of Traffic Using Convolutional Neural Networks on System - On - a - Programmable - ChipThis work is supported by the UK Engineering and Physical Sciences Research Council EPSRC [ EP / R02572X/1 and EP / P017487/1]. 978 - 1 - 5386 - 7753 - 7/18/31.00 © 2018 IEEESomdip Dey , Grigorios Kalliatakis , Sangeet Saha , Amit Kumar Singh , Shoaib Ehsan , Klaus McDonald - MaierEmbedded and Intelligent Systems LaboratoryUniversity of EssexColchester , UK somdip.dey , gkallia , sangeet.saha , a.k.singh , sehsan , kdm @essex.ac.ukIntelligent Transportation Systems ( ITS ) have become an important pillar in modern "" smart city "" framework which demands intelligent involvement of machines ."
RL,reinforcement learning,TR-3651,"Summary of main resultsfidelReinforcement Learning framework[leftmargin=5 mm ] for optimizing and adapting QEC codes , using arbitrary topological QEC codes , arbitrary decoders noise models , adaptable to any optimizer ( RL paradigm ) implementable on arbitrary platforms , applicable off - line ( simulation ) in - situ Simulations using[leftmargin=5 mm ] surface code quantum memory up to 68 fully connected data qubits , optimal linear - time decoder ( SQUAB ) Projective Simulation model for RLSimulations demonstrate agent 's ability[leftmargin=5 mm ] to determine optimal QEC codes for simple standard noise channels as well as non - isotropic noise , and for transfer learning in changing environmentsThe decoder is optimal for the simplified approximate error model that we use for faster estimation of the logical error rate , see Sec ."
ABC,artificial bee colony,TR-3761,"In this work , the main contributions are : ( i ) the objective function in has been reformulated using additional penalty term for optimal performance , ( ii ) two other evolutionary techniques ( DE and GSA ) have been used in the second phase to efficiently deploy the RNs , ( iii ) the effectiveness of the proposed algorithms is compared and contrasted with on the basis of network lifetime enhancement and speed of convergence , and lastly ( iv ) comprehensive experiments have been carried out to show the efficacy ( faster convergence and better optimal solution ) of using DE as opposed to ABC presented in to deploy backbone devices in WSNs ."
SL,strictly local,TR-3774,table*[t]Accuracy on Target SL Stringsets after 100 Epochstab : resultsSL4.5pttabularcccccccccc2c2Training & 2Test & 3cLSTM & 3cs - RNN & 2RPNI & & & 10 & 30 & 100 & 10 & 30 & 100 & 6SL2 & 21k & 1 & 0.772 ( 0.09 ) & 0.717 ( 0.08 ) & 0.711 ( 0.02 ) & 0.766 ( 0.11 ) & 0.761 ( 0.11 ) & 0.762 ( 0.10 ) & 0.855 & & 2 & 0.758 ( 0.09 ) & 0.696 ( 0.10 ) & 0.685 ( 0.02 ) & 0.757 ( 0.15 ) & 0.784 ( 0.17 ) & 0.768 ( 0.15 ) & 0.844 3 - 10 & 210k & 1 & 0.773 ( 0.17 ) & 0.616 ( 0.01 ) & 0.666 ( 0.01 ) & 0.682 ( 0.15 ) & 0.660 ( 0.11 ) & 0.649 ( 0.11 ) & 1.000 & & 2 & 0.772 ( 0.19 ) & 0.602 ( 0.01 ) & 0.650 ( 0.01 ) & 0.675 ( 0.16 ) & 0.650 ( 0.12 ) & 0.639 ( 0.12 ) & 1.000 3 - 10 & 2100k & 1 & 0.684 ( 0.15 ) & 0.615 ( 0.03 ) & 0.644 ( 0.01 ) & 0.700 ( 0.14 ) & 0.723 ( 0.16 ) & 0.620 ( 0.01 ) & 1.000 & & 2 & 0.669 ( 0.16 ) & 0.596 ( 0.02 ) & 0.624 ( 0.01 ) & 0.689 ( 0.16 ) & 0.718 ( 0.18 ) & 0.601 ( 0.01 ) & 1.000 6SL4 & 21k & 1 & 0.902 ( 0.01 ) & 0.907 ( 0.07 ) & 0.884 ( 0.06 ) & 0.913 ( 0.01 ) & 0.956 ( 0.01 ) & 0.968 ( 0.01 ) & 0.918 & & 2 & 0.836 ( 0.01 ) & 0.890 ( 0.04 ) & 0.901 ( 0.02 ) & 0.844 ( 0.01 ) & 0.896 ( 0.01 ) & 0.911 ( 0.01 ) & 0.813 3 - 10 & 210k & 1 & 0.840 ( 0.15 ) & 0.856 ( 0.12 ) & 0.942 ( 0.08 ) & 0.934 ( 0.12 ) & 0.982 ( 0.00 ) & 0.977 ( 0.01 ) & 0.995 & & 2 & 0.836 ( 0.16 ) & 0.852 ( 0.13 ) & 0.938 ( 0.08 ) & 0.938 ( 0.12 ) & 0.993 ( 0.00 ) & 0.991 ( 0.00 ) & 0.978 3 - 10 & 2100k & 1 & 0.975 ( 0.05 ) & 0.917 ( 0.12 ) & 0.898 ( 0.10 ) & 0.905 ( 0.16 ) & 0.989 ( 0.00 ) & 0.986 ( 0.00 ) & 1.000 & & 2 & 0.981 ( 0.04 ) & 0.923 ( 0.12 ) & 0.903 ( 0.10 ) & 0.916 ( 0.16 ) & 0.995 ( 0.00 ) & 0.994 ( 0.00 ) & 1.000 6SL8 & 21k & 1 & 0.981 ( 0.02 ) & 0.976 ( 0.04 ) & 0.995 ( 0.00 ) & 0.989 ( 0.01 ) & 0.999 ( 0.00 ) & 0.999 ( 0.00 ) & 0.991 & & 2 & 0.976 ( 0.02 ) & 0.965 ( 0.03 ) & 0.983 ( 0.01 ) & 0.991 ( 0.00 ) & 0.992 ( 0.00 ) & 0.996 ( 0.00 ) & 0.966 3 - 10 & 210k & 1 & 0.931 ( 0.09 ) & 0.979 ( 0.02 ) & 0.964 ( 0.03 ) & 0.995 ( 0.01 ) & 0.998 ( 0.00 ) & 0.997 ( 0.01 ) & 0.998 & & 2 & 0.980 ( 0.04 ) & 0.998 ( 0.00 ) & 0.999 ( 0.00 ) & 0.998 ( 0.00 ) & 0.998 ( 0.00 ) & 0.997 ( 0.01 ) & 0.994 3 - 10 & 2100k & 1 & 0.909 ( 0.11 ) & 0.864 ( 0.12 ) & 0.849 ( 0.11 ) & 0.995 ( 0.01 ) & 0.997 ( 0.00 ) & 0.997 ( 0.00 ) & 1.000 & & 2 & 0.976 ( 0.05 ) & 0.986 ( 0.02 ) & 0.980 ( 0.03 ) & 0.999 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 tabulartable *
ML,machine learning,TR-3800,"* Summary of the Main Features of Centralized AutoML Frameworks & Core & Training & Optimization & ML & & & & & & Language & Framework & Technique & Task & & & & & & & & & & & & & AutoWeka & 2013 & Java & Weka & Bayesian optimization & & & & & AutoSklearn & 2015 & Python & scikit - learn , & Bayesian optimization & & & & & TPOT & 2016 & Python & scikit - learn & Genetic Algorithm & & & & & SmartML & 2019 & R & & Bayesian optimization & & & & & Auto - MEKA & 2018 & Java & Meka & & & & & & Recipe & 2017 & Python & scikit - learn & & & & & & MLPlan & 2018 & Java & Weka and scikit - learn & & & & & & Hyperopt - sklearn & 2014 & Python & scikit - learn & & & & & & Autostacker & 2018 & - & - & & & & & & VDS & 2019 & - & - & & & & & & AlphaD3 M & 2018 & - & - & & & & & & OBOE & 2019 & Python & scikit - learn & & & & & & PMF & 2018 & Python & scikit - learn & & & & & & Distributed Frameworks * ATM : Framework Architecture ."
RF,radio frequency,TR-3875,"It is worth noting that communications through RISs is different compared with other related technologies currently employed in wireless networks , such as relaying , MIMO beamforming , passive reflect - arrays , and backscatter communications , while having the following major distinguishable features : i ) RISs are nearly passive , and , ideally , they do not need any dedicated energy source for RF signal processing ; ii ) RISs do not amplify or introduce noise when reflecting the signals and provide an inherently full - duplex transmission ; iii ) RISs can be easily deployed , e.g. , on the facades of buildings , ceilings of factories , and indoor spaces ; iv ) RISs are reconfigurable in order to adapt themselves according to the changes of the wireless environment ."
CNN,convolutional neural network,TR-3877,table[H ] tabularccccccc & 2c80 & 2c160 & 2c320 & GMBF & CNNf15 + 11 & GMBF & CNNf15 + 11 & GMBF & CNNf15 + 11 2 * 01 & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & 2*Average & & & & & & & & & & & & tabular Relative improvements over SRP - PHAT for the strategy invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with sequences 15 and 11 ( columns CNNf15 + 11 ) tab : baselineResults+ft15 + 11table
SFC,service function chaining,TR-4014,"However , we point out that , unlike our approach : ( i ) the focus of is on the traffic routing and scheduling between SDN - enabled switches per time - flow , so that the resulting flow scheduler does not support , by design , failure and fault tolerance per link and switch of data time - flow ; ( ii ) the joint flow and computing rate mapping afforded in is , by design , static ; ( iii ) the scheduler in does not perform real - time reconfiguration rerouting , real - time traffic hosted by the serving controller ; ( iv ) the work in does not consider SFC and rerouting ; and ( v ) the scheduler in does not enforce per - flow QoS guarantees on the limited time minimum energy and/or the minimum side - effect ."
FEC,forward error correction,TR-4111,The parameters are defined as follows : FEC - based : accounts for mechanisms that employ FEC ; ARQ - based : mark mechanisms that use ARQ ; QoE - sensitive data : this parameter demonstrates mechanisms that identity and/or considerate the video content to define the EC policy ; Video - aware : check mark is given to mechanisms that use any video characteristics to define the amount of redundancy and/or retransmission ; High - quality video : it is marked if the mechanisms are using videos equal or higher than 720p ( HD ready ) ; Network status : this parameter defines if the mechanisms use the information about the network healthy to define the redundant data ; UEP - enabled : means that different amounts of redundancy are being added to distinct portions of the video .
LC,latent class,TR-4133,"These LC models distinguishbetween conventional random utility and the lexicographic choice criterion ( dataset 1),among choice criteria with different reference points ( dataset 2),(note that standard random utility has no reference point)between standard random utility and the elimination - by - aspects choice criterion ( dataset 3),and between standard random utility and the random - regret choice criterion ( dataset 4).hess / etal:12-tFinally , Swait and Adamowicz show that consumers also fall into different ' decision strategy ' LCs , and that increasing either the complexity of the choice task or the cumulative task burden induces switching toward simpler decision strategies.swait/adamowicz:01-jcrThese results underscore an interpretation of the choice - criterion probabilities that is only implicit in the above - mentioned studies : that(a ) decision makers should not be characterized solely in terms of their modal choice criterion , but in terms of their choice - criterion mixtures , and that(b ) the criterion that is operative for a particular choice task is obtained as a draw from the probability distribution over choice criteria , which in turn is conditional upon features of the context , the framing and presentation of the choice options , and the current psychological characteristics of the decision maker ."
CP,completely positive,TR-4151,"The compact structure for is given by the doubles of the cups and caps of , while the adjoint of a process in the form of Diagram is given by first taking the adjoint in , and then using the following equation for the adjoint of the discarding map : Because the doubled processes and the discarding maps are well - defined CP maps , it is legitimate to rephrase the very definition of the CPM category by saying that its processes are exactly those in the following form : This means that doubled processes and discarding maps are enough to express all CP maps , but to prove results about CP maps we need a graphical axiom relating a generic CPM category to the corresponding original category ."
SGNS,syntactic symmetric pattern,TR-4155,1.01.0l XXContext Type & Training Time & Pairs ( lr)1 - 1 ( lr)2 - 2 ( lr)3 - 3BOW ( win=2 ) & 179mins 27s & 5.974GPOSIT ( win=2 ) & 190mins 12s & 5.974GCOORD ( conjlr ) & 4mins 11s & 129.69 M SP & 1mins 29s & 46.37 M DEPS - All & 103mins 35s & 3.165 G BEST - ADJ & 14mins 5s & 447.4 M BEST - VERBS & 29mins 48s & 828.55 M BEST - NOUNS & 41mins 14s & 1.063 G -0.3emTraining time ( wall - clock time reported ) in minutes for SGNS ( ) with different context types .
CF,collaborative filtering,TR-4307,"filtering ( sec : Generalizing via collaborative filtering ) History & History of the document & History & History Sports & Famous resident & Demographics & Career Awards & Communes without arms & Economy & Personal Life Medal Summary & Content and importance & Education & Honours Statistics & Player movement & Politics & Career Statistics tabulartable*figure*[Topic modeling ( sec : Using topic modeling ) ] FIG / baseline_topic fig : baseline_topic[Category - section counts ( sec : Using category - section counts ) ] FIG / count_based fig : count_based[Generalizing counts via CF ( sec : Generalizing via collaborative filtering ) ] FIG / cf_based fig : cf_based[Human evaluation ( sec : Evaluation by human experts ) ] FIG / human_eval fig : human_evalPrecision and recall as a function of the number of recommended sections , for all methods we evaluate ."
ADN,activity driven networks,TR-4355,"tikzpicture customlegend[legend columns=5,legend style = at=(0.12,1.02),draw = none , column sep=2ex , line width=2 pt , legend entries = Het GDT , Het ADN , 0.364 M , 0.50 M , 1.0 M ] dash dot , line legend , color = black solid , color = black solid , color = red solid , color = blue solid , color = green customlegend tikzpictureszensnt_a.pdf szensnt_b.pdfSensitivity of the graph model with large network sizes : A ) disease prevalence dynamics and B ) cumulative infection over simulation daysfig : netsznfigureTo understand the response of the graph model for larger scale simulation with large network sizes , the simulations are also run on various sizes of the networks ."
ST,split - turnip,TR-4500,"i & BDD - based Network Reliability & HLL & n / a & Hardy2007,Herrmann2010 4*ii & Lomonosov's - Turnip & LT & & Gertsbakh2016 2 - 5 & Sequential Splitting Monte Carlo & ST & , & Vaisman2016 2 - 5 & Generalized Splitting & GS & & Botev2012 2 - 5 & Recursive Variance Reduction & RVR & & Cancela2014 3*iii & Karger 's 2-step Algorithm & K2Simple & & Karger2016 2 - 5 iii & Optimal Monte Carlo Simulation & , & & Dagum2000 , Huber2017 2 - 5 & Counting - based Network Unreliability & & & This paper tabular tableTo the best of our knowledge , methods in Table t : methods are some of the best in their categories as evidenced in the literature ."
FEC,forward error correction,TR-4616,"tabularccccccc -1.5exPacket loss & -1.5exQoE & -1.5exWithout & & -1.5exVideo - aware FEC & & -1.5exViewFEC 1.0exrate & 1.0exMetric & 1.0exFEC & 1.5exVideo - aware FEC & 1.0exImprovement & 1.5exViewFEC & 1.0exImprovement 2*Packet loss 5 & VQM & 3.05 & 1.06 & 65.14 & 1.02 & 66.48 & SSIM & 0.76 & 0.91 & 19.74 & 0.92 & 21.05 2*Packet loss 10 & VQM & 4.01 & 1.11 & 72.36 & 1.12 & 72.09 & SSIM & 0.74 & 0.91 & 22.97 & 0.91 & 22.97 2*Packet loss 15 & VQM & 6.19 & 1.60 & 74.09 & 1.49 & 75.87 & SSIM & 0.50 & 0.90 & 80.00 & 0.89 & 78.00 2*Packet loss 20 & VQM & 8.68 & 1.77 & 79.60 & 1.81 & 79.14 & SSIM & 0.33 & 0.88 & 166.67 & 0.88 & 166.67 tabular tab : vfec : allpktloss center tableTaking into consideration the results of the experiments , it is possible to say that the proposed ViewFEC mechanism showed good performance ."
QA,question answering,TR-4717,"table[!tbh]minipage0.5tabularp2cmlll4cWikiMovies Dataset Perturbations & BLEU & QBLEU & Hit 1 ( ) Original & 100 & 100 & 76.5 Stop Words & 25.4 & 84.0 & 75.6 Relation Words & 29.4 & 64.3 & 54.7 Question Type & 74.0 & 79.3 & 73.5 NER & 41.9 & 48.5 & 17.97 tabularAccuracy reported across different types of questions for WikiMovies Datasetmachine_wikimoviesminipageminipage0.5table - format=-1.2tabularp2cmcccc5cSQuAD Dataset Perturbations & 1lBLEU & 1lQBLEU & 1lF1 & 1lEM Original & 100 & 100 & 76.5 & 66.5 NER & 77.0 & 76.7 & 73.8 & 54.0 Question Type & 80.1 & 70.3 & 69.0 & 59.7 Stop Words & 24.2 & 69.59 & 70.4 & 65.8 Relevant Words & 60.7 & 63.4 & 64.1 & 61.7 tabularAccuracy reported across different types of questions for SQuAD Datasetadv_squadminipageminipage0.5table - format=-1.2tabularp2cmccc4cVQA Dataset Perturbations & 1lBLEU & 1lQBLEU & 1lOverall ( ) Original & 100 & 100 & 64.4 Relevant Words & 82.6 & 78.8 & 60.21 Question Type & 63.7 & 66.36 & 59.81 Stop Words & 10.8 & 42.46 & 57.37 tabularAccuracy reported across different types of questions for VQA Datasetmachine_vqaminipagetable0 subtable0.5table - format=-1.2subtabletableQuestion Generation for Question Answering SystemsAs argued earlier , one important use case of automatically generating questions from text / images is to eventually use these generated questions to train a QA system ."
PC,principal component,TR-4841,"@X rrr rrr @ & 3 c[c ] PC 0 , , & 3c[c]PC 1 , , ( lr)2 - 4(lr)5 - 7 & coeff & p val & LMG & coeff & p val & LMG extraversion & & & 0.8 & & 0.03 & 0.38 openness & & 0.01 & 0.07 & & 0.5 & 0.08 neuroticism & & 0.7 & 0.07 & & 0.01 & 0.48 agreeableness & & 0.5 & 0.03 & & 1.0 & 0.01 conscientiousness & & 1.0 & 0.03 & & 0.6 & 0.04 T=30 , Extraversion , openness , and neuroticism explain spatial behaviour ."
IR,information retrieval,TR-4842,"arrows , shapes , backgroundstikzmarkcalcvertex=[ellipse , fill = black!25,minimum size=20pt , inner sep=0pt]edge = [ draw , thin,-]glabel = [ text width=1cm , text centered , font=]bg bg , main [ 1]switch ( # 1)switchcaseassert[1](#1)SE[SWITCH]SwitchEndSwitch[1 ] # 1 SE[CASE]CaseEndCase[1 ] # 1 * EndSwitch*EndCase[1 ] # 1e.gi.e[scale=0.4](0,.35 ) - ( .25,0 ) - ( 1,.7 ) - ( .25,.15 ) - cycle ; language = java , basicstyle = pcrblack , keywordstyle= , commentstyle= , numbers = none , numberstyle= , backgroundcolor= , showspaces = false , showstringspaces = false , showtabs = false , frame = single , tabsize=2 , rulesepcolor= , captionpos = b , breaklines = true , breakatwhitespace = false , failed to patchfailed to patch[Improving Bug Localization with Context - Aware Query Reformulation]Improving IR - Based Bug Localization with Context - Aware Query ReformulationMohammad Masudur RahmanUniversity of SaskatchewanSaskatoon , Canadamasud.rahman@usask.ca Chanchal K. RoyUniversity of SaskatchewanSaskatoon , Canadachanchal.roy@usask.caRecent findings suggest that Information Retrieval ( IR)-based bug localization techniques do not perform well if the bug report lacks rich structured information ( relevant program entity names ) ."
CNN,convolutional neural network,TR-4870,"table[H ] tabularcccccccccc & 3c80 & 3c160 & 3c320 & SRP & GMBF & CNN & SRP & GMBF & CNN & SRP & GMBF & CNN 2 * 01 & & & & & & & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & & & & & & & 2*Average & & & & & & & & & & & & & & & & & & tabular Baseline results for the SRP - PHAT strategy ( columns SRP ) ; the one invelasco2012-F ( columns GMBF ) , and the CNN trained with synthetic data without applying the fine - tuning procedure ( columns CNN ) for sequences 01 , 02 and 03 for different window sizes ."
LDA,linear discriminant analysis,TR-4903,"& Task&Segment & Feature Extraction&Classifier&Performance 4*Biased & 2*Abdullah2010 & 2 * 10 & 2 * 5 & 2*[2]weeks & 2 * 4 & EC & 2*[5]s & 2*AR & 2*ANN&CRR = [ 97.0 ] & & & & & & EO & & & & CRR = [ 96.0]2 - 11 & Riera2008&51&4 & 34 [ 74]days & 2 & EC & [ 4]s & AR , PSD , MuI , COH , CC & FDA & EER = [ 3.4 ] 2 - 11 & Su2010&40&2 & - & 1 & EC & [ 180]s & AR , PSD & KNN , LDA & CRR = [ 97.5 ] 8*Rigorous&Marcel2007&9&3&[3]days & 8 & MI & [ 1]s & PSD & MAP model & HTER = [ 19.3 ] 2 - 11 & Lee2013&4&2 & [ 10]days - [ 5]months & 1 & EC & [ 50]s & PSD & LDA & AC = [ 100 ] 2 - 11 & 2*Rocca2013 & 2 * 9 & 2 * 2 & 2 * 1 - [ 3]weeks & 3 & 2*EC & 2*[1]s & 2*AR & 2*Linear classier & CRR = [ 100 ] & & & & & 5 & & & & & CRR = [ 100 ] 2 - 11 & 2*Armstrong2015 & 15 & 2 & 5 - [ 40]days & 2 * 1 & 2*ERP & 2*[1.1]s & 2*Time - series & 2*CC & CRR = [ 89.0 ] & & 9 & 3 & 134 - [ 188]days & & & & & & CRR = [ 93.0 ] 2 - 11 & 2*Maiorana2016a & 2 * 50 & 2 * 3 & 2*Ave . ["
CPM,completely positive maps,TR-4937,"The compact structure for is given by the doubles of the cups and caps of , while the adjoint of a process in the form of Diagram is given by first taking the adjoint in , and then using the following equation for the adjoint of the discarding map : Because the doubled processes and the discarding maps are well - defined CP maps , it is legitimate to rephrase the very definition of the CPM category by saying that its processes are exactly those in the following form : This means that doubled processes and discarding maps are enough to express all CP maps , but to prove results about CP maps we need a graphical axiom relating a generic CPM category to the corresponding original category ."
PI,purchase intention,TR-4963,"L ) Magnitude of the correlation between trust and sales , ( M ) Magnitude of the correlation between trust and price , ( N ) Magnitude of the correlation between trust and history , ( O ) Magnitude of the correlation between PI and sales , ( P ) Magnitude of the correlation between price and PI , ( Q ) Magnitude of the correlation between history and PI , ( R ) Magnitude of the correlation between price and sales , ( S ) Magnitude of the correlation between history and sales , ( T ) Magnitude of the correlation between history and price ."
BF,basic feature,TR-5168,table*[ht]Accuracy of 's modules with different classifierstab : classifiertabularccccc2*Classifier Type & 4cAccuracy 2 - 5 & BF Multi - classifier & AF Multi - classifier & Profile Image CNN & Overall Decision Tree & 0.721 & 0.618 & 5 * 0.790 & 0.721 1 - 3 5 - 5 SVM & 0.739 & 0.352 & & 0.800 1 - 3 5 - 5 AdaBoost & 0.790 & 0.704 & & 0.850 1 - 3 5 - 5 GradientBoosting & 0.816 & 0.738 & & 0.842 1 - 3 5 - 5 Random Forest & 0.796 & 0.708 & & 0.899 tabulartable*table*[ht]performance with different feature setstab : featuretabularl c c c c c c c c c c2*Feature Set Description & 3cMale & 3cFemale & 3cBrand & 2*Acc 2 - 10 & R & P & F1 & R & P & F1 & R & P & F1 0 .
CA,cumulative activation,TR-5194,"To summarize , our contributions include : ( a ) we propose the seed minimization and influence maximization problem under cumulativeactivation ( SM - CA problem and IM - CA problem respectively ) , which is a reasonable model for purchasing behavior of customers exposed to repeated information cascades ; ( b)we design an approximate algorithm for SM - CA problem when ; ( c ) we show strong hardness results for SM - CA problem with and IM - CA problem ; ( d ) we propose efficient heuristic algorithms and validate them through extensive experiments on real - world datasets and conclude that one heuristic is the best choice for both SM - CA and IM - CA problems ."
IO,interacting object,TR-5231,"Additionally , the corresponding terms in eq:19 are defined as follows : 0em : Doppler shift for the th RIS : Doppler shift for the th plain IO : Constant phase shift for the th RIS : Constant phase shift for the th plain IO : Initial radio path distance for the th RIS : Initial radio path distance for the th plain IO : Adjustable phase shift of the th RISHere , the Doppler shifts of the RISs and plain IOs are not only dependent on the speed of the MS , but also on their relative positions with respect to the MS , i.e. , angles of arrival for the incoming signals : and , where and are the angles of arrival for the reflected signals of th RIS and th plain IO , respectively ."
CT,computed tomography,TR-5238,figure [ ht ] center minipage0.15 ./fig / snapshot0001new3.png minipage minipage0.15 ./fig / snapshot0011new3.png minipage minipage0.15 ./fig / snapshot0041new3.png minipage minipage0.15 ./fig / snapshot0091new3.png minipage minipage0.15 ./fig / snapshot0002new3.png minipage minipage0.15 ./fig / snapshot0012new3.png minipage minipage0.15 ./fig / snapshot0042new3.png minipage minipage0.15 ./fig / snapshot0092new3.png minipage minipage0.15 ./fig / snapshot0004new.png minipage minipage0.15 ./fig / snapshot0014new.png minipage minipage0.15 ./fig / snapshot0044new.png minipage minipage0.15 ./fig / snapshot0094new.png minipage minipage0.15 ./fig / snapshot0005new2.png minipage minipage0.15 ./fig / snapshot0015new2.png minipage minipage0.15 ./fig / snapshot0045new2.png minipage minipage0.15 ./fig / snapshot0095new2.png minipage minipage0.15 ./fig / snapshot0007new.png minipage minipage0.15 ./fig / snapshot0017new.png minipage minipage0.15 ./fig / snapshot0047new.png minipage minipage0.15 ./fig / snapshot0097new.png minipage minipage0.15 ./fig / snapshot0008new2.png minipage minipage0.15 ./fig / snapshot0018new2.png minipage minipage0.15 ./fig / snapshot0048new2.png minipage minipage0.15 ./fig / snapshot0098new2.png minipage center 5fig : vis1 Visualizations on four test CT images .
MAD,map attention decision,TR-5266,tab : design center tabularl c c c c Strategy & AR@100 & AR@S & AR@M & AR@L Baseline & 68.51 & 49.07 & 62.93 & 75.64 dyTrainScale & 72.58 & 53.41 & 66.34 & 78.83 equilibrium & 69.85 & 50.49 & 63.55 & 76.88 grayCls & 69.77 & 50.21 & 63.19 & 77.04 ZIP + all & 74.22 & 54.39 & 68.47 & 81.53 ZIP + all + MAD & 76.51 & 57.28 & 70.05 & 84.21 blackspatial MAD & 76.50 & 57.11 & 70.13 & 84.09 blackER vector & 71.87 & 51.84 & 65.49 & 76.21 tabular center tableTraining strategies .
RNN,random neural networks,TR-5736,"For all the given values , iterate through the RNN recurrent network learning algorithm till convergence , updating at each step the weights and , , so as to minimize the following error function : Dataset DescriptionHere we give a brief description of the four datasets used for evaluation:[leftmargin=*,labelsep=4.9mm]Iris dataset : Each instance is described by four plants attributes ( sepal length and width , and petal length and width ) all are real numbers and the task is to recognize which class of Iris plants ( Iris Setosa , Iris Versicolour , or Iris Virginica ) a given test instance belongs to ."
MAC,multiple access channels,TR-5813,"The transmitters send power - normalized versions of the estimation error , i.e. , aswhere is a power scaling factor , , and is a modulation coefficient chosen asThe user 's correlation coefficients can be written as , In a symmetric Gaussian MAC i.e. , and for all , the sum - capacity is achievable using F - MEC coding strategy , if satisfies and is the unique solution ofFeedback Effect Analysis of the K - user Symmetric Gaussian MACIn the following , utilizing the derivative of the MI we show that how F - MEC code affects the information rate of a K - user symmetric Gaussian MAC ."
FTE,foveal tilt effects,TR-5911,"the edge maps at fine to medium scales with the overlayed Hough lines are shown in [ fig : B5]Figs to [ fig : B7]. We show the mean tilt angles measured in the DoG edge maps across multiple scales in [ appendix : AppxC]Appendix C.figuresectionC. Quantitative mean tiltsThe absolute mean tilts and the standard errors of detected tilt angles for the Cafe Wall variations tested are provided here in Figs [ fig:]Figs and [ fig : C2]. For the ' foveal tilt effect ' ( FTE , explained in [ sec:3.1.1]Sections and [ sec:3.1.2]Section ) , we used the near horizontal mean tilts at scale 4 , and reflected these values to [ fig:6]Fig ."
US,uncertainty sampling,TR-6002,0.80tabularcccccccccccccc & 6cWhole abstract & 6cSentence & 4*Avg 2 - 13 & 3c2*CNN for CNN & 3cCNN for & 3c2*CNN for CNN & 3cCNN for & & 3c & 3cBiLSTM - CRF & 3c & 3cBiLSTM - CRF & 2 - 13 & Val & Test & Pseudo&Val & Test & Pseudo&Val & Test & Pseudo&Val & Test & Pseudo & EDG & 61.6 & 61.9 & 55.7 & 68.7 & 68.2 & 59.2 & 62.1 & 60.2 & 56.1 & 70.0 & 68.2 & 59.8 & 62.4EDGext1 ( w/o Val ) & 60.4 & 61.5 & 55.4 & 67.4 & 67.0 & 58.6 & 61.9 & 60.0 & 55.2 & 69.0 & 69.2 & 59.1 & 62.0 RND & 56.5 & 57.9 & 51.3 & 64.9 & 64.1 & 55.7 & 58.8 & 57.2 & 54.1 & 65.4 & 65.2 & 56.5 & 59.0Div & 57.9 & 57.8 & 54.2 & 64.4 & 64.6 & 57.2 & 59.3 & 58.4 & 53.8 & 66.2 & 65.7 & 57.0 & 59.7 US & 60.9 & 60.9 & 54.9 & 68.1 & 68.5 & 58.3 & 60.0 & 59.6 & 54.0 & 69.7 & 68.5 & 59.9 & 61.9US+Div & 61.1 & 60.4 & 56.4 & 65.4 & 66.8 & 58.3 & 60.5 & 58.9 & 55.4 & 70.1 & 68.1 & 59.3 & 61.7 US+Div+EDGext2 & 61.9 & 60.1 & 56.8 & 71.7 & 66.1 & 59.1 & 63.1 & 60.8 & 56.8 & 70.6 & 68.6 & 60.1 & 63.0 BALD & 61.0 & 61.4 & 56.4 & 68.9 & 69.4 & 58.8 & 61.0 & 59.7 & 55.1 & 70.9 & 67.8 & 60.1 & 62.5 BALD+EDGext2 & 61.3 & 60.0 & 56.9 & 69.4 & 66.5 & 59.5 & 65.0 & 62.2 & 55.9 & 72.4 & 68.7 & 59.7 & 63.1 tabularThe experiment setup is the same as Table tb : pseudo_exp except that the performances are the maximal micro - F1 ( ) of 5 neural networks rather than their average .
DR,dispersion reduction,TR-6053,1cAP(IoU=0.5 ) & 1cC.R.W2 2cbaseline ( SOTA)1 & 82.5 & 73.2 & 100 & 69.2 & 76.1 3*VGG-16 & MI - FGSM & 41 & 42.6 & 62 & 38.2 & 15.9 & DIM & 39 & 36.5 & 57 & 29.9 & 16.1 & DR ( Ours ) & 23 & 32.9 & 35 & 20.9 & 4.1 3*Resnet-152 & MI - FGSM & 37 & 41.0 & 61 & 40.4 & 17.4 & DIM & 49 & 46.7 & 60 & 34.2 & 15.1 & DR ( Ours ) & 25 & 33.3 & 31 & 34.6 & 9.5 tabulartablenotes [ 1 ] The baseline performance of GCV models can not be measured due to the mismatch between original labels and labels used by Google .
MSE,mean squared error,TR-6460,"equation*propositionproofSubstituting the tailored value of in Equation eqn : M_adaptiveUG for that in Equation eqn : nonInter_mse , we haveeqnarray & o = & 2dr^23(N)^2+d1+dk^2-d^2d(1+d ) ( ( 2+d2d)^d2 + 2d + 12(2d2+d)^2+d2 + 2d ) .eqn : augkm_mseeqnarrayTherefore , when is small , the MSE of the cluster centroids output by isproofcommentcommentfigure*[p ] tabularcc s1-performance.eps & s1-performance-closeup.eps ( a ) S1 dataset & ( b ) S1 dataset , close - up at large gowalla-2d-performance.eps & gowalla-2d-performance-closeup.eps ( c ) Gowalla 2D dataset & ( d ) Gowalla 2D dataset , close - up at large gowalla-3d-performance.eps & gowalla-3d-performance-closeup.eps ( e ) Gowalla 3D dataset & ( f ) Gowalla 3D dataset , close - up at large tabular The comparison of four algorithms proposed in this paper , by varying fig : exp - vary - epsilonfigure*figure*[p]minipage[b]0.45regression - all - in - one.epsThe linear regression for empirically finding the value.fig:regressionminipageminipage[b]0.45ds1.10-scalability.epsThe comparison of the UGkM and DPLloyd with varying dimensions.fig:exp-vary-N-dimensionminipagefigure*commentThe Hybrid Approachssec : hybridOur hybrid approach combines and ."
NEB,next event backtracking,TR-6509,"Plugging Equations eq : rhpdf to eq : stdphotonpdf and the sampler counts into the balance heuristic eq : misweight gives us the searched weights : align * w_e = p_e^*p_sum & & w_E = n_E p_E^*p_sum & & w_P , k = n_Pp_P , k^*p_sum & & w_LP , k = n_LPp_LP , k^*p_sumalign*equation with p_sum = p_e^ * + n_E p_E^ * + n_P _ k=1^p_P , k^ * + n_LP _ k=1^p_LP , k^ * eq : miswequationDensity Estimationsec : densityAn important point for the correctness and performance of the NEB operator is the estimate of the density ( required in Equation eq : flux ) ."
CF,collaborative filtering,TR-6538,"Similar to CF task , the CTR data likelihood is : Then the factorisation machine with logistic activation function is adopted to model the click probability over a specific ad impression : where is modelled by interactions among 3-side featuresDual - Task BridgeTo model the dependency between the two tasks , the weights of the user features and publisher features in CTR task are assumed to be generated from the counterparts in CF task ( as a prior):where is the assumed variance of the Gaussian generation process between each pair of feature weights of CF and CTR tasks and the weight generation is assumed to be independent across features ."
IR,information retrieval,TR-6663,"Our experiments reveal several interesting facts such as : ( a ) Segmentation is actively useful in improving IR performance , even though submitting all segments ( detected by an algorithm ) in double quotes to the IR engine degrades performance ; ( b ) All segmentation strategies , including human segmentations , are yet to reach the best achievable limits in IR performance ; ( c ) In terms of IR metrics , some of the segmentation algorithms perform as good as the best human annotator and better than the average / worst human annotator ; ( d ) Current match - based metrics for comparing query segmentation against human annotations are only weakly correlated with the IR - based metrics , and can not be used as a proxy for IR performance ; and ( e ) There is scope for improvement for the matching metrics that compare segmentations against human annotations by differentially penalizing the straddling , splitting and joining of reference segments ."
AIDA,"atomic , independent , declarative , and absolute",TR-6683,"My previous work introduced the concept of AIDA sentences , which are defined as English sentences that are : Atomic : a sentence describing one thought that can not be further broken down in a practical way ; Independent : a sentence that can stand on its own , without external references like "" this effect "" or "" we "" ; Declarative : a complete sentence ending with a full stop that could ( at least in theory ) be true or false ; and Absolute : a sentence describing the core of a claim ignoring the ( un)certainty about its truth and ignoring how it was discovered ( without phrases such as "" probably "" or "" evaluation showed that "" ) ."
CNN,convolutional neural network,TR-6743,0.80tabularcccccccccccccc & 6cWhole abstract & 6cSentence & 4*Avg 2 - 13 & 3c2*CNN for CNN & 3cCNN for & 3c2*CNN for CNN & 3cCNN for & & 3c & 3cBiLSTM - CRF & 3c & 3cBiLSTM - CRF & 2 - 13 & Val & Test & Pseudo&Val & Test & Pseudo&Val & Test & Pseudo&Val & Test & Pseudo & EDG & 61.6 & 61.9 & 55.7 & 68.7 & 68.2 & 59.2 & 62.1 & 60.2 & 56.1 & 70.0 & 68.2 & 59.8 & 62.4EDGext1 ( w/o Val ) & 60.4 & 61.5 & 55.4 & 67.4 & 67.0 & 58.6 & 61.9 & 60.0 & 55.2 & 69.0 & 69.2 & 59.1 & 62.0 RND & 56.5 & 57.9 & 51.3 & 64.9 & 64.1 & 55.7 & 58.8 & 57.2 & 54.1 & 65.4 & 65.2 & 56.5 & 59.0Div & 57.9 & 57.8 & 54.2 & 64.4 & 64.6 & 57.2 & 59.3 & 58.4 & 53.8 & 66.2 & 65.7 & 57.0 & 59.7 US & 60.9 & 60.9 & 54.9 & 68.1 & 68.5 & 58.3 & 60.0 & 59.6 & 54.0 & 69.7 & 68.5 & 59.9 & 61.9US+Div & 61.1 & 60.4 & 56.4 & 65.4 & 66.8 & 58.3 & 60.5 & 58.9 & 55.4 & 70.1 & 68.1 & 59.3 & 61.7 US+Div+EDGext2 & 61.9 & 60.1 & 56.8 & 71.7 & 66.1 & 59.1 & 63.1 & 60.8 & 56.8 & 70.6 & 68.6 & 60.1 & 63.0 BALD & 61.0 & 61.4 & 56.4 & 68.9 & 69.4 & 58.8 & 61.0 & 59.7 & 55.1 & 70.9 & 67.8 & 60.1 & 62.5 BALD+EDGext2 & 61.3 & 60.0 & 56.9 & 69.4 & 66.5 & 59.5 & 65.0 & 62.2 & 55.9 & 72.4 & 68.7 & 59.7 & 63.1 tabularThe experiment setup is the same as Table tb : pseudo_exp except that the performances are the maximal micro - F1 ( ) of 5 neural networks rather than their average .
MVP,mitral valve prolapse,TR-6815,"B. Variation in ECG patterns for leads I and V1 for patients highlighted in red ( B ) and yellow ( C ) in A. In each case , ECGs show substantial temporal changes in complex morphology , particularly in those features important for a PAH diagnosis ( Figure ) * [ ] [ ] Characteristics of Studies Used to Train Left Ventricular Mass Estimation Model * [ ] [ ] Characteristics of Studies Used to Train Left Atrial Volume Estimation Model * [ ] [ ] Characteristics of Studies Used to Train Mitral Valve Annular e ' Estimation Model * [ ] [ ] Characteristics of Studies Used to Train PAH Classification Model * [ ] [ ] Characteristics of Studies Used to Train HCM Classification Model * [ ] [ ] Characteristics of Studies Used to Train Amyloid Classification Model * [ ] [ ] Characteristics of Studies Used to Train MVP Classification Model * [ ] [ ] Characteristics of Studies Used to Train Left Ventricular Hypertrophy Classification Model * [ ] [ ] Characteristics of Studies Used to Train Diastolic Dysfunction Classification Model * [ ] [ ] Characteristics of Studies Used to Train Left Atrial Enlargement Classification Model * [ ] [ ] Variable Importance for GBM Models for Cardiac Structure and Function * [ ] [ ] Variable Importance for GBM Models for Disease Detection"
SP,strictly piecewise,TR-6913,table*[t]Accuracy on Target SP Stringsets Early Stoppingtab : resultsSPES4.5pttabularcccccccccc2c2Training & 2Test & 3cLSTM & 3cs - RNN & 2RPNI & & & 10 & 30 & 100 & 10 & 30 & 100 & 6SP2 & 21k & 1 & 0.871 ( 0.04 ) & 0.954 ( 0.05 ) & 0.992 ( 0.00 ) & 0.910 ( 0.05 ) & 0.994 ( 0.01 ) & 0.992 ( 0.01 ) & 1.000 & & 2 & 0.960 ( 0.03 ) & 0.989 ( 0.02 ) & 0.998 ( 0.00 ) & 0.976 ( 0.01 ) & 0.998 ( 0.01 ) & 1.000 ( 0.00 ) & 1.000 3 - 10 & 210k & 1 & 0.890 ( 0.07 ) & 0.941 ( 0.04 ) & 0.977 ( 0.02 ) & 0.995 ( 0.01 ) & 0.981 ( 0.05 ) & 0.999 ( 0.00 ) & 1.000 & & 2 & 0.979 ( 0.02 ) & 0.990 ( 0.01 ) & 0.994 ( 0.01 ) & 1.000 ( 0.00 ) & 0.984 ( 0.05 ) & 1.000 ( 0.00 ) & 1.000 3 - 10 & 2100k & 1 & 0.833 ( 0.14 ) & 0.819 ( 0.12 ) & 0.890 ( 0.08 ) & 0.997 ( 0.01 ) & 0.999 ( 0.00 ) & 0.997 ( 0.00 ) & 1.000 & & 2 & 0.838 ( 0.16 ) & 0.805 ( 0.13 ) & 0.872 ( 0.09 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 6SP4 & 21k & 1 & 0.881 ( 0.06 ) & 0.946 ( 0.04 ) & 0.963 ( 0.03 ) & 0.887 ( 0.05 ) & 0.966 ( 0.02 ) & 0.979 ( 0.01 ) & 1.000 & & 2 & 0.950 ( 0.03 ) & 0.960 ( 0.03 ) & 0.983 ( 0.01 ) & 0.883 ( 0.05 ) & 0.975 ( 0.01 ) & 0.979 ( 0.01 ) & 1.000 3 - 10 & 210k & 1 & 0.899 ( 0.11 ) & 0.958 ( 0.07 ) & 0.991 ( 0.01 ) & 0.935 ( 0.08 ) & 0.968 ( 0.04 ) & 0.999 ( 0.00 ) & 1.000 & & 2 & 0.926 ( 0.09 ) & 0.971 ( 0.05 ) & 0.991 ( 0.01 ) & 0.954 ( 0.07 ) & 0.984 ( 0.02 ) & 1.000 ( 0.00 ) & 1.000 3 - 10 & 2100k & 1 & 0.943 ( 0.08 ) & 0.940 ( 0.08 ) & 0.920 ( 0.06 ) & 0.942 ( 0.09 ) & 0.958 ( 0.09 ) & 0.973 ( 0.07 ) & 1.000 & & 2 & 0.928 ( 0.08 ) & 0.930 ( 0.09 ) & 0.911 ( 0.07 ) & 0.951 ( 0.09 ) & 0.962 ( 0.08 ) & 0.974 ( 0.08 ) & 1.000 6SP8 & 21k & 1 & 0.884 ( 0.02 ) & 0.884 ( 0.02 ) & 0.903 ( 0.02 ) & 0.861 ( 0.01 ) & 0.878 ( 0.02 ) & 0.857 ( 0.02 ) & 0.817 & & 2 & 0.733 ( 0.03 ) & 0.643 ( 0.06 ) & 0.688 ( 0.04 ) & 0.730 ( 0.01 ) & 0.681 ( 0.06 ) & 0.625 ( 0.04 ) & 0.587 3 - 10 & 210k & 1 & 0.934 ( 0.05 ) & 0.921 ( 0.05 ) & 0.959 ( 0.03 ) & 0.908 ( 0.02 ) & 0.952 ( 0.03 ) & 0.991 ( 0.00 ) & 0.873 & & 2 & 0.637 ( 0.08 ) & 0.659 ( 0.10 ) & 0.704 ( 0.11 ) & 0.600 ( 0.08 ) & 0.640 ( 0.10 ) & 0.837 ( 0.05 ) & 0.634 3 - 10 & 2100k & 1 & 0.977 ( 0.04 ) & 0.975 ( 0.04 ) & 0.980 ( 0.02 ) & 0.964 ( 0.05 ) & 0.990 ( 0.03 ) & 1.000 ( 0.00 ) & 1.000 & & 2 & 0.881 ( 0.11 ) & 0.865 ( 0.13 ) & 0.864 ( 0.08 ) & 0.890 ( 0.08 ) & 0.942 ( 0.09 ) & 0.984 ( 0.03 ) & 1.000 tabulartable *
WS,write skew,TR-7093,"We define a commutativity dependency graph which summarizes the happens - before dependencies in all executions of a given program between transactions as they appear in the program , transactions where the writes of arewrapfigurer5.5cmbasicstyle = subfigure55mm0.57tikzpicture [ shape = rectangle , draw = none , font= ] ( A ) at ( 0,0 ) [ ] ; [ shape = rectangle , draw = none , font= ] ( B ) at ( 0,1.5 ) [ ] ; [ shape = rectangle , draw = none , font= ] ( C ) at ( 4,0 ) [ ] ; [ shape = rectangle , draw = none , font= ] ( D ) at ( 4,1.5 ) [ ] ; [ shape = rectangle , draw = none , font= ] ( E ) at ( 8,0 ) [ ] ; [ shape = rectangle , draw = none , font= ] ( F ) at ( 8,1.5 ) [ ] ; scope [ every edge/.style = draw = black , very thick ] [ - > ] ( C ) edge [ bend left ] node [ above , font= ] ( B ) ; [ - > ] ( C ) edge [ bend left ] node [ above , font= ] ( F ) ; [ - > ] ( A ) edge [ bend left ] node [ above , font= ] ( B ) ; [ - > ] ( A ) edge [ bend left ] node [ above , font= ] ( F ) ; [ - > ] ( B ) edge [ bend left ] node [ above , font= ] ( C ) ; [ - > ] ( B ) edge [ bend left ] node [ above , font= ] ( A ) ; [ - > ] ( F ) edge [ bend left ] node [ above , font= ] ( C ) ; [ - > ] ( F ) edge [ bend left ] node [ above , font= ] ( A ) ; scopetikzpicturesubfigureCommutativity dependency graph of WS where the read of is omitted ."
MPI,message passing interface,TR-7327,"figure[!htb]subfigure.3 figures / Comparison_IO_compute_scaling_traj_splitting - chain - reader_edited.pdf format = hang Scaling for different components fig : MPIscaling - chain - readersubfiguresubfigure.3 figures / Comparison_tot_time_traj_splitting - chain - reader_edited.pdf Scaling total fig : MPItottime - chain - readersubfiguresubfigure.3 figures / Comparison_Speed_UP_traj_splitting - chain - reader_edited.pdf Speed - up fig : MPIspeedup - chain - readersubfiguresubfigure.45 figures / chain - reader - no - ga - BarPlot - rank - comparison_192_5.pdf format = hang Time comparison of different parts of the calculations per MPI rank using ChainReader with MPI collective communications fig : MPIranks - split - chain - readersubfiguresubfigure.45 figures / chain - reader - ga - BarPlot - rank - comparison_192_3.pdf format = hang Time comparison on different parts of the calculations per MPI rank using ChainReader using Global Arrays fig : MPIranks - split - ga - chain - readersubfigureComparison on the performance of the MDAnalysis ChainReader for the RMSD task on SDSC Comet when the trajectories are split ; for the communication step either collective MPI ( "" MPI "" ) or Global Arrays ( "" ga "" ) was used ."
CNN,convolutional neural network,TR-7354,0.85tabularcccccccccccccc & 6cWhole abstract & 6cSentence & 4*Avg 2 - 13 & 3c2*CNN for CNN & 3cCNN for & 3c2*CNN for CNN & 3cCNN for & & 3c & 3cBiLSTM - CRF & 3c & 3cBiLSTM - CRF & 2 - 13 & Val & Test & Pseudo&Val & Test & Pseudo&Val & Test & Pseudo&Val & Test & Pseudo & EDG & 61.0 & 59.2 & 54.9 & 66.5&66.8&58.7 & 60.1 & 58.2 & 54.7 & 68.2&66.5&59.1&61.2EDGext1 ( w/o Val ) & 59.6&58.5&54.9&65.7&65.5&57.7&60.2&58.4&52.5&67.7&67.8&58.5 & 60.6 RND&56.0&56.3&50.4&62.6&62.6&55.0&57.7&56.1&52.9&63.9 & 64.1&56.2 & 57.8 Div & 55.2 & 55.6 & 52.8 & 63.2 & 63.9&56.1 & 57.7 & 56.2 & 52.5 & 63.9 & 64.2 & 56.6 & 58.2 US&59.7 & 59.0 & 54.1 & 67.2 & 67.4 & 57.8 & 58.3&57.7&53.1 & 68.5 & 68.0 & 58.1 & 60.7US+Div & 60.1 & 59.5 & 56.0 & 64.1&65.7&57.9 & 60.1 & 56.8 & 54.5 & 68.7 & 67.4 & 58.5&60.8 US+Div+EDGext2 & 61.2 & 58.6 & 56.0&69.5 & 65.4 & 58.7 & 62.2 & 59.4 & 55.9 & 69.8 & 66.6 & 58.5 & 61.8 BALD & 59.8 & 59.4 & 55.5 & 67.4 & 67.7 & 58.3&59.9 & 58.8 & 54.9 & 69.7 & 67.3 & 59.5 & 61.5 BALD+EDGext2 & 60.7 & 58.2 & 56.0 & 68.4 & 65.8 & 58.9&63.7 & 60.0 & 54.9 & 71.7 & 67.6 & 59.2 & 62.1 tabularSimulation on pseudo labels for NCBI disease dataset .
AI,artificial intelligence,TR-7371,"author = von Ahn , Luis and Blum , Manuel and Hopper , Nicholas J. and Langford , John , booktitle = In Proceedings of Eurocrypt , citeulike - article - id = 3125288 , citeulike - linkout-0 = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.13.3335 , keywords = dos , security , pages = 294 - 311 , posted - at = 2008 - 08 - 15 07:49:40 , priority = 2 , title = CAPTCHA : Using Hard AI Problems For Security , url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.13.3335 , volume = 2656 , year = 2003@miscBack97hashcash , title = Hashcash , author = Back , Adam , year=1997@inproceedingsBaecher11 , title = Breaking reCAPTCHA : a holistic approach via shape recognition , author = Baecher , Paul and Buscher , Niklas and Fischlin , Marc and Milde , Benjamin , booktitle = IFIP International Information Security Conference , pages=56 - 67 , year=2011 , organization = Springer@articleBaird03 , title = PessimalPrint : a reverse Turing test , author = Baird , Henry S and Coates , Allison L and Fateman , Richard J , journal = International Journal on Document Analysis and Recognition , volume=5 , number=2 - 3 , pages=158 - 163 , year=2003 , publisher = Springer@articleBaird05 , author = Baird , H S and Bentley , J L , doi = 10.1117/12.590944 , issn = 0277786X , journal = Proceedings of SPIE - The International Society for Optical Engineering , keywords = Abuse of web sites and services , CAPTCHAs , Human interactive proofs , Implicit CAPTCHAs , Legibility , Usability , pages = 191 - 196 , title = Implicit CAPTCHAs , volume = 5676 , year = 2005@incollectionBaird06 , title = Complex image recognition and web security , author = Baird , Henry S , booktitle = Data Complexity in Pattern Recognition , pages=287 - 298 , year=2006 , publisher = Springer@miscBadBehaviour , author = Hampton , Michael , title = Bad Behaviour CAPTCHA , url = http://bad - behavior.ioerror.us/ , year=2016,@inproceedingsBergstra10 , title = Theano : A CPU and GPU math compiler in Python , author = Bergstra , James and Breuleux , Olivier and Bastien , Frederic and Lamblin , Pascal and Pascanu , Razvan and Desjardins , Guillaume and Turian , Joseph and Warde - Farley , David and Bengio , Yoshua , booktitle = Proc ."
ML,maximum likelihood,TR-7436,"While the choice of KNN and ML are obvious for subspaces formed by Kernel projections as noted in , we make a remark on choice of SRC as an additional classifier to measure efficacy of subspaces - this choice is motivated not only by the observation that SRC has emerged as a powerful classification approach for high dimensional remote sensing data and that it exploits the inherent sparsity when representing samples using training dictionaries , but also because popular solvers used ( e.g. Orthogonal Matching Pursuit , OMP ) are driven by inner products to learn the sparse representation and hence they essentially exploit angular information ."
NE,nash equilibrium,TR-7452,"When players use "" canonical co - evolutionary genetic algorithms "" as learning algorithms , the process of the game is an ergodic Markov Chain , and therefore we analyze simulation results using both the relevant methodology and more general statistical tests , to find that in the "" social "" case , states leading to NE play are highly frequent at the stationary distribution of the chain , in contrast to the "" individual learning "" case , when NE is not reached at all in our simulations ; to find that the expected Hamming distance of the states at the limiting distribution from the "" NE state "" is significantly smaller in the "" social "" than in the "" individual learning case "" ; to estimate the expected time that the "" social "" algorithms need to get to the "" NE state "" and verify their robustness and finally to show that a large fraction of the games played are indeed at the Nash Equilibrium ."
MRE,median recovery error,TR-7865,"WARNING : use overfitting instead of only ' memorization ' which is more ambiguous , ' verbatim ' seems to strongToDo list : look at distribution of attributes in the latent spaceIf we have time : Do a figure with auto - encoder ( if we manage to sample the latent space , for instance by fitting a Gaussian to the encoded images in the latent space)Show a picture of a few examples of Eiffel tower or london bridge in LSUN that seem to be memorize but are just samples learn from templateadd appendix with extra experiments ( a lot of images of recovery)show ( at least on some images ) that VGG , perceptual loss , L1 or L1 on Laplacian pyramid gives the same results for recovery because GLO reported some mitigated results on this ( sometimes L2 is better , sometimes pyramid : in the end , they mixed the two metrics)give statistics about the recovery precision with LBFGS : for a given * generated * target image , the distribution of errors using a lot of random initializations ( to demonstrate that the problem is almost convex , or at least ' easy ' to optimize ) show convergence speed vs SGD or other optimization method ( because it is was is generaly used in the literature : this finding makes it possible to experiment more easily ) , for a given distorted target generated image , the average / median error vs the distorsion : : you did that on training image but it would be nice to show it first on generated images , to show robustnessdiscuss difference LBFGS vs SGD : LBFGS is much faster to converge , not prone to gradient step setting , but can suffer from instabilitiesadd experiments on Auto Encoder : visual recovery results , histograms , MRE values , and maybe FID if we have time ( sampling from the latent space)add a small paragraph to explain experimental settings , i.e. that we reproduce several architectures from the literature and trained again on splitting , ... tell somewhere the difference ( I think in the intro ) between over - fitting and verbatim memorization , for which we provide a solid definition : add more experiments on LSUN : show histograms with PG - GAN without GAP , show recovery failure resultsadd experiments on MNIST , CIFAR ?"
MRE,median recovery error,TR-8003,"fig : LBFGS_visual_realfigure0 ToDo itemize look at distribution of attributes in the latent space If we have time : Do a figure with auto - encoder ( if we manage to sample the latent space , for instance by fitting a Gaussian to the encoded images in the latent space ) Show a picture of a few examples of Eiffel tower or london bridge in LSUN that seem to be memorize but are just samples learn from template show ( at least on some images ) that VGG , perceptual loss , L1 or L1 on Laplacian pyramid gives the same results for recovery because GLO reported some mitigated results on this ( sometimes L2 is better , sometimes pyramid : in the end , they mixed the two metrics ) give statistics about the recovery precision with LBFGS : itemize for a given * generated * target image , the distribution of errors using a lot of random initializations ( to demonstrate that the problem is almost convex , or at least ' easy ' to optimize ) show convergence speed vs SGD or other optimization method ( because it is was is generaly used in the literature : this finding makes it possible to experiment more easily ) , for a given distorted target generated image , the average / median error vs the distorsion : : you did that on training image but it would be nice to show it first on generated images , to show robustness itemize discuss difference LBFGS vs SGD : LBFGS is much faster to converge , not prone to gradient step setting , but can suffer from instabilities add experiments on Auto Encoder : visual recovery results , histograms , MRE values , and maybe FID if we have time ( sampling from the latent space ) add a small paragraph to explain experimental settings , i.e. that we reproduce several architectures from the literature and trained again on splitting , ... tell somewhere the difference ( I think in the intro ) between over - fitting and verbatim memorization , for which we provide a solid definition : add more experiments on LSUN : show histograms with PG - GAN without GAP , show recovery failure results add experiments on MNIST , CIFAR ?"
WS,write skew,TR-8005,"fig : rob0subfiguresubfigure73mm0.57tikzpicture [ shape = rectangle , draw = none , font= ] ( A ) at ( 0,0 ) [ ] [ r1 = y ; [ shape = rectangle , draw = none , font= ] ( C ) at ( 1.3,0 ) [ ] ; x = 1 ] ; [ shape = rectangle , draw = none , font= ] ( B ) at ( 5,0 ) [ ] [ r2 = x ; [ shape = rectangle , draw = none , font= ] ( D ) at ( 6.3,0 ) [ ] ; y = 1 ] ; scope [ every edge/.style = draw = red , very thick ] [ - > ] ( A ) edge [ bend left ] node [ above , font= ] conflict ( D ) ; [ - > ] ( B ) edge [ bend left ] node [ above , font= ] conflict ( C ) ; scopetikzpictureA WS execution trace ."
OCM,oz computation model,TR-8131,"In addition , the Ozy orchestration container also includes the following contributions : A new notion of partial activation as a dual to the existing notion of partial termination A technology neutral orchestration architecture inspired by the elements of a network - based architecture An open source orchestration container that implements the orchestration architecture ( 2 ) , implements an Oz language interpreter based on the OCM , and exploits the notions of partial activation ( 1 ) and partial termination to support a persistent execution stateThis paper describes the Ozy framework with special attention paid to the limitations of current approaches ."
MAP,maximum a posteriori,TR-8142,"& Task&Segment & Feature Extraction&Classifier&Performance 4*Biased & 2*Abdullah2010 & 2 * 10 & 2 * 5 & 2*[2]weeks & 2 * 4 & EC & 2*[5]s & 2*AR & 2*ANN&CRR = [ 97.0 ] & & & & & & EO & & & & CRR = [ 96.0]2 - 11 & Riera2008&51&4 & 34 [ 74]days & 2 & EC & [ 4]s & AR , PSD , MuI , COH , CC & FDA & EER = [ 3.4 ] 2 - 11 & Su2010&40&2 & - & 1 & EC & [ 180]s & AR , PSD & KNN , LDA & CRR = [ 97.5 ] 8*Rigorous&Marcel2007&9&3&[3]days & 8 & MI & [ 1]s & PSD & MAP model & HTER = [ 19.3 ] 2 - 11 & Lee2013&4&2 & [ 10]days - [ 5]months & 1 & EC & [ 50]s & PSD & LDA & AC = [ 100 ] 2 - 11 & 2*Rocca2013 & 2 * 9 & 2 * 2 & 2 * 1 - [ 3]weeks & 3 & 2*EC & 2*[1]s & 2*AR & 2*Linear classier & CRR = [ 100 ] & & & & & 5 & & & & & CRR = [ 100 ] 2 - 11 & 2*Armstrong2015 & 15 & 2 & 5 - [ 40]days & 2 * 1 & 2*ERP & 2*[1.1]s & 2*Time - series & 2*CC & CRR = [ 89.0 ] & & 9 & 3 & 134 - [ 188]days & & & & & & CRR = [ 93.0 ] 2 - 11 & 2*Maiorana2016a & 2 * 50 & 2 * 3 & 2*Ave . ["
RQ,reformulated queries,TR-8145,"Experimental ResultsWe first show the performance of our technique in terms of appropriate metrics ( RQ-(a ) ) , then discuss the impacts of different adopted parameters upon the performance ( RQ-(b ) ) , and finally show our comparison with the baseline queries ( RQ ) as follows : Performance of BLIZZARD in Bug Localization * = Significantly higher than baseline , Emboldened= Comparatively higher Selection of Baseline Queries , and Establishment of Baseline Technique and Baseline Performance : Existing studies suggest that text retrieval performances could be affected by query quality , underlying retrieval engine or even text preprocessing steps ."
ASA,adaptive segmentation algorithm,TR-8155,"In this section , we describe our iterative deep learning approach which can be described in five steps : 1 ) we train a deep learning model ( Unet ) on EDF images , and their corresponding ASA accepted masks that match manual annotation images ; 2 ) A prediction was made on EDF images of ASA rejected masks that do not match manual annotation , and we refer to this set of images as the "" active set "" ; 3 ) Another set that does not overlap with either the training nor the active set is the "" test set "" which contains EDF images of different sections of a unique mouse ( mouse i d 17 ) ; 4 ) The results of testing a trained deep learning model on the active set were verified by the user by comparing the predicted mask and manual annotation similarity ( as described in the previous paragraph ) ."
TS,thompson sampling,TR-8236,figure*[!t ] subfigure[b]0.2455 results / cascade / regret_heart - disease_CEMD - TS_10000_100.png alg : CSMD_TS fig : heartCMED - TS subfigure subfigure[b]0.2455 results / cascade / regret_heart - disease_CEMD - KL_10000_100.png alg : CSMD_KL ( with ) fig : heartCMED - KL subfigure subfigure[b]0.2455 results / cascade / regret_diabetes_CEMD - TS_10000_100.png alg : CSMD_TS fig : diabetesCMED - TS subfigure subfigure[b]0.2455 results / cascade / regret_diabetes_CEMD - KL_10000_100.png alg : CSMD_KL ( with ) fig : diabetesCMED - KL subfigure Cumulative regret of alg : CSMD_TS and alg : CSMD_KL with for different problem instances of Heart Disease dataset ( Fig .
ICP,iterative closest point,TR-8320,"tb ] Left : Recovery rate ( colorbar ) vs. missing data ratio ( y - axis ) vs. SNR ( x - axis ) , Middle : Recovery rate ( colorbar ) vs. outlier ratio ( y - axis ) vs. SNR ( x - axis ) , Right : Recovery rate ( y - axis ) vs. outlier ratio ( x - axis ) , blue : rRWOC , red : ICP , yellow : randomized rRWOC ( ) , purple : randomized rRWOC ( ) Randomized approximation algorithm for The exhaustive approach for the dimensional case requires -subset comparisons of , in order to guarantee hitting correct ( in the noiseless case ) or approximately correct ( in the noisy case ) regression coefficients , with complexity ."
ML,machine learning,TR-8334,"htpb]Notation and symbols used in this paper.-5pxc>[HTML]fcfcf4lLightCyanNotations & 1cDescription [ HTML]E8E8AB & Input data ( unmodified data ) , , [ HTML]E8E8AB & Label of class in the classification problem , [ HTML]E8E8AB & Adversarial example ( modified input data)[HTML]E8E8AB & Label of adversarial class in target adversarial example [ HTML]E8E8AB & Number of features , , [ HTML]E8E8AB & ML model , [ HTML]E8E8AB & Parameters of ML model [ HTML]E8E8AB & Percentage of features[HTML]E8E8AB & Number of poison samples-15pxAttack strategy and scenariosThe attack strategy defines how the attacker compromises the system , based on the hypothesized goal , knowledge , and capabilities ."
SVM,support vector machine,TR-8416,"In fact , the interval [ 0 5 [ is divided into two groups : [ 0 2 [ and [ 2 5 [ , and two distinct sub - conditions are thus considered : Sub - condition 1 : The embedding is performed by setting to 1 for [ 0 2 [ and 0.5 for [ 2 5[. Sub - condition 2 : The embedding is performed by setting to 0.5 for [ 0 2 [ and 0.1 for [ 2 5[.The accuracy is not increased by more than 1 in all conditions when SVM is used , while the opposite for NN classifier ."
EI,epidemic intelligence,TR-8464,"tabularclll 1cID & 1lDisease & 1lCountry & 1lEvent period in 2011 1 & Anthrax & Bangladesh & [ June - August ] 2 & Botulism & France & September 3 & Cholera & Kenya & [ November - December ] 4 & Escherichia Coli & Germany & [ May - July ] 5 & Mumps & Canada & [ June - August ] tabular tab : OutbreakListtableOur goal is to provide a broader EI system , able to detect and monitor outbreak events in Twitter , for multiple locations and for multiple diseases , including sudden and unexpected outbreaks ( cf ."
HDT,header dictionary triple,TR-8484,tab : compareredland 0.6 tabularlrrrrrrrrrr & & & & & 3cSpeedup6 - 8 data set & 1c Redland & 1cMentok & 1cHDT&1cTripleID & 1cRedland & 1c Mentok & 1c HDT & & & & & TripleID & TripleID & TripleID01 & 6.29 & 0.59 & 0.16 & 0.13 & 48.38 & 4.57 & 1.230103 & 28.22 & 1.82&0.43&0.20 & 141.10 & 7.26 & 2.150203 & 21.31 & 1.16 & 0.37 & 0.23 & 92.65 & 5.06 & 1.610207 & 11.55 & 0.70 & 0.19&0.12 & 96.25 & 5.86 & 1.58012347 & 36.98 & 1.90 & 0.69&0.18 & 205.44 & 10.55 & 3.83BTC - small & 35.39 & N / A & 0.79 & 0.09 & 393.27 & N / A & 8.78tabulartable The speedup of querying using TripleID over Redland is significant which is about 48 - 390 times faster .
RS,rate - selective,TR-8532,"Substituting Eq : MGF_end_Final and Eq : OP_FSDF_Difm to Eq : MGF_SSDF , a closed - form expression for of rate - selective RS over INID Nakagami- fading with integer 's and distinct 's is derived aswhile , substituting Eq : MGF_end_Final_Equalm and Eq : OP_FSDF_Equalm to Eq : MGF_SSDF , yields the following closed - form expression for of rate - selective RS over IID Nakagami- fading channels with integer : Performance Analysis of ODF Relaying SchemesIn this section , the performance of ODF relaying schemes with repetitive and RS - based transmission over INID Nakagami- fading channels will be analyzed ."
CNN,convolutional neural network,TR-8673,table[H ] tabularccccccc & 2c80 & 2c160 & 2c320 & GMBF & CNNf15 & GMBF & CNNf15 & GMBF & CNNf15 2 * 01 & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & 2 * 03 & & & & & & & & & & - & & - 2*Average & & & & & & & & & & & & tabular Results for the stratgy invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with sequence 15 ( columns CNNf15 ) .
RRC,rank residual constraint,TR-8787,"a ) Original image ; ( b ) Noisy image ; ( c ) NNM ( PSNR = 21.03dB , SSIM = 0.5996 ) ; ( d ) BM3D ( PSNR = 22.52dB , SSIM = 0.7021 ) ; ( e ) LSSC ( PSNR = 22.24dB , SSIM = 0.6999 ) ; ( f ) EPLL ( PSNR = 22.24dB , SSIM = 0.6771 ) ; ( g ) Plow ( PSNR = 21.83dB , SSIM = 0.6102 ) ; ( h ) NCSR ( PSNR = 22.10dB , SSIM = 0.7109 ) ; ( i ) GID ( PSNR = 20.73dB , SSIM = 0.6361 ) ; ( j ) PGPD ( PSNR = 22.56dB , SSIM = 0.7029 ) ; ( k ) LINC ( PSNR = 22.10dB , SSIM = 0.7037 ) ; ( l ) aGMM ( PSNR = 22.42dB , SSIM = 0.6823 ) ; ( m ) OGLR ( PSNR = 21.87dB , SSIM = 0.6419 ) ; ( n ) RRC ( PSNR = 22.76dB , SSIM = 0.7312 ) ."
FEC,forward error correction,TR-8790,"The information is collected by the receiver and sent to the transmitter ; Loss Rate Prediction - Using the feedback statistics , the properties of the error probability are estimated on the server side ; Video Characteristics - This module fetches information from the video sequences that are being transmitted to identify video characteristics such as the frame type and size , as well as the motion vectors ; Ant Colony Optimization - The ACO is responsible for making a joint analysis of all the information gathered by the other modules , establishing the most suitable amount of redundancy to each FEC block ; FEC Blocks - The FEC blocks are built and a specific amount of redundancy designed by the ACO is assigned to each one ."
SO,smart object,TR-8900,"table[H ] tabularccc Scenario & Simple & Complex & queries & queries Consumer SO 2 & 0.18 ms & 0.44 ms tabularVarying query complexity - Consumer SO tab : lev2tab3tabletable * tabularccccccccccc Queries & Q1 & Q2 & Q3 & Q4 & Q5 & Q6 & Q7 & Q8 & Q9 & Q10 Query selectivity & 3,5 & 1,84 & 0,85 & 0,72 & 0,55 & 0,37 & 0,28 & 0,2 & 0,15 & 0,13 Extra bits per output tuple & 98 & 140 & 210 & 245 & 294 & 336 & 392 & 455 & 483 & 546 Bandwidth overload per hour & 3440 & 2576 & 1785 & 1764 & 1617 & 1243 & 1097 & 910 & 724 & 709 tabularVarying query complexity - SO network - bandwidth overhead tab : bandwidthtable*Smart Object network : We simulate a smart object network via a Streambase query , where each single Streambase operator acts as a smart object ."
PSO,particle swarm optimization,TR-8988,"Let the position , the velocity and the learning exemplars of the particle be , * * Based on the proposed learning approach ( as shown in Algorithm- ) , the velocity of the particle is updated as follows : Determination of the cardinality of all sets ( and ): * * Evaluation of the cardinality learning sets ( , and ): * * Evaluation of the feature learning sets ( , and ): * * Derivation of the final learning sets ( , and ) * * Velocity update of the particle ( ) as per ( ) * Position UpdateIn the majority of binary PSO variants , the position update process entails the decision to exclude or include each feature ."
HDT,header dictionary triple,TR-8993,"tab : entail tabular[width=]llcccccccccccData & Rule & Res1 & Dist1 & Res2 & Dist2 & All & 1c TripleID & HDT & Stardog & Virtuoso & MySQL & TID / C 012347 & R2 & 8,395 & 2,437 & 226,433 & 169 & 169,776 & 18.09 & 34.15&764.95 & 3,073.12 & 4,402.72 & 53.45 & R3 & 9,589 & 2,505 & 226,099 & 186 & 62,005 & 2.46&30.85&740.23 & 752.29 & 4,904.71 & 35.41 & R5 & 6,545 & 450 & 0 & 0 & 0 & 0.28 & 0.53 & 0.19&0.23 & 4,177.85 & 6.7 & R7 & 6,545 & 1,120 & 32,433 & 95 & 22,855 & 0.55&38.39 & 128.88 & 1,776.76 & 6.18 & 20.87 & R9 & 10 & 4 & 1 & 1 & 1 & 0.19 & 7.03&200.72 & 0.25 & 3.36 & 0.03 & R11 & 26,785 & 4,716 & 87 & 47 & 90 & 1.24 & 0.65&2.42&11.99 & 53.06 & 69.92 btc - small & R2 & 10,185 & 3,596 & 301,680 & 205 & 219,698 & 23.08 & 49.87&509.12&4,485.87 & 7,416.90 & 88.08 & R3 & 11,438 & 3,592 & 305,591 & 210 & 89,372 & 3.53 & 45.61 & 455.42 & 913.18 & 8,125.93 & 77.38 & R5 & 7,980 & 584 & 0 & 0 & 0 & 0.39 & 0.68&0.76 & 0.22 & 7,098.16 & 11.58 & R7 & 7,980 & 1,496 & 57,884 & 100 & 40,622 & 1.00 & 53.39&442.34 & 1,907.66 & 8.65 & 28.32 & R9 & 10 & 4 & 1 & 1 & 1 & 0.21 & 9.04&657.43 & 0.05 & 4.55 & 0.26 & R11 & 36,561 & 6,739 & 91 & 49 & 98 & 3.06 & 0.85&3.72 & 6.22 & 97.22 & 124.58 tabular table*In Table tab : entail , after eliminating redundant results from the first GPU search ( Column "" Res1 "" ) , size is much smaller ."
CRF,conditional random field,TR-9146,a ) FCN-32 ; ( b ) FCN-16s ; ( c ) ResNet - DUC ; ( d ) E - Net ; ( e ) SegNet ; ( f ) U - Net ; ( g ) FCN-8s ; ( h ) CWGAN - GP ; ( i ) FC - DenseNet ; ( j ) DSFE - CRF ; ( k ) DSFE - GCN ; ( l ) DSFE - GraphSAGE ; ( m ) DSFE - GGNN ; ( n ) DSFE - GGCN ; ( o ) Ground truth ; ( p ) Optical image .
SE,small enough,TR-9182,table[ht]tabularlllrrr Data & & Method & & SE ( ) & LR(c ) & - & True & -14247 & 0 & 71 100D & 10 & PPS(1 ) & -14236 & 11.8 & 23 & & PPS(2 ) & -14244 & 13.6 & 67 & & SRS & -14234 & 2197.8 & 70 2 - 6 & 100 & PPS(1 ) & -14248 & 4.3 & 67 & & PPS(2 ) & -14249 & 4.5 & 73 & & SRS & -13823 & 598.8 & 60 2 - 6 & 1000 & PPS(1 ) & -14245 & 1.5 & 69 & & PPS(2 ) & -14248 & 1.5 & 71 & & SRS & -14068 & 212.9 & 67 LR(c ) & - & True & -14272 & 0 & 71 10D & 10 & PPS(1 ) & -14272 & 3.2 & 88 & & PPS(2 ) & -14269 & 3.6 & 81 & & SRS & -18096 & 3310.2 & 105 2 - 6 & 100 & PPS(1 ) & -14272 & 1.2 & 87 & & PPS(2 ) & -14272 & 1.1 & 78 & & SRS & -13921 & 669.9 & 67 2 - 6 & 1000 & PPS(1 ) & -14272 & 0.4 & 75 & & PPS(2 ) & -14272 & 0.4 & 69 & & SRS & -14266 & 223.3 & 71 tabularEffect of subsampling proportional to log predictive density .
LSU,louisiana state university,TR-9241,figure[!htb ] subfigure.4 figures / main - RMSD - t_total - SuperMIC.pdf Scaling total fig : MPIscaling - SuperMIC subfigure subfigure.4 figures / main - RMSD - speed_up - SuperMIC.pdf Speed - up fig : MPIspeedup - SuperMIC subfigure subfigure.4 figures / main - RMSD - time_comp_IO_comparison - SuperMIC.pdf format = hang Scaling for different components fig : ScalingComputeIO - SuperMIC subfigure subfigure .5 figures / main - RMSD - BarPlot - rank - comparison_80_5-SuperMIC.pdf format = hang Time comparison on different parts of the calculations per MPI rank ( example ) fig : MPIranks - SuperMIC subfigure LSU SuperMIC : Performance of the RMSD task with MPI .
GA,global arrays,TR-9286,sidewaystable[hp]adjustboxmax width = tabularc c c c c c c c c c c c 10r ( r)5 - 12 Cluster & Gather & File Access & Time & Serial & tabularc Comet : 24 Bridges : 24 SuperMIC : 20 tabular & tabularc Comet : 48 Bridges : 48 SuperMIC : 40 tabular & tabularc Comet : 72 Bridges : 60 SuperMIC : 80 tabular & tabularc Comet : 96 Bridges : 78 tabular & tabularc Comet : 144 Bridges : 84 SuperMIC : 160tabular & Comet : 192 & tabularc Comet : 384 SuperMIC : 320tabular Comet & MPI & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - & - & - Bridges & MPI & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - & - & - SuperMIC & MPI & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & - Comet & GA & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - & - & - Comet & MPI & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - SuperMIC & MPI & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & - Comet & GA & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - SuperMIC & GA & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & - Comet & MPI & PHDF5 & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular Bridges & MPI & PHDF5 & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - SuperMIC & MPI & PHDF5 & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & tabularc tabular tabularadjustboxComparison of the compute and I / O scaling for different test cases and number of processes .
SE,signed error,TR-9339,"C > p6.2emg>[RGB]195 , 195 , 195Ck > GrayC*[htbp ] The SE ( ) , AE ( ) and HD ( ) calculated using the results of different methods ( Dufour 's method , OCTRMIA3D and GDM ) and the ground truth manual segmentation , for the IS - OS ( ) surface in each of the 10 OCT volumesC > p6.2emg>[RGB]195 , 195 , 195Ck > GrayC*[htbp ] The OSE ( ) , OAE ( ) and OHD ( ) calculated using the results of different methods ( Dufour 's method , OCTRMIA3D and GDM ) and the ground truth manual segmentation , for overall retina surfaces in each of the 10 OCT volumes[h ! ]"
GPA,graph partition algorithm,TR-9374,"table[t]Datasets.tbl : exp - datatabularlcrrr Dataset & Category & Nodes & Edges & Labels Enron[1 ] & email & 36,692 & 183,831 & 0 GRQC[2 ] & collaboration & 5,242 & 14,496 & 0 Blog[3 ] & social & 10,312 & 333,983 & 39 Wiki[4 ] & word & 4,777 & 184,812 & 40 tabulartable[1]http://www.cs.cmu.edu / enron[2]http://snap.stanford.edu / data / ca - GrQc.html[3]http://socialcomputing.asu.edu / datasets / BlogCatalog[4]www.mattmahoney.net / dc / textdataExperimental Evaluations sec : experimentstable*[t]Precisions in the task of link prediction evaluated by Cosine similarity and Euclidean similarity.tbl:exp-link-predictiontabularcccccccccc 2*Algorithm & 2*Initialization & 4cCosine Similarity & 4cEuclidean Similarity 3 - 10 & & Enron & GRQC & Blog & Wiki & Enron & GRQC & Blog & Wiki 3*node2vec & GPA & 0.9579 & 0.9933 & 0.9816 & 0.9325 & 0.9665 & 0.9947 & 0.9887 & 0.9438 2 - 10 & HARP & 0.9209 & 0.9621 & 0.9708 & 0.9210 & 0.9418 & 0.9846 & 0.9618 & 0.9258 2 - 10 & Random & 0.9136 & 0.9533 & 0.9631 & 0.9117 & 0.9309 & 0.9817 & 0.9587 & 0.9217 3*DeepWalk & GPA & 0.9702 & 0.9937 & 0.9820 & 0.9315 & 0.9691 & 0.9958 & 0.9879 & 0.9411 2 - 10 & HARP & 0.9352 & 0.9625 & 0.9717 & 0.9178 & 0.9449 & 0.9842 & 0.9658 & 0.9354 2 - 10 & Random & 0.9218 & 0.9430 & 0.9535 & 0.9024 & 0.9355 & 0.9764 & 0.9517 & 0.9276 3*LINE & GPA & 0.7849 & 0.9852 & 0.9436 & 0.8175 & 0.5790 & 0.9665 & 0.9274 & 0.8356 2 - 10 & HARP & 0.7484 & 0.9526 & 0.9298 & 0.7849 & 0.5372 & 0.9471 & 0.9016 & 0.8126 2 - 10 & Random & 0.7414 & 0.9411 & 0.9127 & 0.7658 & 0.5237 & 0.9392 & 0.8836 & 0.8028 tabulartable*In this section , we demonstrate that the proposed graph partition based algorithm , dubbed as GPA , outperforms the state - of - the - art , i.e. , HARPhbys18 , as well as the randomized method , denoted by Random , on various datasets and over different tasks , such as link prediction and node classification ."
MAP,mean average precision,TR-9426,table*[t]0.7 - 0.0emtabularxl l cc ccc ccc & & 3cENNL & 3cENIT & 2cENFI ( lr)3 - 5 ( lr)6 - 8 ( lr)9 - 10CL Embs & Model & 2001 & 2002 & 2003 & 2001 & 2002 & 2003 & 2002 & 2003 - & LM - UNI & .119 & .196 & .136 & .085 & .167 & .137 & .111 & .142 & BWE - Agg - Add & .111 & .138 & .137 & .087 & .114 & .147 & .026 & .084 & BWE - Agg - IDF & .144 & .203 & .189 & .127 & .157 & .188 & .082 & .125 CL - CD & TbT - QT & .125 & .196 & .120 & .106 & .148 & .143 & .176 & .140 2 - 10 & Ensemble ( ) & .145 & .216 & .174 & .120 & .183 & .216 & .179 & .189 & Ensemble ( ) & .142 & .216 & .180 & .127 & .180 & .207 & .183 & .197 & BWE - Agg - Add & .149 & .168 & .203 & .138 & .155 & .236 & .078 & .217 & BWE - Agg - IDF & .185 & .196 & .243 & .169 & .166 & .248 & .086 & .204 CL - WT & TbT - QT & .159 & .164 & .176 & .129 & .150 & .218 & .095 & .095 2 - 10 & Ensemble ( ) & .202 & .198 & .280 & .187 & .168 & .228 & .117 & .190 & Ensemble ( ) & .202 & .198 & .263 & .181 & .171 & .230 & .120 & .164 & BWE - Agg - Add & .125 & .153 & .198 & .119 & .126 & .213 & .078 & .239 & BWE - Agg - IDF & .172 & .204 & .250 & .157 & .161 & .253 & .102 & .223 CL - UNSUP & TbT - QT & .229 & .257 & .299 & .232 & .257 & .345 & .145 & .243 2 - 10 & Ensemble ( ) & .258 & .300 & .330 & .225 & .248 & .325 & .154 & .307 & Ensemble ( ) & .259 & .303 & .336 & .236 & .253 & .347 & .151 & .307 tabularxCLIR performance on all three test language pairs for all models in comparison ( MAP scores reported ) .
HD,hausdorff distance,TR-9451,"tabularlllllll & & & LMSTAPLE & COLLATE & MV & & & & LocalMAPSTAPLE & COLLATE & LMS7 & DM & 92.62.4 & 91.73.0 & 87.34.5 & 85.15.3 & 83.87.3 & 82.39.0 HD & 7.42.6 & 8.23.3 & 9.84.8 & 12.06.2 & 13.97.4 & 14.78.2 & & - & & & & tabulartab : SegRes1table figure*[t]minipage[b]0.315figure = GT_Med_OurAll.eps , height=4cm , width=4 cm ( a)minipageminipage[b]0.315figure = GT_Med_Our.eps , height=4cm , width=4 cm ( b)minipageminipage[b]0.315figure = GT_Med_LMStaple.eps , height=4cm , width=4 cm ( c)minipageminipage[b]0.315figure = GT_Med_MStaple.eps , height=4cm , width=4 cm ( d)minipageminipage[b]0.315figure = GT_Med_Staple.eps , height=4cm , width=4 cm ( e)minipageminipage[b]0.315figure = GT_Med_wSSL.eps , height=4cm , width=4 cm ( f)minipageThe predicted ground truth for UCL Patient 23 by different methods : ( a ) ; ( b ) ; ( c)LocalMAPSTAPLE ; ( d)COLLATE ; ( e)LMS7 ; and ( f ) ."
LDA,linear discriminant analysis,TR-9627,"& Task&Segment & Feature Extraction&Classifier&Performance 4*Biased & 2*Abdullah2010 & 2 * 10 & 2 * 5 & 2*[2]weeks & 2 * 4 & EC & 2*[5]s & 2*AR & 2*ANN&CRR = [ 97.0 ] & & & & & & EO & & & & CRR = [ 96.0]2 - 11 & Riera2008&51&4 & 34 [ 74]days & 2 & EC & [ 4]s & AR , PSD , MuI , COH , CC & FDA & EER = [ 3.4 ] 2 - 11 & Su2010&40&2 & - & 1 & EC & [ 180]s & AR , PSD & KNN , LDA & CRR = [ 97.5 ] 8*Rigorous&Marcel2007&9&3&[3]days & 8 & MI & [ 1]s & PSD & MAP model & HTER = [ 19.3 ] 2 - 11 & Lee2013&4&2 & [ 10]days - [ 5]months & 1 & EC & [ 50]s & PSD & LDA & AC = [ 100 ] 2 - 11 & 2*Rocca2013 & 2 * 9 & 2 * 2 & 2 * 1 - [ 3]weeks & 3 & 2*EC & 2*[1]s & 2*AR & 2*Linear classier & CRR = [ 100 ] & & & & & 5 & & & & & CRR = [ 100 ] 2 - 11 & 2*Armstrong2015 & 15 & 2 & 5 - [ 40]days & 2 * 1 & 2*ERP & 2*[1.1]s & 2*Time - series & 2*CC & CRR = [ 89.0 ] & & 9 & 3 & 134 - [ 188]days & & & & & & CRR = [ 93.0 ] 2 - 11 & 2*Maiorana2016a & 2 * 50 & 2 * 3 & 2*Ave . ["
FDA,fisher 's discriminant analysis,TR-9709,"& Task&Segment & Feature Extraction&Classifier&Performance 4*Biased & 2*Abdullah2010 & 2 * 10 & 2 * 5 & 2*[2]weeks & 2 * 4 & EC & 2*[5]s & 2*AR & 2*ANN&CRR = [ 97.0 ] & & & & & & EO & & & & CRR = [ 96.0]2 - 11 & Riera2008&51&4 & 34 [ 74]days & 2 & EC & [ 4]s & AR , PSD , MuI , COH , CC & FDA & EER = [ 3.4 ] 2 - 11 & Su2010&40&2 & - & 1 & EC & [ 180]s & AR , PSD & KNN , LDA & CRR = [ 97.5 ] 8*Rigorous&Marcel2007&9&3&[3]days & 8 & MI & [ 1]s & PSD & MAP model & HTER = [ 19.3 ] 2 - 11 & Lee2013&4&2 & [ 10]days - [ 5]months & 1 & EC & [ 50]s & PSD & LDA & AC = [ 100 ] 2 - 11 & 2*Rocca2013 & 2 * 9 & 2 * 2 & 2 * 1 - [ 3]weeks & 3 & 2*EC & 2*[1]s & 2*AR & 2*Linear classier & CRR = [ 100 ] & & & & & 5 & & & & & CRR = [ 100 ] 2 - 11 & 2*Armstrong2015 & 15 & 2 & 5 - [ 40]days & 2 * 1 & 2*ERP & 2*[1.1]s & 2*Time - series & 2*CC & CRR = [ 89.0 ] & & 9 & 3 & 134 - [ 188]days & & & & & & CRR = [ 93.0 ] 2 - 11 & 2*Maiorana2016a & 2 * 50 & 2 * 3 & 2*Ave . ["
ILP,integer linear programming,TR-9742,"We are now able to complete the Schnyder orders ILP by adding : & & s_e & _ i=0 ^ 2 a^i_e , v & & e E , v e & & a^i_e , v & t^i_u , v & & i [ 3 ] , e E , u e , v e & & _ i=0 ^ 2 t^i_u , v & 2 & & u , v Vu v eq : so - intersect & & t^i_u , v + t^i_v , w - 1 & t^i_u , w & & i [ 3 ] , u , v , w V & & t^i_u , v + t^i_v , u & = 1 & & i [ 3 ] , u , v Vu v Constraints so - need - order ensure that for any edge in the solution the Schnyder - property for any non - incident node is satisfied by at least one of the three orders ."
CNN,convolutional neural network,TR-9756,& Stride & 2*Group 1 & Conv7 - 64 & 7x7 & 1 & 64 2 - 5 & Max2 & 2x2 & 2 & 64 2*Group 2 & Conv5 - 64 & 5x5 & 1 & 64 2 - 5 & Max2 & 2x2 & 2 & 64 2*Group 3 & Conv3 - 128 & 3x3 & 1 & 128 2 - 5 & Max2 & 3x3 & 2 & 128 2*Group 4 & Conv3 - 128 & 3x3 & 1 & 128 2 - 5 & Max2 & 3x3 & 2 & 128 2*Group 5 & Conv3 - 256 & 3x3 & 1 & 256 2 - 5 & Max2 & 3x3 & 2 & 256 2*Group 6 & Conv3 - 256 & 3x3 & 1 & 256 2 - 5 & Max2 & 3x3 & 2 & 256 2*Group 7 & Conv3 - 512 & 3x3 & 1 & 512 2 - 5 & Max2 & 2x2 & 2 & 512 The framework of the CNN used is shown in Fig . .
PCA,principal component analysis,TR-9854,"IEEE Computer Society Conference on , pages=586 - 591 , year=1991 , organization = IEEE@articlewelling2005fisher , title = Fisher linear discriminant analysis , author = Welling , Max , journal = Department of Computer Science , University of Toronto , volume=3 , number=1 , year=2005@incollectionyambor2002analyzing , title = Analyzing PCA - based face recognition algorithms : Eigenvector selection and distance measures , author = Yambor , Wendy S and Draper , Bruce A and Beveridge , J Ross , booktitle = Empirical evaluation methods in computer vision , pages=39 - 60 , year=2002 , publisher = World Scientific@articledelac2005independent , title = Independent comparative study of PCA , ICA , and LDA on the FERET data set , author = Delac , Kresimir and Grgic , Mislav and Grgic , Sonja , journal = International Journal of Imaging Systems and Technology , volume=15 , number=5 , pages=252 - 260 , year=2005 , publisher = Wiley Online Library@articlemelivsek2008support , title = Support vector machines , PCA and LDA in face recognition , author = Melisek , Jan Mazanec — Martin and Pavlovicova , Milos Oravec — Jarmila , journal = J. Electr ."
NP,new persian,TR-9868,"[ noitemsep]*huarnah-/*farnah- MP xwarrah NP farr ' glory'*uarna - ka- ' wool ' Phl wlk ' , MMP wrg /warrag/ NP barrah ' lamb'*parna- Phl pl , MMP pr /parr/ NP par(r ) ' feather'OP krnuvaka < k - r - nu - u - v - k - a > ' stonemason ' Phl < k(y)lwk ' > , MMP < qrwg > ' artisan ' , generally transcribed as kirrog on the basis of Armenian krogpet , though late OP * uva would seem to yield MP u*d(a)r - n- Phl dl- /darr/ NP darr- ' to rend , tear up'darrah*us - prna- Phl spwl , MMP ' spwr /aspurr/ ' accomplished'It has been suggested that the changes * rn rr and * rn l are interconnected , and that l(l ) r(r ) variation in reflexes of * rn represents dialectal variation within West Iranian ( [ 292 , fn ."
BP,best performing,TR-9945,"The iterative BP algorithm is applied as follows : all factor nodes associated to direct measurements send messages to corresponding variable nodes ; all variable nodes send messages along incidence edges ( except to an edge towards a factor node associated to direct measurement ) the form of the messages is : messages are equal to the message from step 1 , if variable nodes have direct measurements ; messages take the form of the "" flat start "" given by distribution with means or and variances or , if variable nodes do not have direct measurements ; all factor nodes compute messages to incident variable nodes according to eqn21 ; all variable variable nodes compute messages to incident factor nodes according to eqn16 ; all variable nodes compute corresponding marginal distributions ; repeat steps 3 , 4 , 5 until BP converges ."
PCC,pearson correlation coefficient,TR-9967,* [ t]cccccccccccccSubject & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 PCC of Activation and YMRS & -0.02 & 0.46 & 0.19 & 0.04 & 0.11 & 0.06 & 0.13 & 0.19 & 0.08 & -0.18 & 0.39 & 0.55 PCC of Activation and HamD & -0.11 & -0.13 & -0.25 & -0.11 & -0.04 & -0.10 & 0.04 & -0.10 & -0.57 & -0.11 & -0.16 & -0.80 0pt3ex PCC of Valence and YMRS & 0.00 & 0.44 & 0.17 & 0.00 & 0.08 & 0.13 & 0.19 & 0.20 & -0.04 & -0.14 & 0.40 & 0.55 PCC of Valence and HamD & -0.10 & -0.13 & -0.33 & -0.10 & -0.06 & -0.13 & 0.09 & -0.10 & -0.52 & -0.05 & 0.09 & -0.76 The PCC between each emotion rating and mood state .
FEC,forward error correction,TR-10069,The parameters are defined as follows : FEC - based : accounts for mechanisms that employ FEC ; ARQ - based : mark mechanisms that use ARQ ; QoE - sensitive data : this parameter demonstrates mechanisms that identity and/or considerate the video content to define the EC policy ; Video - aware : check mark is given to mechanisms that use any video characteristics to define the amount of redundancy and/or retransmission ; High - quality video : it is marked if the mechanisms are using videos equal or higher than 720p ( HD ready ) ; Network status : this parameter defines if the mechanisms use the information about the network healthy to define the redundant data ; UEP - enabled : means that different amounts of redundancy are being added to distinct portions of the video .
AI,artificial intelligence,TR-10146,"In order to pass Turing Test , the computer need to include advanced capabilities such as natural language processing , knowledge representation , automated reasoning , machine learning and computer vision [ 36]. AI research started later in the mid-1950s , where a summer workshop organized by Martin Minsky and Claude Shannon at Dartmouth College resulted in the birth of the field of AI [ 37]. The first contribution , however , was made in 1943 by McCulloch and Pitts , when they proposed the first model for artificial neural networks , in which each neuron has a binary output ( -1,+1 ) with a sign activation function [ 37]. Adoption of AI approaches increased , thanks to key contributions led to the emergence of new sub - fields such as expert systems , fuzzy logic and evolutionary computation ."
CNN,convolutional neural network,TR-10164,table[H ] tabularccccccc & 2c80 & 2c160 & 2c320 & GMBF & CNNf15+static & GMBF & CNNf15+static & GMBF & CNNf15+static 2 * 01 & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & 2*Average & & & & & & & & & & & & tabular Relative improvements over SRP - PHAT for strategy invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with the sequences described in Table tab : fine - tuning - material ( columns CNNf15+static ) .
CP,completely positive,TR-10286,"However , we can switch the summations in Equation to obtain a sum by columns of the table , where each still appears times in each column ( by the cyclic definition ) , but now is constant along each column : We can then sum up all for each , and use Equation ( together with Equation to cancel out the contribution from ) to finally obtain the desired solution to system : Quantum realisabilityIn quantum theory , i.e. in the probabilistic CP * category , many of the requirements of generalised Mermin - type arguments are automatically satisfied : canonical -SCFAs in ( i.e. -SCFAs in ) always have enough classical states ( and finitely many so ) , the semiring of scalars is positive , and any non - zero integer is invertible in it ."
NB,naive bayes,TR-10300,"tab : suprestabularllllllllll2 * & 3cRBWH & 3cRCH & 3cGCH & P & R & F1 & P & R & F1 & P & R & F1 SVM & 0.8539 & 0.8122 & 0.8325 & 0.9366 & 0.8811 & 0.9080 & 0.9347 & 0.8810 & 0.9071 SGD & 0.8575 & 0.7329 & 0.7903 & 0.9104 & 0.8276 & 0.8670 & 0.8713 & 0.7951 & 0.8315 NB & 0.9353 & 0.7102 & 0.8074 & 0.8409 & 0.9048 & 0.8717 & 0.8049 & 0.9281 & 0.8621 RF & 0.8508 & 0.7524 & 0.7986 & 0.9182 & 0.7552 & 0.8288 & 0.8654 & 0.8210 & 0.8426 LR & 0.8872 & 0.6912 & 0.7770 & 0.7003 & 0.0725 & 0.1314 & 0.9751 & 0.5043 & 0.6648 CNN & 0.9159 & 0.9028 & 0.9085 * & 0.9370 & 0.9408 & 0.9367 * & 0.9359 & 0.9342 & 0.9335 * tabulartableSemi - supervised Learning Performancesec : semsupresTable tab : sslres presents the performance of the self - trained CNN across RBWH , RCH , and GCH ."
AM,arithmetic mean,TR-10326,& & & & & 0.2 & 0.812 0.033 & 0.8195 0.03 & 0.782 0.034 & 0.8136 0.028 & 0.78 0.041 & 0.816 0.024 0.3 & 0.831 0.0270 & 0.839 0.0281 & 0.828 0.032 & 0.830 0.036 & 0.829 0.031 & 0.840 0.034 0.4 & 0.842 0.021 & 0.847 0.018 & 0.849 0.016 & 0.821 0.017 & 0.827 0.020 & 0.827 0.019 0.5 & 0.850 0.0170 & 0.848 0.016 & 0.848 0.014 & 0.831 0.019 & 0.837 0.0186 & 0.837 0.018 0.6 & 0.841 0.023 & 0.850 0.024 & 0.849 0.019 & 0.846 0.023 & 0.840 0.026 & 0.842 0.023 0.7 & 0.859 0.025 & 0.862 0.019 & 0.867 0.018 & 0.838 0.018 & 0.843 0.031 & 0.844 0.027 0.8 & 0.845 0.031 & 0.846 0.032 & 0.850 0.029 & 0.844 0.024 & 0.841 0.030 & 0.843 0.024 Averaged AM over 10 trials of the linear classifiers obtained from based regularized ERM on two synthetic datasets .
DA,dialogue acts,TR-10366,table*[t]tab : LSTM_result Baseline and DAMIC LSTM results with or without utterance collapsing -1 cm -1cmtabularlllllllllllllll2l2 * & 1c2*F1 & 12cF1 per DA 4 - 15 2l & 1c & CQ & FD & FQ & GG & IR & JK & NF & O & OQ & PA & PF & RQ 2*Baseline & -collapsed & 51.85 & 16.54 & 49.93 & 28.50 & 49.06 & 32.56 & 7.14 & 22.55 & 1.81 & 85.35 & 72.76 & 35.12 & 19.41 2 - 15 & + collapsed & 52.48 & 19.04 & 47.81 & 23.73 & 51.54 & 27.60 & 6.64 & 25.52 & 1.13 & 86.88 & 73.86 & 34.68 & 17.56 2*DAMIC & -collapsed & 64.39 & 23.83 & 55.79 & 27.87 & 75.88 & 34.50 & 8.06 & 32.31 & 3.10 & 93.60 & 85.40 & 50.07 & 34.42 2 - 15 & collapsed & 65.00 & 21.22 & 52.13 & 27.76 & 77.91 & 34.86 & 8.93 & 32.54 & 2.09 & 93.59 & 85.73 & 52.99 & 32.14 tabulartable *
MGE,minimum generation error,TR-10368,"Performing back - propagation with the new criterion involves the following steps : [ ( a ) ] Initialise the weights for the MGE - DNN from a conventionally - trained DNN ( i.e. , using MMSE ) ; [ ( b ) ] Given a sequence of input linguistic features , perform a forward propagation step just as in conventional training , to predict observation ; [ ( c ) ] Restore the mean and variance for ( because mean - variance normalisation is performance for the acoustic features before training ) ; [ ( d ) ] Perform MLPG to generate acoustic feature trajectories using Eq . ( ) ;"
CC,cross - correlation,TR-10433,"We highlight the following a few observations : 1 ) NMSRVI ( 1 ) achieves the best performances , and outperforms ANTs in terms of both MSE and mean CC on both tasks , likely due to the representation and optimization efficiency of deep neural nets ; 2 ) NMSRV yields consistently better results than VoxelMorph , demonstrating the efficacy of self - supervised optimization during the test phase for improving velocity field estimation and reducing the estimation gap between training and testing ; 3 ) NMSR ( 1 ) achieves better performance than NMSR on all experiments , demonstrating the benefit of sequential multi - scale optimization in echocardiogram registration ."
CNN,convolutional neural network,TR-10557,table[H ] tabularcccccccccc & 3c80 & 3c160 & 3c320 & SRP & GMBF & CNNf15+static & SRP & GMBF & CNNf15+static & SRP & GMBF & CNNf15+static 2 * 01 & & & & & & & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & & & & & & & 2*Average & & & & & & & & & & & & & & & & & & tabular Results for the SRP - PHAT strategy ( columns SRP ) ; the one invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with the sequences described in Table tab : fine - tuning - material ( columns CNNf15+static ) .
TA,threshold algorithm,TR-10593,figure*[!ht ] minipage[t]0.23 figures / TimeVsDimensionalityFullSpace.pdf Varying query size fig : algorithms minipage minipage[t]0.23 figures / TimeVsDimensionality.pdf Varying query size fig : syn_TimeVsDimension_Dist minipage minipage[t]0.23 figures / TimeVsN.pdf Varying number of tuples fig : syn_TimeVsN minipage minipage[t]0.25 figures / TimeVsCardinality.pdf Varying cardinality fig : syn_TimeVsC minipagefigure*figure*[!ht ] minipage[t]0.25 figures / TimeVsOutputTuple.pdf Time vs number of skylines returned fig : syn_TimeVsNumberOfSkylines minipage minipage[t]0.25 figures / NumberOfTuplesVsOutputTuples.pdf Tuples accessed vs number of skylines returned fig : syn_NumberOfTuplesVsSkylines minipage minipage[t]0.21 figures / TimeVsDimensionalityAirbnb.pdf AirBnB : Varying query size fig : algorithmsAirbnb minipage minipage[t]0.23 figures / TimeVsDimensionalityAirbnbTASky.pdf AirBnB : TA - SKY performance v.s .
FEC,forward error correction,TR-10665,"table[!h ] Average SSIM , VQM , and network footprint center tabularlccccc & 1lSHIELD & 1lCORVETTE & 1lVaUEP & 1lVaEEP & 1lWithout FEC 6cUrban environment SSIM & 0,895 & 0,787 & 0,701 & 0,684 & 0,551 VQM & 1,459 & 2,441 & 4,034 & 4,323 & 6,688 Overhead & 17,333 & 21,778 & 46,660 & 65,984 & - 6cHighway environment SSIM & 0,911 & 0,809 & 0,744 & 0,729 & 0,627 VQM & 1,328 & 2,095 & 3,414 & 3,728 & 5,281 Overhead & 12,333 & 17,112 & 46,660 & 65,984 & - tabular tab : shield : Sumary center tableSummaryThis chapter described and assessed two proposed mechanisms to increase the video transmission resiliency over VANETs ."
ML,machine learning,TR-10803,"Binary Classification based NID*[t ] ML based 2-class NID with LR and SVM(Mean Std - Dev Percent ) * 6p2 cm p2.0 cm c c c c 0em 1lIntrusion Type & 1lNID Model Name & 1cAccuracy & 1cPrecision & 1cRecall & 1cF1-Score 0em 0em 4*'apache2 ' & NID - LR & 98.51 0.42 & 0.00 & 0.00 & - & NID - DA - LR & 99.53 0.05 & 77.29 1.21 & 90.70 3.41 & 83.45 2.14 0em 2 - 6 0em & NID - SVM & 98.97 0.02 & 55.73 0.56 & 99.32 0.38 & 71.39 0.49 & NID - DA - SVM & 99.94 0.01 & 95.61 0.87 & 99.70 0.07 & 97.61 0.44 0em 0em 4*'mailbomb ' & NID - LR & 91.93 0.44 & 0.00 & 0.00 & - & NID - DA - LR & 97.84 0.41 & 78.11 3.37 & 99.89 0.14 & 87.63 2.08 0em 2 - 6 0em & NID - SVM & 93.04 0.03 & 80.38 1.58 & 11.53 0.33 & 20.16 0.50 & NID - DA - SVM & 99.34 0.34 & 92.29 3.65 & 99.78 0.16 & 95.86 2.05 0em 0em 4*'processtable ' & NID - LR & 99.33 0.05 & 64.98 1.78 & 98.74 2.03 & 78.37 1.72 & NID - DA - LR & 99.53 0.04 & 72.46 1.81 & 100.00 0.00 & 84.02 1.21 0em 2 - 6 0em & NID - SVM & 99.87 0.06 & 90.79 3.86 & 99.60 0.56 & 94.96 2.14 & NID - DA - SVM & 99.90 0.04 & 92.60 2.17 & 99.71 0.15 & 96.02 1.22 0em 0em 4*'mscan ' & NID - LR & 97.78 0.11 & 42.58 1.35 & 84.90 1.48 & 56.71 1.42 & NID - DA - LR & 99.01 0.06 & 64.81 1.48 & 92.29 0.46 & 76.14 1.14 0em 2 - 6 0em & NID - SVM & 99.61 0.12 & 85.10 4.34 & 94.11 0.18 & 89.32 2.73 & NID - DA - SVM & 99.73 0.06 & 90.03 4.12 & 95.12 1.04 & 92.44 1.65 0em 0em 4*'saint ' & NID - LR & 98.22 0.22 & 40.17 2.89 & 96.47 1.45 & 56.65 2.81 & NID - DA - LR & 98.47 0.04 & 43.78 0.75 & 96.93 0.65 & 60.32 0.83 0em 2 - 6 0em & NID - SVM & 98.56 0.03 & 45.39 0.50 & 96.41 0.75 & 61.72 0.46 & NID - DA - SVM & 98.60 0.01 & 46.08 0.24 & 97.47 0.33 & 62.58 0.27 0em 0em 4*'guesspasswd ' & NID - LR & 88.59 0.75 & 34.21 1.50 & 75.10 2.87 & 46.98 1.44 & NID - DA - LR & 89.07 0.24 & 35.65 0.59 & 77.74 0.19 & 48.89 0.57 0em 2 - 6 0em & NID - SVM & 94.59 0.27 & 89.06 1.93 & 22.21 4.10 & 35.42 5.19 & NID - DA - SVM & 98.95 0.10 & 90.57 2.17 & 94.29 0.22 & 92.38 1.19 0em 0em 4*'snmpgetattack ' & NID - LR & 88.67 0.00 & 0.00 & 0.00 & - & NID - DA - LR & 80.42 0.58 & 36.61 0.69 & 99.43 0.07 & 53.51 0.72 0em 2 - 6 0em & NID - SVM & 88.65 0.02 & 0.00 & 0.00 & - & NID - DA - SVM & 82.42 0.03 & 39.13 0.03 & 99.39 0.21 & 56.15 0.04 0em 0em 4*'snmpguess ' & NID - LR & 98.84 0.15 & 78.61 2.51 & 95.93 0.05 & 86.39 1.55 & NID - DA - LR & 99.07 0.06 & 82.66 1.17 & 95.84 0.00 & 88.76 0.68 0em 2 - 6 0em & NID - SVM & 96.18 0.00 & 0.00 & 0.00 & - & NID - DA - SVM & 81.20 0.04 & 16.85 0.02 & 99.72 0.10 & 28.83 0.03 0em In this subsection , we evaluate binary classification performance of the DA enhanced NID framework ."
SO,smart object,TR-10811,"& Consumer & Smart dimensions & SO 1 & SO 2 & SO & Object & & & & network Varying & & & & complexity & & & & of PP & & & & Varying & & & & query & & & & * complexity & & & & Varying & & & & number of & & & & sensing SOs & & & & tabular Experiments : - time overhead , * time and bandwidth overhead tab : expTabletable Experimental resultsIn executing our experiments , we considered three main characteristics that may impact the performance of the proposed solution ."
SO,smart object,TR-11401,"& Consumer & Smart dimensions & SO 1 & SO 2 & SO & Object & & & & network Varying & & & & complexity & & & & of PP & & & & Varying & & & & query & & & & * complexity & & & & Varying & & & & number of & & & & sensing SOs & & & & tabular Experiments : - time overhead , * time and bandwidth overhead tab : expTabletable Experimental resultsIn executing our experiments , we considered three main characteristics that may impact the performance of the proposed solution ."
RC,red clump,TR-11456,"Due to the strong line broadening , they are among the most poorly - fitted spectra in the survey;Group 4 : Has two classes covering almost the same range of and as group 1 , RGB stars , but with higher metallicities;Group 5 : Contains three classes formed by stars from the RC and the warm end of RGB , with stellar populations from both the thin and thick disk;Group 6 : Formed of five classes composed of dwarf stars over a wide range of temperatures;Group 7 : Including five classes with peculiar stars;Group 8 : Collects 18 classes with all the outliers of the classification , less than 1 per cent of the spectra in SDSS DR12 ."
MRE,median recovery error,TR-11491,"0 ToDo look at distribution of attributes in the latent space If we have time : Do a figure with auto - encoder ( if we manage to sample the latent space , for instance by fitting a Gaussian to the encoded images in the latent space ) Show a picture of a few examples of Eiffel tower or london bridge in LSUN that seem to be memorize but are just samples learn from template show ( at least on some images ) that VGG , perceptual loss , L1 or L1 on Laplacian pyramid gives the same results for recovery because GLO reported some mitigated results on this ( sometimes L2 is better , sometimes pyramid : in the end , they mixed the two metrics ) give statistics about the recovery precision with LBFGS : for a given * generated * target image , the distribution of errors using a lot of random initializations ( to demonstrate that the problem is almost convex , or at least ' easy ' to optimize ) show convergence speed vs SGD or other optimization method ( because it is was is generaly used in the literature : this finding makes it possible to experiment more easily ) , for a given distorted target generated image , the average / median error vs the distorsion : : you did that on training image but it would be nice to show it first on generated images , to show robustness discuss difference LBFGS vs SGD : LBFGS is much faster to converge , not prone to gradient step setting , but can suffer from instabilities add experiments on Auto Encoder : visual recovery results , histograms , MRE values , and maybe FID if we have time ( sampling from the latent space ) add a small paragraph to explain experimental settings , i.e. that we reproduce several architectures from the literature and trained again on splitting , ... tell somewhere the difference ( I think in the intro ) between over - fitting and verbatim memorization , for which we provide a solid definition : add more experiments on LSUN : show histograms with PG - GAN without GAP , show recovery failure results add experiments on MNIST , CIFAR ?"
CG,conjugate gradient,TR-11500,"tabularc c c c cNetwork & Number of Nodes & Number of Edges & Average Node Degree & DBLPlcc & 93,156 & 178,145 & 3.82 & 39.5753 Arxivlcc & 86,376 & 517,563 & 11.98 & 99.3319 Email - Enron & 36,692 & 183,831 & 10.02 & 111.2871 Gowalla & 196,591 & 950,327 & 9.67 & 169.3612 Flickr & 513,969 & 3,190,452 & 12.41 & 663.3587 Hollywood-2009 & 1,139,905 & 113,891,327 & 99.13 & 2247.5591 PPI Data & 12,976 & 99,814 & 7.6916 & 94.4121 DBLP2006 - 2008 & 179,266 & 765,346 & 4.2693 & 61.7765tabulartable*figure[ht ] center [ ] fig : Alpha - a RunTimeDBLP_lcc.eps [ ] fig : Alpha - b RunTimeArxiv_lcc.eps [ ] fig : Alpha - c RunTimeEmail.eps [ ] fig : Alpha - d RunTimeGowalla.eps [ ] fig : Alpha - e RunTimeFlickr.eps [ ] fig : Alpha - f RunTimeHollywood.eps center Runtime in seconds of and CG to process queries for Katz proximity as a function of ranging from to In these experiments the reported numbers are the averages across randomly chosen query nodes ."
IVR,immersive virtual reality,TR-11503,"Traffic Classification * [ ] Traffic Classification Example & Burst size ( ) & Arrival model ( ) & Applications 1 & 99.9 - 99.99999 & ms & 1 - 10 KB & & T , A , HIC 2 & 99.9 - 99.99999 & 10 - 50 ms & 1 - 20 KB & & T , A , HIC 3 & 99.9 - 99.999 & 2 - 10 ms & 1 - 30 KB & 10 - 5000 pkt / s ( P ) & IVR , T , A , IoD , HIC 4 & 99.99999 & 2 ms & 80 B & 100 - 50000 pkt / s ( GE ) & T , A , HIC 5 & 99.999 & 1 ms & 800 B & 10 - 5000 pkt / s ( GE ) & T , A , HIC 6 & 99.99999 & 2 ms & 5 KB & 100 - 1000 pkt / s ( E ) & A , IoD 7 & 99.999 & 2 ms & 8 KB & 100 - 500 pkt / s ( E ) & IVR , IoD 8 & 99.999 & 0.5 ms & 5 KB & 100 - 500 pkt / s ( E ) & IVR , T , A , IoD In Table , typical traffic characteristics of the use - cases in Section are summarized , in which the traffic characteristics and QoS are represented by the typical air latency , target reliability , packet size , packet arrival rate and model ."
BM,bare metal,TR-11533,tabular > gray!25 & 2cx86 - 64 & 2cPOWER & 2cARM & 2cGPU & 2cFPGA & 2cASIC gray!25 -2*Provider & BM & VM & BM & VM & BM & VM & BM & VM & BM & VM & BM & VM Amazon aws : iaas & * & & & & & & & & & & & Microsoft azure : iaas & & & & & & & & & & & & Google gcp : iaas & & & & & & & & & & & & IBM ibm : iaas & * & & * & & & & & & & & & Oracle oracle : iaas & * & * & & & & & & & & & & Scaleway scaleway : iaas & * & & & & * & & & & & & & tabular tab : cloudresources table
HD,hausdorff distance,TR-11574,"tabularlllllll & & & LMSTAPLE & COLLATE & MV & & & & LocalMAPSTAPLE & COLLATE & LMS7 & DM & 92.62.4 & 91.73.0 & 87.34.5 & 85.15.3 & 83.87.3 & 82.39.0 HD & 7.42.6 & 8.23.3 & 9.84.8 & 12.06.2 & 13.97.4 & 14.78.2 & & - & & & & tabulartab : SegRes1table figure*[t]minipage[b]0.315figure = GT_Med_OurAll.eps , height=4cm , width=4 cm ( a)minipageminipage[b]0.315figure = GT_Med_Our.eps , height=4cm , width=4 cm ( b)minipageminipage[b]0.315figure = GT_Med_LMStaple.eps , height=4cm , width=4 cm ( c)minipageminipage[b]0.315figure = GT_Med_MStaple.eps , height=4cm , width=4 cm ( d)minipageminipage[b]0.315figure = GT_Med_Staple.eps , height=4cm , width=4 cm ( e)minipageminipage[b]0.315figure = GT_Med_wSSL.eps , height=4cm , width=4 cm ( f)minipageThe predicted ground truth for UCL Patient 23 by different methods : ( a ) ; ( b ) ; ( c)LocalMAPSTAPLE ; ( d)COLLATE ; ( e)LMS7 ; and ( f ) ."
CT,computed tomography,TR-11674,figure [ ht ] center minipage0.15 ./fig / snapshot0101new3.png minipage minipage0.15 ./fig / snapshot0121new3.png minipage minipage0.15 ./fig / snapshot0131new3.png minipage minipage0.15 ./fig / snapshot0141new3.png minipage minipage0.15 ./fig / snapshot0102new3.png minipage minipage0.15 ./fig / snapshot0122new3.png minipage minipage0.15 ./fig / snapshot0132new3.png minipage minipage0.15 ./fig / snapshot0142new3.png minipage minipage0.15 ./fig / snapshot0104new.png minipage minipage0.15 ./fig / snapshot0124new.png minipage minipage0.15 ./fig / snapshot0134new.png minipage minipage0.15 ./fig / snapshot0144new.png minipage minipage0.15 ./fig / snapshot0105new2.png minipage minipage0.15 ./fig / snapshot0125new2.png minipage minipage0.15 ./fig / snapshot0135new2.png minipage minipage0.15 ./fig / snapshot0145new2.png minipage minipage0.15 ./fig / snapshot0107new.png minipage minipage0.15 ./fig / snapshot0127new.png minipage minipage0.15 ./fig / snapshot0137new.png minipage minipage0.15 ./fig / snapshot0147new.png minipage minipage0.15 ./fig / snapshot0108new2.png minipage minipage0.15 ./fig / snapshot0128new2.png minipage minipage0.15 ./fig / snapshot0138new2.png minipage minipage0.15 ./fig / snapshot0148new2.png minipage center 5fig : vis2 Visualizations for the first four anatomy on the first four holdout CT images .
AP,average precision,TR-11809,"TP : yellow yellow , FP : red red , FN : green green ) fig : ground - seg figure * tablecentertabularccccMethod & Mean & Building & Road FCNfcn & 77.64 & 70.44 & 73.32 ResNetresnet & 78.46 & 69.15 & 76.44 tabularcenter Aerial image semantic segmentation IoU. tab : seg table tablecentertabularcccccMethod & WeightedCov & AP & Re-50 & Pr-50 FCN & 39.74 & 8.04 & 19.64 & 18.38 FCN + Open & 43.19 & 16.45 & 24.55 & 36.09 ResNet & 38.70 & 10.47 & 21.30 & 21.93 ResNet + Open & 41.10 & 22.92 & 22.78 & 43.78 tabularcenter Building instance segmentation IoU. tab : instance table Semantic Segmentation from Polyline Data Our maps provide two types of road structures : curbs defining the road boundaries as well as center lines defining the connectivity ( adjacency ) in the street network ."
SCS,statistical compressed sensing,TR-11940,"GMM assumes that each signal is independently drawn from one of these Gaussians with an unknown index , whose probability density function isTo decode a measured signal , the GMM - based SCS decoder estimates the signal and selects the Gaussian model by maximizing the log a - posteriori probability eqn : MAP : x : k is calculated by first computingthe linear MAP decoder eqn : MAP : Sigma using each of the Gaussian models , and then selecting a best model that maximizes the log a - posteriori probability among all the models whose corresponding decoder implements a piecewise linear estimate : The model selection eqn : GMM : model : selection is at the heart of the GMM - based SCS.(Correct model / class selection from compressed measurements is at the core of numerous applications beyond signal reconstruction , see for example and references therein . )"
ML,machine learning,TR-11947,"Multiple Classification based NID*[t ] ML based 4-class NID with SVM(Mean Std - Dev Percent ) * 6p1.5 cm p2.0 cm c c c c 0em 1lCategory & 1lNID Model Name & 1cAccuracy & 1cPrecision & 1cRecall & 1cF1-Score 0em 0em 3*NORMAL & NID - SVM & 76.30 0.01 & 75.91 0.01 & 98.69 0.02 & 85.81 0.01 & NID - PGM - SVM & 76.16 0.08 & 75.90 0.03 & 98.41 0.11 & 85.70 0.06 & NID - DA - SVM & 82.87 0.17 & 98.39 0.20 & 77.68 0.32 & 86.82 0.16 0em 0em 3*DOS & NID - SVM & 94.00 0.01 & 93.35 1.15 & 25.34 0.16 & 39.86 0.10 & NID - PGM - SVM & 93.84 0.10 & 86.98 2.63 & 25.33 0.49 & 39.23 0.80 & NID - DA - SVM & 99.48 0.05 & 94.09 0.58 & 99.70 0.06 & 96.81 0.29 0em 0em 3*PROBE & NID - SVM & 98.82 0.02 & 68.54 0.43 & 83.27 0.53 & 75.19 0.37 & NID - PGM - SVM & 98.80 0.02 & 67.39 0.31 & 85.89 0.38 & 75.53 0.30 & NID - DA - SVM & 99.32 0.04 & 88.70 0.88 & 78.13 1.85 & 83.07 1.00 0em 0em 3*R2L & NID - SVM & 83.43 0.01 & 97.91 0.64 & 4.83 0.02 & 9.20 0.04 & NID - PGM - SVM & 83.34 0.05 & 94.15 5.15 & 4.51 0.02 & 8.60 0.04 & NID - DA - SVM & 83.74 0.19 & 51.75 0.31 & 96.57 0.66 & 67.38 0.23 0em * [ t ] DL based 4-class NID with DNN(Mean Std - Dev Percent ) * 6p1.5 cm p2.0 cm c c c c 0em 1lCategory & 1lNID Model Name & 1cAccuracy & 1cPrecision & 1cRecall & 1cF1-Score 0em 0em 3*NORMAL & NID - DNN & 77.15 3.22 & 76.5 2.81 & 99.15 0.51 & 86.33 1.64 & NID - PGM - DNN & 83.17 1.40 & 81.58 1.31 & 99.26 0.37 & 89.55 0.76 & NID - DA - DNN & 87.96 0.12 & 86.95 0.28 & 98.15 0.61 & 92.21 0.11 0em 0em 3*DOS & NID - DNN & 93.81 2.94 & 89.70 6.58 & 23.55 9.17 & 27.72 7.37 & NID - PGM - DNN & 99.04 0.22 & 97.84 3.73 & 89.97 0.98 & 93.7 1.34 & NID - DA - DNN & 99.59 0.08 & 97.14 1.36 & 97.65 0.40 & 97.39 0.52 0em 0em 3*PROBE & NID - DNN & 98.92 0.31 & 74.54 1.93 & 81.49 8.26 & 76.79 3.29 & NID - PGM - DNN & 98.40 0.07 & 64.93 1.90 & 55.84 7.72 & 59.78 3.86 & NID - DA - DNN & 99.05 0.07 & 75.27 1.24 & 83.47 4.52 & 79.11 2.17 0em 0em 3*R2L & NID - DNN & 83.90 2.72 & 58.99 3.87 & 7.55 5.81 & 11.28 3.13 & NID - PGM - DNN & 84.90 1.26 & 88.45 5.67 & 13.92 7.48 & 23.60 2.57 & NID - DA - DNN & 89.09 0.10 & 92.17 4.86 & 40.97 1.82 & 56.64 0.92 0em In order to verify the performance of the DA module on enhancing the existing learning based IDSs , we have undertaken multi - class based NID experiments ."
RP,reciprocal pagerank,TR-11954,"table*[!h ] tableThe comparison of Spearman Correlations to the KORE gold standard of different methods from each dataset over time - varying graphs and aggregated graphs with redirects tab : corr - all threeparttable tabularllllllllll & Jaccard ( I+O ) & tabular[c]@l@ Extended Jaccard RP ( I+O ) tabular & tabular[c]@l@ Extended Jaccard RD ( I+O ) tabular 2007 & 0.522472 & 0.530393 & 0.540804 2008 & 0.532665 & 0.529899 & 0.580926 2009 & 0.580944 & 0.613450 & 0.651450 2010 & 0.586593 & 0.637930 & 0.696506 2011 & 0.561152 & 0.619950 & 0.669867 2012 & 0.537278 & 0.582106 & 0.634255 2013 & 0.543986 & 0.589968 & 0.640649 2014 & 0.483211 & 0.530217 & 0.589831 2015 & 0.506170 & 0.529598 & 0.579989 2016 & 0.494577 & 0.520003 & 0.544250 Intersection & 0.470378 & 0.465568 & 0.487342 Union & 0.531830 & 0.652417 & 0.708271 tabular tablenotes[para , flushleft ] , , , and mean the Union model using the Extended Jaccard with Reciprocal Degree Centrality considering both in - links and out - links is significantly better than this result with p - value 0.1 , p - value 0.05 , p - value 0.01 and p - value 0.001 respectively ."
CNS,copenhagen networks study,TR-11982,"@Xrrr@Model M1 : Activity space size , & coeff & p val & LMG Social circle size , & & & 0.94 gender & & 0.05 & 0.05 time coverage & & 0.06 & 0.01 [ , , ] Model M2 : Activity space entropy , Social circle entropy , & & & 0.42 gender & & & 0.22 time coverage & & & 0.36 [ , , ] Model M3 : New locations / week , & New ties / week , & & & 0.9 gender & & & 0.08 time coverage & & 1.0 & 0.01 [ , , ] & & & Model M4 : Activity space stability , & Social circle stability , & & & 0.6 gender & & 0.05 & 0.04 time coverage & & & 0.36 [ , , ] & & & Model M5 : Activity space rank turnover , & Social circle rank turnover , & & & 0.98 gender & & 0.06 & 0.01 time coverage & & 0.07 & 0.01 [ , , ] & & & Linear regression models for the CNS dataset ."
SM,scalar multiplication,TR-12141,"ht]Improvements on different SM methods & [ HTML]FFFFFF & & Roughly 50 SM & [ HTML]FFFFFF & mbNAF & 50 SM & and [ HTML]FFFFFF & Traditional DA & PA is 1/2 PD & [ HTML]FFFFFF & & & ( 163,233 ) [ HTML]FFFFFF & Frobenius with GLV & 28.3 SM & [ HTML]FFFFFF & & 3n/4 PA and 1/2 PD & [ HTML]FFFFFF & Traditional window & Reduce PA and PD compared with NAF & ( 192,256,512 ) [ HTML]FFFFFF & & 10 better than NAF & [ HTML]FFFFFF & & 27.4 better than wNAF & [ HTML]FFFFFF & Traditional cm & & [ HTML]FFFFFF & & & [ HTML]FFFFFF & Traditional Montgomery & & ( 160,256 ) [ HTML]FFFFFF & Improved Montgomery & & and [ HTML]FFFFFF & Improved Montgomery & & [ HTML]FFFFFF & & roughly 51 SM with time=0.056ms & [ HTML]FFFFFF & & & ( 159,191,223,255 ) [ HTML]FFFFFF & SM with side information & 40 is better than traditional verification & [ HTML]FFFFFF & SM with Fibonacci & & [ HTML]FFFFFF & & Improve signature verification ( 50 ) & [ HTML]FFFFFF & & & ( 192,256 ) [ HTML]FFFFFF & SM with binary tree & Reduce SM time and complex computations & [ HTML]FFFFFF & SM with random n - cover & & Efficiency Improvement of Coordinate SystemsECDSA has to perform complex operations ; these operations consume resources in constrained devices ."
PSO,particle swarm optimization,TR-12150,"Basically , one needs to create a main file to call PSO procedure as follows:#include "" common.h""#include "" function.h""#include "" pso.h""int main ( ) SearchSpace * s = NULL ; s = ReadSearchSpaceFromFile(""pso_model.txt "" , _ PSO _ ) ; InitializeSearchSpace(s , _ PSO _ ) ; if(CheckSearchSpace(s , _ PSO _ ) ) runPSO(s , MyFunction ) ; DestroySearchSpace(&s , _ PSO _ ) ; return 0;As one can observe , it is quite simple to execute PSO , since we need to call five main functions only : ReadSearchSpaceFromFile : it reads the model file and creates a search space ; InitializeSearchSpace : it initializes the search space ; CheckSearchSpace : it checks wether the search space is valid or not ; runPSO : it minimizes function MyFunction ; and DestroySearchSpace : it deallocates the search space ."
SD,standard deviation,TR-12151,"max width=0.75Classification performance of the compared algorithms with NB classifier ( averaged over 40 runs ) ' Mean'and ' SD ' - Mean and standard deviation of classification error over 40 runs ' PI'- Percentage improvement in the classification error relative to the classification error achieved with all features max width=0.75Classification performance of the compared algorithms with k - NN Classifier ( averaged over 40 runs ) ' Mean'and ' SD ' - Mean and standard deviation of classification error over 40 runs ' PI'- Percentage improvement in the classification error relative to the classification error achieved with all features Further , paired t - tests were used to evaluate the statistical significance of the results relative to the compared algorithm ."
TDS,taint dependency sequences,TR-12240,"language=[ANSI]C++lstlisting[basicstyle= , numberstyle= , numbers = left , frame = single , name = Pseudo - code for Genetic Algorithm , caption= , captionpos = b , label = fig : GA]popsize : = desired population sizeP : = [ ] for popsize times do P : = P U < new random individuals > Best : = [ ] repeat for each individual Pi in P do AssessFitness(Pi ) if Best = = [ ] or Fitness(Pi ) > Fitness(Best ) then Best : = Pi Q : = [ ] for popsize/2 times do Parent Pa : = SelectwithReplacement(P ) Parent Pb : = SelectwithReplacement(P ) Children Ca , Cb : = Crossover(Copy(Pa ) , Copy(Pb ) ) Q : = P U < Mutate(Ca ) , Mutate(Cb ) > P : = Quntil Best is the ideal solution OR we have run out of timereturn BestlstlistingInitial Populationsec : initpopAs mentioned before , in order to execute a TDS a set of constraints ( corresponding to the IF and WHILE conditions associated to this TDS ) has to be satisfied ."
MPI,multiple parallel instances,TR-12255,"MPI - parallel Multi - frame RMSD using Global Arrays alg : GA Input : size : Total number of frames assigned to each rank ga : Initialized Global Arrays xref0 : mobile group in the initial frame which will be considered as reference start stop : that tell which block of trajectory ( frames ) is assigned to each rank topology trajectory : files to read the data structure from Include : BlockRMSD ( ) from Algorithm alg : RMSD algorithmic[1 ] bsize ceil(trajectory.numberframes / size ) ga ga.create(ga.CDBL , [ bsize*size,2 ] , "" RMSD "" ) buf np.zeros([bsize*size,2 ] , dtype = float ) out BlockRMSD(topology , trajectory , xref0 , start = start , stop = stop ) ga.put(ga , out , ( start,0 ) , ( stop,2 ) ) rank = = 0 buf ga.get(ga , lo = None , hi = None ) algorithmicalgorithmMPI and Parallel HDF5sec : methods - hdf5HDF5 is a structured self - describing hierarchical data format which is the standard mechanism for storing large quantities of numerical data in Python ( http://www.hdfgroup.org/HDF5,pythonhdf5 ) ."
TA,threshold algorithm,TR-12292,"Therefore , effects the total execution time of TA - SKY and ST - S.figure*[!ht ] minipage[t]0.23 figures / TimeVsNAirbnb.pdf AirBnB : Varying the number of tuples fig : Airbnbn minipage minipage[t]0.23 figures / TimeVsOutputTupleAirbnb.pdf AirBnB : Time vs the number of skylines returned fig : Airbnbp1 minipage minipage[t]0.25 figures / NumberOfTuplesVsOutputTuplesAirbnb.pdf AirBnB : Number of accessed tuples vs the number of skylines fig : Airbnbp2 minipage minipage[t]0.25 figures / TimeVsDimensionalityZillow.pdf Zillow : Varying query size fig : Zillowm minipagefigure*figure*[!ht ] minipage[t]0.3 figures / TimeVsNZillow.pdf Zillow : Varying the number of tuples fig : TimeVsNZillow minipage minipage[t]0.3 figures / TimeVsOutputTupleZillow.pdf Zillow : Time vs the number of skylines returned fig : NumberOfTuplesVsSkylines minipage minipage[t]0.3 figures / NumberOfTuplesVsOutputTuplesZillow.pdf Zillow : Number of accessed tuples vs the number of skylines fig : NumberOfTuplesVsOutputTuplesZillow minipagefigure*Progressive Behavior of TA - SKY : Figure fig : syn_TimeVsNumberOfSkylines and fig : syn_NumberOfTuplesVsSkylines demonstrates the incremental performance of TA - SKY for discovering the new skylines for a specific query of size , while M and all the attributes having cardinality ."
PC,principal component,TR-12297,"@X rr rr @ & 2 cCNS & 2 cMDC ( lr)2 - 3(lr)4 - 5 & PC 0 & PC 1 & PC 0 & PC 1 Social circle size , & 0.41 & 0.18 & -0.36 & 0.04 Activity space size , & 0.42 & -0.23 & -0.40 & -0.05 New ties / week , & 0.33 & 0.27 & -0.24 & -0.35 New locations / week , & 0.39 & -0.11 & -0.37 & -0.22 Social circle entropy , & 0.29 & 0.33 & -0.36 & -0.23 Activity space entropy , & 0.38 & -0.10 & -0.35 & 0.13 Social circle stability , & -0.12 & -0.50 & -0.11 & 0.56 Activity space stability , & -0.06 & -0.50 & -0.03 & 0.62 Social circle rank turnover , & -0.17 & 0.26 & 0.28 & -0.19 Activity space rank turnover , & -0.35 & 0.37 & 0.42 & -0.18 T=30 , Principal Components ."
CNN,convolutional neural network,TR-12586,"tab : suprestabularllllllllll2 * & 3cRBWH & 3cRCH & 3cGCH & P & R & F1 & P & R & F1 & P & R & F1 SVM & 0.8539 & 0.8122 & 0.8325 & 0.9366 & 0.8811 & 0.9080 & 0.9347 & 0.8810 & 0.9071 SGD & 0.8575 & 0.7329 & 0.7903 & 0.9104 & 0.8276 & 0.8670 & 0.8713 & 0.7951 & 0.8315 NB & 0.9353 & 0.7102 & 0.8074 & 0.8409 & 0.9048 & 0.8717 & 0.8049 & 0.9281 & 0.8621 RF & 0.8508 & 0.7524 & 0.7986 & 0.9182 & 0.7552 & 0.8288 & 0.8654 & 0.8210 & 0.8426 LR & 0.8872 & 0.6912 & 0.7770 & 0.7003 & 0.0725 & 0.1314 & 0.9751 & 0.5043 & 0.6648 CNN & 0.9159 & 0.9028 & 0.9085 * & 0.9370 & 0.9408 & 0.9367 * & 0.9359 & 0.9342 & 0.9335 * tabulartableSemi - supervised Learning Performancesec : semsupresTable tab : sslres presents the performance of the self - trained CNN across RBWH , RCH , and GCH ."
MRE,median recovery error,TR-12635,"WARNING : use overfitting instead of only ' memorization ' which is more ambiguous , ' verbatim ' seems to strongToDo list : look at distribution of attributes in the latent spaceIf we have time : Do a figure with auto - encoder ( if we manage to sample the latent space , for instance by fitting a Gaussian to the encoded images in the latent space)Show a picture of a few examples of Eiffel tower or london bridge in LSUN that seem to be memorize but are just samples learn from templateadd appendix with extra experiments ( a lot of images of recovery)show ( at least on some images ) that VGG , perceptual loss , L1 or L1 on Laplacian pyramid gives the same results for recovery because GLO reported some mitigated results on this ( sometimes L2 is better , sometimes pyramid : in the end , they mixed the two metrics)give statistics about the recovery precision with LBFGS : for a given * generated * target image , the distribution of errors using a lot of random initializations ( to demonstrate that the problem is almost convex , or at least ' easy ' to optimize ) show convergence speed vs SGD or other optimization method ( because it is was is generaly used in the literature : this finding makes it possible to experiment more easily ) , for a given distorted target generated image , the average / median error vs the distorsion : : you did that on training image but it would be nice to show it first on generated images , to show robustnessdiscuss difference LBFGS vs SGD : LBFGS is much faster to converge , not prone to gradient step setting , but can suffer from instabilitiesadd experiments on Auto Encoder : visual recovery results , histograms , MRE values , and maybe FID if we have time ( sampling from the latent space)add a small paragraph to explain experimental settings , i.e. that we reproduce several architectures from the literature and trained again on splitting , ... tell somewhere the difference ( I think in the intro ) between over - fitting and verbatim memorization , for which we provide a solid definition : add more experiments on LSUN : show histograms with PG - GAN without GAP , show recovery failure resultsadd experiments on MNIST , CIFAR ?"
FJ,friendly jamming,TR-12713,"Major abbreviations 8.7cmSlX mygray Abbreviation & mygray Description ACA - A & Alternative Ascending Clock Auction ACA - T & Traditional Ascending Clock Auction CA / CDF & Cryptographic Authority / Cumulative Distribution Function CIA & Confidentiality , Integrity and Availability triad CRN & Cognitive Ratio Network DoS / DDoS & Denial - of - Service / Distributed Denial - of - Service EBV&Encrypted Bit Vector FJ & Friendly Jamming HMAC&Hash Message Authentication Code KKT & Karush - Kuhn - Tucker LSA & Licensed Shared Access MANET & Mobile Ad hoc NETwork NUM & Network Utility Maximization OPE & Order Preserving Encryption PD / PU / SU&Primary Destination / Primary User / Secondary User SINR & Signal - to - Interference - plus - Noise Ratio SSDF & Spectrum Sensing Data Falsification TLC / TTP & Time Lapse Cryptography / Trusted Third Party VCG / GSP & Vickrey - Clarke - Groves / Generalized Second - Price auction Overview of wireless security issuesWireless networks play an extremely important role in many applications ."
ASA,adaptive segmentation algorithm,TR-12809,"htp]Results of the proposed method that shows the number of accepted images from active set in every iterartion , and the error rate ( ) on a test set ( Mouse i d 17)[][ASA][][deep learning ] Example from our data set , where a ) the ASA masks contour overlaid on manual annotation image ( counted neurons have blue marks ) , b ) the iterative deep learning predicted masks contour ( accepted on the fifth iteration of our iterative deep learning based unbiased stereology ) overlaid on manual annotation imageTest mouse cells count using manual , ASA , and Unet ( deep learning ) [ !"
ECC,error correcting code,TR-12836,"0.4intable tabularplcccccc cccccc & & 6cTesla K40c ( ECC on ) & 6cGeForce GTX 1080 ( r)3 - 8(l)9 - 14 & & 3cKey - only & 3cKey - value & 3cKey - only & 3cKey - value ( r)3 - 5 ( r)6 - 8 ( l)9 - 11 ( r)12 - 14 & & 6cNumber of buckets ( m ) & 6cNumber of buckets ( m ) ( r)3 - 8 ( l)9 - 14 Algorithm & Stage & 2 & 8 & 32 & 2 & 8 & 32 & 2 & 8 & 32 & 2 & 8 & 32 4*DMS & Pre - scan & 1.40 & 1.53 & 3.98 & 1.40 & 1.53 & 3.98 & 0.61 & 0.72 & 1.80 & 0.61 & 0.72 & 1.80 & Scan & 0.13 & 0.39 & 1.47 & 0.13 & 0.39 & 1.47 & 0.10 & 0.31 & 1.16 & 0.09 & 0.31 & 1.16 & Post - scan & 2.29 & 2.94 & 4.85 & 3.34 & 4.05 & 11.84 & 1.19 & 2.02 & 3.10 & 2.29 & 3.71 & 6.60 & Total & 3.82 & 4.86 & 10.29 & 4.87 & 5.97 & 17.28 & 1.90 & 3.05 & 6.06 & 3.00 & 4.74 & 9.56 4*WMS & Pre - scan & 0.79 & 0.93 & 1.38 & 0.89 & 0.97 & 1.39 & 0.58 & 0.60 & 0.93 & 0.59 & 0.62 & 0.93 & Scan & 0.05 & 0.08 & 0.40 & 0.06 & 0.13 & 0.39 & 0.04 & 0.06 & 0.31 & 0.04 & 0.10 & 0.31 & Post - scan & 1.85 & 2.38 & 2.66 & 3.09 & 4.06 & 5.53 & 1.15 & 1.20 & 1.51 & 2.32 & 2.38 & 2.94 & Total & 2.69 & 3.39 & 4.43 & 4.04 & 5.16 & 7.31 & 1.77 & 1.87 & 2.75 & 2.95 & 3.11 & 4.17 4*BMS & Pre - scan & 0.88 & 0.84 & 1.11 & 0.83 & 0.93 & 1.35 & 0.57 & 0.58 & 0.62 & 0.57 & 0.58 & 0.62 & Scan & 0.04 & 0.05 & 0.08 & 0.04 & 0.05 & 0.08 & 0.03 & 0.04 & 0.06 & 0.03 & 0.04 & 0.06 & Post - scan & 3.04 & 3.28 & 3.97 & 3.78 & 4.37 & 5.08 & 1.22 & 1.27 & 1.33 & 2.27 & 2.29 & 2.36 & Total & 3.96 & 4.17 & 5.15 & 4.65 & 5.35 & 6.52 & 1.82 & 1.89 & 2.02 & 2.88 & 2.90 & 3.04 4*RB - sort & Labeling & 1.69 & 1.67 & 1.67 & 1.69 & 1.67 & 1.67 & 1.16 & 1.15 & 1.14 & 1.13 & 1.15 & 1.13 & Sorting & 4.39 & 4.87 & 6.98 & 5.81 & 7.17 & 10.58 & 2.97 & 3.00 & 3.11 & 4.11 & 4.16 & 4.38 & ( un)Packing & - & - & - & 5.66 & 5.67 & 5.67 & - & - & - & 4.51 & 4.50 & 4.52 & Total & 6.08 & 6.53 & 8.65 & 13.13 & 14.51 & 17.92 & 4.13 & 4.15 & 4.24 & 9.75 & 9.81 & 10.04 tabular Average running time ( ms ) for different stages of our multisplit approaches and reduced - bit sort , with and a varying number of buckets.table:timingtable"
MD,mean diffusivity,TR-12904,table [ ] tabularlll 3cAmyloid Load ( PiB Positivity ) Set 1 & PiB Angular L / R & PiB Cingulum Ant L / R & PiB Cingulum Post L / R & PiB Frontal Med Orb L / R & PiB Precuneus L / R & PiB Temporal Sup L / R & PiB Temporal Mid L / R & PiB SupraMarginal L Set 2 & FA Cerebral peduncle R & FA Cerebral peduncle L & MD Corticospinal tract R & MD Corticospinal tract L & Trail - Making Test Part A Score & MD Cerebral peduncle R & PET Cingulum Post R & tabular Group difference across Amyloid Load ( PiB Positivity ) tab : wrapPIBtableC ) Graph Scan Statistics on slope differences across amyloid load positivity .
CNN,convolutional neural network,TR-12942,"& LinearRegression & LogisticRegression & NN & CNN 2*Training ( ) & ABY3 & & & & & This & & & & 2*Prediction ( ) & ABY3 & & & & & This & & & & tabular Total Online Runtime ( in seconds ) of ABY3 ( Malicious ) and This for Training and Prediction of Linear , Logistic , NN , and CNN models for ( lower = better ) over a WAN setting.tab:Total_RuntimetableComparison with the ML framework of ABY3 MR18 in the semi - honest settingapp : Comp_ABY3SemiWe compare the performance of our protocol with the semi - honest version of ABY3 , giving them an advantage in terms of the threat model ."
FJ,friendly jamming,TR-13169,"Major abbreviations 8.7cmSlX mygray Abbreviation & mygray Description ACA - A & Alternative Ascending Clock Auction ACA - T & Traditional Ascending Clock Auction CA / CDF & Cryptographic Authority / Cumulative Distribution Function CIA & Confidentiality , Integrity and Availability triad CRN & Cognitive Ratio Network DoS / DDoS & Denial - of - Service / Distributed Denial - of - Service EBV&Encrypted Bit Vector FJ & Friendly Jamming HMAC&Hash Message Authentication Code KKT & Karush - Kuhn - Tucker LSA & Licensed Shared Access MANET & Mobile Ad hoc NETwork NUM & Network Utility Maximization OPE & Order Preserving Encryption PD / PU / SU&Primary Destination / Primary User / Secondary User SINR & Signal - to - Interference - plus - Noise Ratio SSDF & Spectrum Sensing Data Falsification TLC / TTP & Time Lapse Cryptography / Trusted Third Party VCG / GSP & Vickrey - Clarke - Groves / Generalized Second - Price auction Overview of wireless security issuesWireless networks play an extremely important role in many applications ."
SD,selection diversity,TR-13189,"In particular , by first deriving using Eq : PDF_best_Distinct the following closed - form expression for the MGF of in INID Nakagami- fading with integer values of 's and distinct ' s : a closed - form expression for the MGF of of repetitive transmission with SD over INID Nakagami- fading channels with integer values of 's and distinct 's is given byFor equal 's , i.e. , IID Nakagami- fading with and , following a similar procedure as for the derivation of Eq : MGF_end_Final_repSD and using the binomial and multinomial theorems , we first obtain the following closed - form expression for for integer where , , and symbol is used for short - hand representation of multiple summations ."
RTF,region templates framework,TR-13489,"The specific contributions of this work are presented below with a reference to the section in which they are described : enumerate A graphical user interface for simplifying the deployment of workflows for the RTF , which is coupled with a code generator that allows the flexible use of the RTF on distinct domains [ Section sec : improve ] ; The development and analysis of multi - level reuse algorithms : enumerate A coarse - grain merging algorithm was implemented [ Section sec : stage - merging ] ; A fine - grain Naive Merging Algorithm was proposed and implemented [ Section sec : naive - merging ] ; The fine - grain Smart Cut Merging Algorithm was proposed and implemented [ Section sec : sca ] ; The fine - grain Reuse - Tree Merging Algorithm was proposed and implemented [ Section sec : rtma ] ; enumerate Proposal and implementation of the Task - Balanced Reuse - Tree Merging Algorithm that reduces the issue of loss of parallelism due to load imbalance provoked by the Reuse - Tree Merging Algorithm [ Section sec : TRTMA ] ; The performance gains of the proposed algorithms with a real - world microscopy image analysis application were demonstrated using different SA strategies ( e.g MOAT and VBD ) at different scales ."
PI,provider independent,TR-13533,"On the other hand , if the fee is set to the largest cost per address currently used by the RIRs ( e.g. , to the cost per address used in /48 PI allocations ) , this would render the cost of a larger block impractically high ( the cost of a /32 would be tens of millions of US if the cost per address of a /48 is used).It is challenging for the InBlock to have different cost per address depending on the size of the allocation , because this may incentivize applications for larger blocks even when not needed , resulting in address waste ( note that we do not have a complementary mechanism such as a need assessment procedure , to modulate user requests ) ."
IR,information retrieval,TR-13609,"arrows , shapes , backgroundstikzmarkcalcvertex=[ellipse , fill = black!25,minimum size=20pt , inner sep=0pt]edge = [ draw , thin,-]glabel = [ text width=1cm , text centered , font=]bg bg , main [ 1]switch ( # 1)switchcaseassert[1](#1)SE[SWITCH]SwitchEndSwitch[1 ] # 1 SE[CASE]CaseEndCase[1 ] # 1 * EndSwitch*EndCase[1 ] # 1e.gi.e[scale=0.4](0,.35 ) - ( .25,0 ) - ( 1,.7 ) - ( .25,.15 ) - cycle ; language = java , basicstyle = pcrblack , keywordstyle= , commentstyle= , numbers = none , numberstyle= , backgroundcolor= , showspaces = false , showstringspaces = false , showtabs = false , frame = single , tabsize=2 , rulesepcolor= , captionpos = b , breaklines = true , breakatwhitespace = false , failed to patchfailed to patch[Improving Bug Localization with Context - Aware Query Reformulation]Improving IR - Based Bug Localization with Context - Aware Query ReformulationMohammad Masudur RahmanUniversity of SaskatchewanSaskatoon , Canadamasud.rahman@usask.ca Chanchal K. RoyUniversity of SaskatchewanSaskatoon , Canadachanchal.roy@usask.caRecent findings suggest that Information Retrieval ( IR)-based bug localization techniques do not perform well if the bug report lacks rich structured information ( relevant program entity names ) ."
TP,temporal pooler,TR-13659,"tabular l l Variable & Description & Length of the past & Length of the lookbehind part & ( past + current step ) & Length of the future & Length of the whole sequence & Index of a layer & Index of an Expert in the layer & Set of cluster centers ( states ) & of an Expert & Dimension of , number of & cluster centers & Sequence of complete observations & Sequence of observations of & Expert in layer & Hidden state of the Expert in layer & Output vector of the Expert in layer & Number of sequences considered in a TP & Set of all providers of context to & an Expert & Likelihoods of seeing each context & element from each provider in each & position of each sequence tabulart : notationSelected notation ."
ECC,error correcting code,TR-13668,tabularlll cccccccc & & & 8cThroughput ( speedup against radix - sort ) 4 - 11 & & & 8cNumber of buckets ( m ) 4 - 11 & & Method & 2 & 4 & 8 & 16 & 32 & 64 & 128 & 256 8*turn90K40c ( ECC on)turn & 4*turn90key - onlyturn & DMS & 8.79 ( 6.8 x ) & 8.36 ( 6.5 x ) & 6.91 ( 5.4 x ) & 6.90 ( 5.4 x ) & 3.26 ( 2.5 x ) & - & - & - & & WMS & 12.48 ( 9.7 x ) & 9.79 ( 7.6 x ) & 9.90 ( 7.7 x ) & 8.71 ( 6.8 x ) & 7.57 ( 5.9 x ) & - & - & - & & BMS & 8.47 ( 6.6 x ) & 8.39 ( 6.5 x ) & 8.05 ( 6.2 x ) & 7.72 ( 6.0 x ) & 6.51 ( 5.0 x ) & 5.14 ( 4.0 x ) & 3.61 ( 2.8 x ) & 2.50 ( 1.9 x ) & & RB - sort & 5.52 ( 4.3 x ) & 5.49 ( 4.3 x ) & 5.14 ( 4.0 x ) & 4.44 ( 3.4 x ) & 3.88 ( 3.0 x ) & 2.80 ( 2.2 x ) & 2.70 ( 2.1 x ) & 2.50 ( 1.9 x ) 2 - 11 & 4*turn90key - valueturn & DMS & 6.90 ( 9.0 x ) & 6.31 ( 8.2 x ) & 5.62 ( 7.3 x ) & 5.62 ( 7.3 x ) & 1.94 ( 2.5 x ) & - & - & - & & WMS & 8.31 ( 10.8 x ) & 8.01 ( 10.4 x ) & 6.51 ( 8.5 x ) & 5.90 ( 7.7 x ) & 4.59 ( 6.0 x ) & - & - & - & & BMS & 7.22 ( 9.4 x ) & 6.98 ( 9.1 x ) & 6.27 ( 8.1 x ) & 5.68 ( 7.4 x ) & 5.15 ( 6.7 x ) & 4.62 ( 6.0 x ) & 3.09 ( 4.0 x ) & 1.82 ( 2.4 x ) & & RB - sort & 2.56 ( 3.3 x ) & 2.47 ( 3.2 x ) & 2.31 ( 3.0 x ) & 2.01 ( 2.6 x ) & 1.87 ( 2.4 x ) & 1.47 ( 1.9 x ) & 1.41 ( 1.8 x ) & 1.29 ( 1.7 x ) 8*turn90K40c ( ECC off)turn & 4*turn90key - onlyturn & DMS & 8.99 ( 5.2 x ) & 8.52 ( 4.9 x ) & 6.98 ( 4.0 x ) & 4.94 ( 2.9 x ) & 3.26 ( 1.9 x ) & - & - & - & & WMS & 14.15 ( 8.2 x ) & 11.74 ( 6.8 x ) & 11.65 ( 6.7 x ) & 8.68 ( 5.0 x ) & 7.57 ( 4.4 x ) & - & - & - & & BMS & 8.74 ( 5.1 x ) & 8.59 ( 5.0 x ) & 8.07 ( 4.7 x ) & 7.69 ( 4.4 x ) & 6.47 ( 3.7 x ) & 5.10 ( 2.9 x ) & 3.59 ( 2.1 x ) & 2.48 ( 1.4 x ) & & RB - sort & 6.42 ( 3.7 x ) & 6.40 ( 3.7 x ) & 6.37 ( 3.7 x ) & 6.30 ( 3.6 x ) & 5.70 ( 3.3 x ) & 3.72 ( 2.2 x ) & 3.72 ( 2.1 x ) & 3.69 ( 2.1 x ) 2 - 11 & 4*turn90key - valueturn & DMS & 8.99 ( 7.7 x ) & 7.05 ( 6.0 x ) & 5.71 ( 4.9 x ) & 3.98 ( 3.4 x ) & 1.96 ( 1.7 x ) & - & - & - & & WMS & 9.58 ( 8.2 x ) & 8.90 ( 7.6 x ) & 7.55 ( 6.5 x ) & 6.78 ( 5.8 x ) & 4.57 ( 3.9 x ) & - & - & - & & BMS & 7.23 ( 6.2 x ) & 6.99 ( 6.0 x ) & 6.28 ( 5.4 x ) & 5.66 ( 4.8 x ) & 5.13 ( 4.4 x ) & 4.59 ( 3.9 x ) & 3.06 ( 2.6 x ) & 1.81 ( 1.5 x ) & & RB - sort & 2.98 ( 2.6 x ) & 2.98 ( 2.5 x ) & 2.78 ( 2.4 x ) & 2.63 ( 2.2 x ) & 2.67 ( 2.3 x ) & 1.92 ( 1.6 x ) & 1.84 ( 1.6 x ) & 1.76 ( 1.5 x ) 8*turn90GTX 1080 turn & 4*turn90key - onlyturn & DMS & 17.67 ( 5.2 x ) & 14.38 ( 4.2 x ) & 11.00 ( 3.2 x ) & 7.73 ( 2.3 x ) & 5.54 ( 1.6 x ) & - & - & - & & WMS & 18.93 ( 5.6 x ) & 17.54 ( 5.2 x ) & 17.98 ( 5.3 x ) & 16.18 ( 4.8 x ) & 12.20 ( 3.6 x ) & - & - & - & & BMS & 18.42 ( 5.4 x ) & 17.84 ( 5.2 x ) & 17.79 ( 5.2 x ) & 18.01 ( 5.3 x ) & 16.64 ( 4.9 x ) & 14.14 ( 4.2 x ) & 11.43 ( 3.4 x ) & 7.05 ( 2.1 x ) & & RB - sort & 8.13 ( 2.4 x ) & 8.13 ( 2.4 x ) & 8.09 ( 2.4 x ) & 8.06 ( 2.4 x ) & 7.91 ( 2.3 x ) & 7.51 ( 2.2 x ) & 6.43 ( 1.9 x ) & 4.51 ( 1.3 x ) 2 - 11 & 4*turn90key - valueturn & DMS & 11.17 ( 5.9 x ) & 9.75 ( 5.1 x ) & 7.07 ( 3.7 x ) & 4.95 ( 2.6 x ) & 3.51 ( 1.8 x ) & - & - & - & & WMS & 11.38 ( 6.0 x ) & 11.21 ( 5.9 x ) & 10.81 ( 5.7 x ) & 10.37 ( 5.5 x ) & 8.04 ( 4.2 x ) & - & - & - & & BMS & 11.67 ( 6.1 x ) & 11.62 ( 6.1 x ) & 11.57 ( 6.1 x ) & 11.40 ( 6.0 x ) & 11.04 ( 5.8 x ) & 10.64 ( 5.6 x ) & 9.78 ( 5.1 x ) & 5.85 ( 3.1 x ) & & RB - sort & 3.44 ( 1.8 x ) & 3.44 ( 1.8 x ) & 3.42 ( 1.8 x ) & 3.40 ( 1.8 x ) & 3.34 ( 1.8 x ) & 3.19 ( 1.7 x ) & 2.83 ( 1.5 x ) & 2.31 ( 1.2 x ) tabular Multisplit with delta - buckets and random keys uniformly distributed among buckets .
PSC,pittsburgh supercomputing center,TR-13904,"adjustboxmax width= tabularc c c c c c c c c Name & Nodes & Number of Nodes & CPUs & RAM & Network Topology & Scheduler and Resource Manager & parallelfile system SDSC Comet & Compute & 6400 & 2 Intel Xeon ( E5 - 2680v3 ) 12 cores / CPU , 2.5 GHz & 128 GB DDR4 DRAM & 56 Gbps IB & SLURM & Lustre PSC Bridges & RSM & 752 & 2 Intel Haswell ( E5 - 2695 v3 ) 14 cores / CPU , 2.3 GHz & 128 GB , DDR4 - 2133MHz & 12.37 Gbps OPA & SLURM & Lustre LSU SuperMIC & Standard & 360 & 2 Intel Ivy Bridge ( E5 - 2680 ) 10 cores / CPU , 2.8 GHz & 64 GB , DDR3 - 1866MHz & 56 Gbps IB & PBS & Lustre tabular adjustbox [ Configuration of HPC resources ] Configuration of the HPC resources that were benchmarked ."
FEC,forward error correction,TR-14105,"table[!h ] Average SSIM , VQM , and network footprint center tabularlccccc & 1lSHIELD & 1lCORVETTE & 1lVaUEP & 1lVaEEP & 1lWithout FEC 6cUrban environment SSIM & 0,895 & 0,787 & 0,701 & 0,684 & 0,551 VQM & 1,459 & 2,441 & 4,034 & 4,323 & 6,688 Overhead & 17,333 & 21,778 & 46,660 & 65,984 & - 6cHighway environment SSIM & 0,911 & 0,809 & 0,744 & 0,729 & 0,627 VQM & 1,328 & 2,095 & 3,414 & 3,728 & 5,281 Overhead & 12,333 & 17,112 & 46,660 & 65,984 & - tabular tab : shield : Sumary center tableSummaryThis chapter described and assessed two proposed mechanisms to increase the video transmission resiliency over VANETs ."
US,uncertainty sampling,TR-14177,0.80tabularcccccccccccccc & 6cWhole abstract & 6cSentence & 4*Avg 2 - 13 & 3c2*CNN for CNN & 3cCNN for & 3c2*CNN for CNN & 3cCNN for & & 3c & 3cBiLSTM - CRF & 3c & 3cBiLSTM - CRF & 2 - 13 & Val & Test & Pseudo&Val & Test & Pseudo&Val & Test & Pseudo&Val & Test & Pseudo & EDG & 61.6 & 61.9 & 55.7 & 68.7 & 68.2 & 59.2 & 62.1 & 60.2 & 56.1 & 70.0 & 68.2 & 59.8 & 62.4EDGext1 ( w/o Val ) & 60.4 & 61.5 & 55.4 & 67.4 & 67.0 & 58.6 & 61.9 & 60.0 & 55.2 & 69.0 & 69.2 & 59.1 & 62.0 RND & 56.5 & 57.9 & 51.3 & 64.9 & 64.1 & 55.7 & 58.8 & 57.2 & 54.1 & 65.4 & 65.2 & 56.5 & 59.0Div & 57.9 & 57.8 & 54.2 & 64.4 & 64.6 & 57.2 & 59.3 & 58.4 & 53.8 & 66.2 & 65.7 & 57.0 & 59.7 US & 60.9 & 60.9 & 54.9 & 68.1 & 68.5 & 58.3 & 60.0 & 59.6 & 54.0 & 69.7 & 68.5 & 59.9 & 61.9US+Div & 61.1 & 60.4 & 56.4 & 65.4 & 66.8 & 58.3 & 60.5 & 58.9 & 55.4 & 70.1 & 68.1 & 59.3 & 61.7 US+Div+EDGext2 & 61.9 & 60.1 & 56.8 & 71.7 & 66.1 & 59.1 & 63.1 & 60.8 & 56.8 & 70.6 & 68.6 & 60.1 & 63.0 BALD & 61.0 & 61.4 & 56.4 & 68.9 & 69.4 & 58.8 & 61.0 & 59.7 & 55.1 & 70.9 & 67.8 & 60.1 & 62.5 BALD+EDGext2 & 61.3 & 60.0 & 56.9 & 69.4 & 66.5 & 59.5 & 65.0 & 62.2 & 55.9 & 72.4 & 68.7 & 59.7 & 63.1 tabularThe experiment setup is the same as Table tb : pseudo_exp except that the performances are the maximal micro - F1 ( ) of 5 neural networks rather than their average .
NC,next corollary,TR-14582,"However , even when faulty nodes are able to tamper message paths or even fake and transmit non - existing messages , as long as ( i ) the number of faked messages is finite ( each faulty node can not create too many non - existing messages);and ( ii ) for each message tampered / faked by the faulty node , must satisfy , i.e. , the faulty node can not conceal itself from the message path , using the same line of arguments as in Section and Section , it can be shown that the Condition NC is also necessary and sufficient for the existence of approximate consensus under the relaxed model ."
MRE,median recovery error,TR-14607,"0 ToDo look at distribution of attributes in the latent space If we have time : Do a figure with auto - encoder ( if we manage to sample the latent space , for instance by fitting a Gaussian to the encoded images in the latent space ) Show a picture of a few examples of Eiffel tower or london bridge in LSUN that seem to be memorize but are just samples learn from template show ( at least on some images ) that VGG , perceptual loss , L1 or L1 on Laplacian pyramid gives the same results for recovery because GLO reported some mitigated results on this ( sometimes L2 is better , sometimes pyramid : in the end , they mixed the two metrics ) give statistics about the recovery precision with LBFGS : for a given * generated * target image , the distribution of errors using a lot of random initializations ( to demonstrate that the problem is almost convex , or at least ' easy ' to optimize ) show convergence speed vs SGD or other optimization method ( because it is was is generaly used in the literature : this finding makes it possible to experiment more easily ) , for a given distorted target generated image , the average / median error vs the distorsion : : you did that on training image but it would be nice to show it first on generated images , to show robustness discuss difference LBFGS vs SGD : LBFGS is much faster to converge , not prone to gradient step setting , but can suffer from instabilities add experiments on Auto Encoder : visual recovery results , histograms , MRE values , and maybe FID if we have time ( sampling from the latent space ) add a small paragraph to explain experimental settings , i.e. that we reproduce several architectures from the literature and trained again on splitting , ... tell somewhere the difference ( I think in the intro ) between over - fitting and verbatim memorization , for which we provide a solid definition : add more experiments on LSUN : show histograms with PG - GAN without GAP , show recovery failure results add experiments on MNIST , CIFAR ?"
PC,principal component,TR-14616,"@X rrr rrr @ & 3 c[c ] PC 0 , , & 3c[c]PC 1 , , ( lr)2 - 4(lr)5 - 7 & coeff & p val & LMG & coeff & p val & LMG extraversion & & & 0.8 & & 0.03 & 0.38 openness & & 0.01 & 0.07 & & 0.5 & 0.08 neuroticism & & 0.7 & 0.07 & & 0.01 & 0.48 agreeableness & & 0.5 & 0.03 & & 1.0 & 0.01 conscientiousness & & 1.0 & 0.03 & & 0.6 & 0.04 T=30 , Extraversion , openness , and neuroticism explain spatial behaviour ."
LDE,learnable dictionary encoding,TR-14693,table*[htb]centertabularllccccc & Front - end model & Loss & Dims & Aggregation & Training set & EER ( ) VoxCeleb1 test set Nagrani Nagrani17 & I - vectors + PLDA & - & - & - & VoxCeleb1 & 8.8Nagrani Nagrani17 & VGG - M & Softmax & 1024 & TAP & VoxCeleb1 & 10.2Cai cai2018exploring & ResNet-34 & A - Softmax + PLDA & 128 & TAP & VoxCeleb1 & 4.46 Cai cai2018exploring & ResNet-34 & A - Softmax + PLDA & 128 & SAP & VoxCeleb1 & 4.40 Cai cai2018exploring & ResNet-34 & A - Softmax + PLDA & 128 & LDE & VoxCeleb1 & 4.48 Okabe okabe2018attentive & TDNN ( x - vector ) & Softmax & 1500 & TAP & VoxCeleb1 & 4.70 Okabe okabe2018attentive & TDNN ( x - vector ) & Softmax & 1500 & SAP & VoxCeleb1 & 4.19 Okabe okabe2018attentive & TDNN ( x - vector ) & Softmax & 1500 & ASP & VoxCeleb1 & 3.85 Hajibabaei hajibabaei2018unified & ResNet-20 & A - Softmax & 128 & TAP & VoxCeleb1 & 4.40 Hajibabaei hajibabaei2018unified & ResNet-20 & AM - Softmax & 128 & TAP & VoxCeleb1 & 4.30 Chung Chung18a & ResNet-34 & Softmax + Contrastive & 512 & TAP & VoxCeleb2 & 5.04 Chung Chung18a & ResNet-50 & Softmax + Contrastive & 512 & TAP & VoxCeleb2 & 4.19 Ours & Thin ResNet-34 & Softmax & 512 & TAP & VoxCeleb2 & 10.48 Ours & Thin ResNet-34 & Softmax & 512 & NetVLAD & VoxCeleb2 & 3.57 Ours & Thin ResNet-34 & AM - Softmax & 512 & NetVLAD & VoxCeleb2 & 3.32 Ours & Thin ResNet-34 & Softmax & 512 & GhostVLAD & VoxCeleb2 & 3.22 Ours & Thin ResNet-34 & AM - Softmax & 512 & GhostVLAD & VoxCeleb2 & 3.23 VoxCeleb1-EC Chung Chung18a & ResNet-50 & Softmax + Contrastive & 512 & TAP & VoxCeleb2 & 4.32 Ours ( cleaned ) & Thin ResNet-34 & Softmax & 512 & GhostVLAD & VoxCeleb2 & 3.13 VoxCeleb1-HC Chung Chung18a & ResNet-50 & Softmax + Contrastive & 512 & TAP & VoxCeleb2 & 7.22 Ours ( cleaned ) & Thin ResNet-34 & Softmax & 512 & GhostVLAD & VoxCeleb2 & 5.06 tabularcenterResults for verification on the original VoxCeleb1test set Nagrani17 and the extended and hardtest sets ( VoxCeleb - E and VoxCeleb - H ) Chung18a .
GCN,graph convolution networks,TR-14754,"For evaluating structure fusion generalization , we compare structure fusion based graph convolutional networks ( SF - GCN ) , propagation fusion based graph convolutional networks ( PF - GCN ) and structure propagation fusion based graph convolutional networks ( SPF - GCN).In Table , we observe that the performance of SPF - GCN is better than that of other method , and the least improvement of SPF - GCN respectively is for Cora , for Citeseer and for PubMed , while the performance of SP is superior to that of PF - GCN , and the improvement of SF - GCN respectively is for Cora , for Citeseer and for PubMed Therefore , PF and SF both are benefit for further mining the structure information and the role of SF is more important than that of PF ."
MAP,mean average precision,TR-14892,adjustboxmax width=0.8tabularl c c c c c c2*Method & 3cRobust04 & 3cClueWeb ( lr)2 - 4 ( lr)5 - 7 & MAP & P@20 & nDCG@20 & MAP & P@20 & nDCG@20 RankSVM + & 0.1983 & 0.2841 & 0.3375 & 0.0761 & 0.1840 & 0.1637 RankSVM + & 0.2307 & 0.3260 & 0.3794 & 0.0862 & 0.2170 & 0.1939 RankSVM + ( Pretrained ( external ) + IDF weighting ) & 0.1539 & 0.2121 & 0.1852 & 0.0633 & 0.1572 & 0.1494 ( one layer with no nonlinearity ) + & 0.2103 & 0.3986 & 0.3160 & 0.0645 & 0.1421 & 0.1322 tabularadjustboxtable *
FC,fully connected,TR-14913,"table[ht]Sparsity for each layer in ResNet-18 on Cifar10 datasettabularccccccccconv1 & conv2 - 5 & conv6 - 9 & conv10 - 13 & con14 - 17 & FC layer & Total & Test error 22.80 & tabular[c]@c@10.06 22.87 20.05 12.99tabular & tabular[c]@c@9.24 5.45 4.94 10.73tabular & tabular[c]@c@20.64 15.04 10.77 4.61tabular & tabular[c]@c@1.43 0.19 0 0tabular & 10.33 & 2.96 & tabular[c]@c@6.58 ( baseline ) 6.23 ( our method)tabular tabulartable : res18tableEfficient Hessian Computationappdendix : hessian calculation methodsCompute the Hessian of FC Layerappendix : fchessianThe mathematical operation in a fully - connected layer could be formulated as : aligneq : fc_feedforward h_j^o = W_ij^o a_i , a_i = ( h_i ) alignwhere is the pre - activation value for node and is the activation value ."
FEC,forward error correction,TR-14919,"figure[!htb ] center ./loss_distribution_MINT.eps center MINT - FEC 's experiment PLR distribution fig : MINT : lossDistfiguretable[!ht ] MINT - FEC Simulation parameters center tabularll Parameters & Value Display sizes & 1920x1080 , 1280x720 , and 800x600 Frame rate mode & Constant Frame rate & 29.970 fps GoP & 19:2 Video format & H.264 Codec & x264 Container & MP4 Propagation model & FriisPropagationLossModel Mobility model & Gauss - Markov UAV velocity & 45 - 65 km / h ( 28 - 40 mph ) LTE Frequency band & 800MHz LTE Mode & FDD LTE Bandwidth & 5 MHz eNodeB Operating Power & 22 dBm Antenna Gain & 16 dBi tabular tab : MINT : parameters center tableFive different schemes were simulated as follows : ( 1 ) without any FEC mechanism ."
RPE,relative pose error,TR-14984,"tabularcY1.4cmY1.4cmY1.4cmY1.4cmY1.4cmY1.4 cm Sequence & 3c09 & 3c10 1*Method & ATE(m ) & RPE(m ) & RPE ( ) & ATE(m ) & RPE(m ) & RPE ( ) Ours(noconf ) & 53.40 & 0.356 & 0.931 & 58.50 & 0.308 & 1.058 Ours(noconf , iterative ) & 33.18 & 0.248 & 0.421 & 35.87 & 0.280 & 0.803 Flownet2.0ilg2017flownet & 29.64 & 0.349 & 0.838 & 51.90 & 0.222 & 0.954 Flownet2.0(iterative)ilg2017flownet & 24.61 & 0.185 & 0.400 & 22.61 & 0.142 & 0.484 EpicFlowrevaud2015epicflow & 119.0 & 0.566 & 0.931 & 20.98 & 0.199 & 0.853 EpicFlow(iterative)revaud2015epicflow & 59.79 & 0.379 & 0.459 & 14.80 & 0.154 & 0.581 Ours(full - single iteration ) & 31.20 & 0.089 & 0.324 & 24.10 & 0.095 & 0.389 Ours(full - til convergence ) & 16.55 & 0.047 & 0.128 & 9.846 & 0.039 & 0.138 tabular tab : kitti_confidencetableWe also demonstrate qualitatively one of the ways in which estimating confidence improves our pose estimation in Figure fig : flow_confidence ."
RL,reinforcement learning,TR-15172,"Summary of main resultsfidelReinforcement Learning framework[leftmargin=5 mm ] for optimizing and adapting QEC codes , using arbitrary topological QEC codes , arbitrary decoders noise models , adaptable to any optimizer ( RL paradigm ) implementable on arbitrary platforms , applicable off - line ( simulation ) in - situ Simulations using[leftmargin=5 mm ] surface code quantum memory up to 68 fully connected data qubits , optimal linear - time decoder ( SQUAB ) Projective Simulation model for RLSimulations demonstrate agent 's ability[leftmargin=5 mm ] to determine optimal QEC codes for simple standard noise channels as well as non - isotropic noise , and for transfer learning in changing environmentsThe decoder is optimal for the simplified approximate error model that we use for faster estimation of the logical error rate , see Sec ."
LTE,long term evolution,TR-15248,"theoremTheoremlemma[theorem]Lemmadefinition[theorem]DefinitionpropositionPropositioncorollary[theorem]CorollaryremarkRemark*noteNote1 * argmax*argmin op - tical net - works semi - conduc - torPioneering Studies on LTE eMBMS : Towards 5 G Point - to - Multipoint TransmissionsHongzhi Chen1 , De Mi1 , Manuel Fuentes2 , David Vargas 3 , Eduardo Garro4 , Jose Luis Carcel4 , Belkacem Mouhouche2 , Pei Xiao1 and Rahim Tafazolli1 1Institute for Communication Systems , University of Surrey , United Kingdom 2Samsung Electronics RD UK , United Kingdom 3BBC RD , United Kingdom 4Institute of Telecommunications and Multimedia Applications , Universitat Politecnica de Valencia , Spain Email : hongzhi.chen , d.mi , p.xiao , r.tafazolli@surrey.ac.uk , m.fuentes , b.mouhouche@samsung.com , david.vargas@bbc.co.uk , edgarcre , jocarcer@iteam.upv.esThe first 5 G ( 5th generation wireless systems ) New Radio Release-15 was recently completed ."
BN,bayesian network,TR-15286,"The main benefits of SV over first - order and total effect sensitivity measures include : ( 1 ) the uncertainty contributions sum up to total variance of output ; ( 2 ) SV can automatically account for probabilistic dependence and structural interactions occurring in the complex production process ; and ( 3 ) combing SV with BN ( represented by BN - SV ) can facilitate the appropriate and interpretable risk and sensitivity analysis since BN is built based on underlying physical mechanics causing the interdependence of raw material quality , production process , and bio - drug properties ."
RV,random vaccination,TR-15345,"tikzpicture customlegend[legend columns=4,legend style = at=(0.12,1.02),draw = none , column sep=3ex , line width=2pt , font= , legend entries = RV , AV , IMV , DV , direct , indirect ] solid , color = blue color = red color = green color = magenta color = black dashdotted , color = black customlegend tikzpicture pvan_a.pdfpvan_b.pdfpvan_c.pdfpvan_d.pdfAverage outbreak sizes at various vaccination rates ( percentage of total nodes ) of different strategies : ( A , B ) nodes are vaccinated with contacts created for direct interactions and ( C , D ) comparison of outbreak sizes for vaccinating nodes with contacts based on the direct interactions ( solid lines ) and contact based on any direct or indirect interactions ( dashed lines)-1.5emfig : avacfigurefigure[h ! ]"
FEC,forward error correction,TR-15355,"The information is collected by the receiver and sent to the transmitter ; Loss Rate Prediction - Using the feedback statistics , the properties of the error probability are estimated on the server side ; Video Characteristics - This module fetches information from the video sequences that are being transmitted to identify video characteristics such as the frame type and size , as well as the motion vectors ; Ant Colony Optimization - The ACO is responsible for making a joint analysis of all the information gathered by the other modules , establishing the most suitable amount of redundancy to each FEC block ; FEC Blocks - The FEC blocks are built and a specific amount of redundancy designed by the ACO is assigned to each one ."
MRE,median recovery error,TR-15380,"0 ToDo look at distribution of attributes in the latent space If we have time : Do a figure with auto - encoder ( if we manage to sample the latent space , for instance by fitting a Gaussian to the encoded images in the latent space ) Show a picture of a few examples of Eiffel tower or london bridge in LSUN that seem to be memorize but are just samples learn from template show ( at least on some images ) that VGG , perceptual loss , L1 or L1 on Laplacian pyramid gives the same results for recovery because GLO reported some mitigated results on this ( sometimes L2 is better , sometimes pyramid : in the end , they mixed the two metrics ) give statistics about the recovery precision with LBFGS : for a given * generated * target image , the distribution of errors using a lot of random initializations ( to demonstrate that the problem is almost convex , or at least ' easy ' to optimize ) show convergence speed vs SGD or other optimization method ( because it is was is generaly used in the literature : this finding makes it possible to experiment more easily ) , for a given distorted target generated image , the average / median error vs the distorsion : : you did that on training image but it would be nice to show it first on generated images , to show robustness discuss difference LBFGS vs SGD : LBFGS is much faster to converge , not prone to gradient step setting , but can suffer from instabilities add experiments on Auto Encoder : visual recovery results , histograms , MRE values , and maybe FID if we have time ( sampling from the latent space ) add a small paragraph to explain experimental settings , i.e. that we reproduce several architectures from the literature and trained again on splitting , ... tell somewhere the difference ( I think in the intro ) between over - fitting and verbatim memorization , for which we provide a solid definition : add more experiments on LSUN : show histograms with PG - GAN without GAP , show recovery failure results add experiments on MNIST , CIFAR ?"
ER,experience replay,TR-15505,tab : cifar_er_agem_analysistabularlcccc1lBucket & 2cNumber of Examples ( fraction of corrects ) & 2cAverage Loss & ER - Random & A - GEM & ER - Random & A - GEM ( r)2 - 3 ( l)4 - 5Both and predict correctly & 3129 ( 0.67 ) & 3129 ( 0.70 ) & 0.25 & 0.42 predicts correctly and fails & 1520 ( 0.33 ) & 1520 & 0.41 & 1.74 fails and predicts correctly & 1360 & 1360 ( 0.30 ) & 2.14 & 0.62 tabular table*figure*[tb ] subfigure0.5 center figs / cifar_acc_train.pdf Train center subfigure subfigure0.5 center figs / cifar_acc_mem.pdf Memory center subfigure subfigure1.0 center figs / cifar_acc_test.pdf Test center subfigure CIFAR Analysis : Evolution of accuracy as a function of tasks on Train/ Memory and Test sets .
DE,dataexplorer,TR-15585,table[ht]3pttabularllccccccccccccccccc Task type & Task & a & aE & DE & dM & d & EPD & e & eR & fM & i & R & SE & s & v & x 4*Dataset & Variable types & & x & x & x & x & & x & & x & x & & x & x & x & & Dimensions & & x & x & x & x & x & & & x & x & & x & & x & & Other info & & & x & & & & & & & x & & & & x & & Compare datasets & x & & & & & & & & x & x & & & & x & 5*Validity & Missing values & & x & x & x & x & x & x & & x & x & & x & x & x & x & Redundant col . &
DR,dispersion reduction,TR-15681,1cAP(IoU=0.5 ) & 1cC.R.W2 2cbaseline ( SOTA)1 & 82.5 & 73.2 & 100 & 69.2 & 76.1 3*VGG-16 & MI - FGSM & 41 & 42.6 & 62 & 38.2 & 15.9 & DIM & 39 & 36.5 & 57 & 29.9 & 16.1 & DR ( Ours ) & 23 & 32.9 & 35 & 20.9 & 4.1 3*Resnet-152 & MI - FGSM & 37 & 41.0 & 61 & 40.4 & 17.4 & DIM & 49 & 46.7 & 60 & 34.2 & 15.1 & DR ( Ours ) & 25 & 33.3 & 31 & 34.6 & 9.5 tabulartablenotes [ 1 ] The baseline performance of GCV models can not be measured due to the mismatch between original labels and labels used by Google .
ART,adaptive radix tree,TR-15693,"At the end of the query process of Q3 , those data in P3 and new P5 are sorted and inserted into the ART index , and the ranges in the range lookup table are replaced by a new key - value pair , an interval [ 80 , 350 ] and a merged column slice across P2 , P3 , P4 , and P5 , because both [ 80,110 ] and [ 220 , 300 ] can be covered by [ 80 , 350].In this way , the complete ART index is gradually constructed by continuously arrived range queries ."
ADN,activity driven networks,TR-15777,"tikzpicture customlegend[legend columns=4,legend style = at=(0.12,1.02),draw = none , column sep=2ex , line width=2 pt , legend entries = DDT , Hom GDT , Hom ADN , Het GDT , Het ADN , , , days ] solid , line legend , color = blue solid , color = red solid , color = cyan color = yellow color = black only marks , mark = triangle only marks , mark = o only marks , mark= x customlegend tikzpicturedispm_1a.pdf dispm_1b.pdf1emdispm_1c.pdf dispm_1d.pdfSensitivity analysis of the contact networks with disease parameters infectiousness and infectious period : A ) homogeneous GDT network , B ) homogeneous ADN network , C ) heterogeneous GDT network and D ) heterogeneous ADN networkfig : netdisfigureNow , the model 's sensitivity to the various diffusion parameters is studied ."
RV,random vaccination,TR-15786,"List of SymbolsList of abbreviationslongtablecc Acronym & Descriptions ADN & Activity driven network modelling AV & Acquaintance vaccination APV & Absolute percentage variation CIP & co - location interaction parameters CN & Common neighbours DST network & Dense SPST network DDT network & Dense SPDT network DDT1 & Vaccinating neighbours in DDT network with direct linksDDT2 & Vaccinating neighbours in DDT network with any linksGDT & Generated SPDT network with 364 K nodes GST & Generated SPST network with 364 K nodes IMV & Individual movement based vaccination strategy IMVE & Individual movement based vaccination strategy with exact information IMVT & Individual movement based vaccination strategy with temporal information LST & SPDT network with the same number of links that of DDT network LST & SPST network with the same number of links that of DST network MLE & Maximum likelihood estimator OSN & Online social network PFU & Plaque - forming unit RV & Random vaccination DV & Degree vaccination RSE & Root squared error SPST & Same place same time transmission SPDT & Same place different time transmission SIR & Susceptible - infected - recovered SDT network & Sparse SPDT network of links having direct and indirect components SST network & Sparse SPST network of links having indirect component onlySPDT graph & graph based on SPDT diffusion longtableList of symbolslongtablecc Symbol & Descriptions A & Set of active copies of nodes in SPDT graph b & active particle decay rates from an area of interaction & activity potential of node & active periods of a node C & Particle concentration in interaction area C , C , C & scenario 1 , 2 and 3 d & Activation degree - number of SPDT links created during an activation E & Intake dose or exposure of infectious particles & Average volume fraction of room air introduced by exhaled breath f & distribution function F & Disease spreading force in the network at the current day of simulation & Average disease spreading force in the network & Graph & Dynamic graph g & Particle generation rate by an infected individual h & Activation frequency I & Number of infected individuals in the system & Number of infected individuals at a simulation day & Number of infected individuals in the system at the current time that disease prevalence & Number of infected individuals up to a simulation day L & links set N & Total number of individuals , nodes , users p & Pulmonary rate of susceptible individual & Infection probability for an intake dose & Probability of creating a link during an activation & Probability of breaking a created link Q & air exchange rate from an area q & Transition probability for changing inactive to active state R & disease reproduction ability of an infected individual r & Particle removal rate from interaction area r & Median of particles removal rates S & Number of susceptible individual T & Simulation period or disease observation period & Activation period or period host user or node stays at the interacted location & Link creation delay or delay neighbour user or node arrives at the interacted location & Stay duration of user or node stays at the interacted location V & Air volume of interaction area , , & waiting periods of a node Y & Labelling sets in graph X & updates z & Number of time step Z & set of nodes in the SPDT graph & power law exponent & Infection rate at the current day of simulation in the network for an infected individual & Indirect transmission period & central tendency & Scaling parameter of activation degree distribution & neighbour proportion & average volume fraction of room air that is exhaled by an susceptible individual & links presence function & state probability & nodes presence function & switching probability form active to inactive states & Infectiousness of infection particles & Fraction of dose or exposure reaches to the target infection site & Duration that virus is generated or infectious period of infected individual & inter - event time for node in activity driven networks & lower limit of active degree distribution & activation potential in ADN networks & particle accumulation rate & Transition probability for changing active to inactive state longtable"
FEC,forward error correction,TR-15896,The parameters are defined as follows : FEC - based : accounts for mechanisms that employ FEC ; ARQ - based : mark mechanisms that use ARQ ; QoE - sensitive data : this parameter demonstrates mechanisms that identity and/or considerate the video content to define the EC policy ; Video - aware : check mark is given to mechanisms that use any video characteristics to define the amount of redundancy and/or retransmission ; High - quality video : it is marked if the mechanisms are using videos equal or higher than 720p ( HD ready ) ; Network status : this parameter defines if the mechanisms use the information about the network healthy to define the redundant data ; UEP - enabled : means that different amounts of redundancy are being added to distinct portions of the video .
DE,differential evolution,TR-16089,"In this work , the main contributions are : ( i ) the objective function in has been reformulated using additional penalty term for optimal performance , ( ii ) two other evolutionary techniques ( DE and GSA ) have been used in the second phase to efficiently deploy the RNs , ( iii ) the effectiveness of the proposed algorithms is compared and contrasted with on the basis of network lifetime enhancement and speed of convergence , and lastly ( iv ) comprehensive experiments have been carried out to show the efficacy ( faster convergence and better optimal solution ) of using DE as opposed to ABC presented in to deploy backbone devices in WSNs ."
FPR,false positive rate,TR-16113,"tabularlccccccc1c2*Method & 6cShift Deviation Percentile ( mm ) & FPR 2 - 71c & 10th & 25th & 50th & 75th & 90th & Mean & ( ) Simplex - PI & 0.00 & 0.21 & 0.33 & 0.53 & 25.00 & 5.22 & 16.22 Simplex - MI & 0.38 & 0.64 & 1.12 & 2.40 & 21.41 & 5.28 & 27.78 Simplex - CC & 0.34 & 0.49 & 0.68 & 0.91 & 1.08 & 0.70 & 0.00 Powell - PI & 0.19 & 0.40 & 18.21 & 23.83 & 31.00 & 139.10 & 66.89 Powell - MI & 0.34 & 0.66 & 1.41 & 20.18 & 27.53 & 10.66 & 44.56 Powell - CC & 0.22 & 0.38 & 0.69 & 18.45 & 30.89 & 8.78 & 31.56 CNN - 1 iter & 0.46 & 0.77 & 1.14 & 3.08 & 20.90 & 6.37 & 37.22 CNN - 2 iters & 0.36 & 0.59 & 0.89 & 2.37 & 16.22 & 4.93 & 26.81 CNN - 3 iters & 0.23 & 0.38 & 0.55 & 1.48 & 10.29 & 4.15 & 12.79 tabulartab_resultstableAmong the optimization methods , it was observed that the best results were obtained with the Downhill Simplex method ."
AV,acquaintance vaccination,TR-16175,"tikzpicture customlegend[legend columns=4,legend style = at=(0.12,1.02),draw = none , column sep=3ex , line width=2pt , font= , legend entries = RV , AV , IMV , DV , direct , indirect ] solid , color = blue color = red color = green color = magenta color = black dashdotted , color = black customlegend tikzpicture pvan_a.pdfpvan_b.pdfpvan_c.pdfpvan_d.pdfAverage outbreak sizes at various vaccination rates ( percentage of total nodes ) of different strategies : ( A , B ) nodes are vaccinated with contacts created for direct interactions and ( C , D ) comparison of outbreak sizes for vaccinating nodes with contacts based on the direct interactions ( solid lines ) and contact based on any direct or indirect interactions ( dashed lines)-1.5emfig : avacfigurefigure[h ! ]"
ADN,activity driven networks,TR-16288,var ( ) 3*DDT & 60 & 1731 & 11221 & 0 & 0 2 - 6 & 40 & 1332 & 8671 & 0 & 0 2 - 6 & 20 & 966 & 6460 & 0 & 0 3*Het GDT & 60 & 2094 & 11929 & 15 & 10 2 - 6 & 40 & 1531 & 9584 & 18 & 12 2 - 6 & 20 & 987 & 6853 & 22 & 13 3*Het ADN & 60 & 1007 & 7809 & 29 & 37 2 - 6 & 40 & 673 & 5666 & 30 & 38 2 - 6 & 20 & 335 & 3170 & 45 & 44 3*Hom GDT & 60 & 698 & 3600 & 61 & 50 2 - 6 & 40 & 626 & 2110 & 69 & 53 2 - 6 & 20 & 586 & 1592 & 70 & 51 3*Hom ADN & 60 & 600 & 1892 & 73 & 62 2 - 6 & 40 & 581 & 1645 & 73 & 59 2 - 6 & 20 & 562 & 1386 & 72 & 55 tabulartablefigure[h ! ]
AV,acquaintance vaccination,TR-16298,"List of SymbolsList of abbreviationslongtablecc Acronym & Descriptions ADN & Activity driven network modelling AV & Acquaintance vaccination APV & Absolute percentage variation CIP & co - location interaction parameters CN & Common neighbours DST network & Dense SPST network DDT network & Dense SPDT network DDT1 & Vaccinating neighbours in DDT network with direct linksDDT2 & Vaccinating neighbours in DDT network with any linksGDT & Generated SPDT network with 364 K nodes GST & Generated SPST network with 364 K nodes IMV & Individual movement based vaccination strategy IMVE & Individual movement based vaccination strategy with exact information IMVT & Individual movement based vaccination strategy with temporal information LST & SPDT network with the same number of links that of DDT network LST & SPST network with the same number of links that of DST network MLE & Maximum likelihood estimator OSN & Online social network PFU & Plaque - forming unit RV & Random vaccination DV & Degree vaccination RSE & Root squared error SPST & Same place same time transmission SPDT & Same place different time transmission SIR & Susceptible - infected - recovered SDT network & Sparse SPDT network of links having direct and indirect components SST network & Sparse SPST network of links having indirect component onlySPDT graph & graph based on SPDT diffusion longtableList of symbolslongtablecc Symbol & Descriptions A & Set of active copies of nodes in SPDT graph b & active particle decay rates from an area of interaction & activity potential of node & active periods of a node C & Particle concentration in interaction area C , C , C & scenario 1 , 2 and 3 d & Activation degree - number of SPDT links created during an activation E & Intake dose or exposure of infectious particles & Average volume fraction of room air introduced by exhaled breath f & distribution function F & Disease spreading force in the network at the current day of simulation & Average disease spreading force in the network & Graph & Dynamic graph g & Particle generation rate by an infected individual h & Activation frequency I & Number of infected individuals in the system & Number of infected individuals at a simulation day & Number of infected individuals in the system at the current time that disease prevalence & Number of infected individuals up to a simulation day L & links set N & Total number of individuals , nodes , users p & Pulmonary rate of susceptible individual & Infection probability for an intake dose & Probability of creating a link during an activation & Probability of breaking a created link Q & air exchange rate from an area q & Transition probability for changing inactive to active state R & disease reproduction ability of an infected individual r & Particle removal rate from interaction area r & Median of particles removal rates S & Number of susceptible individual T & Simulation period or disease observation period & Activation period or period host user or node stays at the interacted location & Link creation delay or delay neighbour user or node arrives at the interacted location & Stay duration of user or node stays at the interacted location V & Air volume of interaction area , , & waiting periods of a node Y & Labelling sets in graph X & updates z & Number of time step Z & set of nodes in the SPDT graph & power law exponent & Infection rate at the current day of simulation in the network for an infected individual & Indirect transmission period & central tendency & Scaling parameter of activation degree distribution & neighbour proportion & average volume fraction of room air that is exhaled by an susceptible individual & links presence function & state probability & nodes presence function & switching probability form active to inactive states & Infectiousness of infection particles & Fraction of dose or exposure reaches to the target infection site & Duration that virus is generated or infectious period of infected individual & inter - event time for node in activity driven networks & lower limit of active degree distribution & activation potential in ADN networks & particle accumulation rate & Transition probability for changing active to inactive state longtable"
PG,property graph,TR-16392,"However , there are also some differences between them : An RDF graph contains nodes of type resource ( whose label is an IRI ) and nodes of type Literal ( whose label is a value ) , whereas a PG allows a single type of node ; Each node or edge in an RDF graph contains just a single value ( i.e. a label ) , whereas each node or edge in a PG could contain multiple labels and properties respectively;An RDF graph supports multi - value properties , whereas a PG usually just support mono - value properties;An RDF graph allows to have edges between edges , a feature which is n't supported in a PG ( by definition);A node in an RDF graph could be associated with zero or more classes or resources , while a node in a PG usually has a single node type ."
MSA,modern standard arabic,TR-16407,table1.380mm16mmtabular@lllllllll@ & EGY & GLF & LAV & MSA & NOR & Total Truth & PRC EGY & 221 & 15 & 57 & 13 & 9 & 315 & 50.3 GLF & 45 & 121 & 82 & 12 & 5 & 265 & 55.8 LAV & 74 & 43 & 199 & 18 & 14 & 348 & 46.9 MSA & 19 & 17 & 20 & 218 & 5 & 279 & 77 NOR & 80 & 21 & 66 & 22 & 166 & 355 & 83.4 class & 439 & 217 & 424 & 283 & 199 RCL & 70.2 & 45.7 & 57.2 & 78.1 & 46.8 tabularConfusion Matrix for DID.tab : cmtabletable[H]1.350mm10.5mmtabular@lllllll@ Expected Dialect & EGY & GLF & LAV & NOR & MSA EGY & 65 & & & & 32 GLF & & 41 & 4 & & 53 LAV & 1 & 1 & 53 & & 39 NOR & 1 & & & 69 & 28 tabularExpected dialect of each speech segment from particular dialectal speakers.tab:expansion2table
LSC,leicester scientific corpus,TR-16500,"The most frequent 20 words in the LScD. L1.6 cm R4 cm L1.6 cm R4 cm Word & Number of documents containing the word & Word & Number of documents containing the word use & 902,033 & also & 400,642 result & 812,154 & present & 389,735 studi & 723,827 & increas & 383,676 show & 498,705 & two & 375,586 method & 491,586 & model & 372,911 effect & 476,757 & signific & 370,435 base & 446,436 & compar & 355,381 differ & 445,739 & paper & 346,514 can & 441,512 & time & 344,817 high & 402,737 & perform & 341,547 Processing the LSC and Building the LScD The main challenge of using text data is that it is mess and not concretely structured ."
TVD,total variation diminishing,TR-16585,"After all four variants of SIMPLE - TS algorithms are tested , which are noted as follow : explicit TVD second - order scheme - approximate convective terms with explicit ( Forward Euler ) and TVD second - order schemeexplicit upwind first - order scheme - approximate convective terms with explicit ( Forward Euler ) and upwind first - order schemeimplicit TVD second - order scheme - approximate convective terms with explicit ( Backward Euler ) and TVD second - order schemeimplicit upwind first - order scheme - approximate convective terms with explicit ( Backward Euler ) and upwind first - order schemeExplicit and implicit schemes possess well - known advantages and disadvantages ."
TDS,taint dependency sequences,TR-16617,"language=[ANSI]C++lstlisting[basicstyle= , numberstyle= , numbers = left , frame = single , name = Pseudo - code for Genetic Algorithm , caption= , captionpos = b , label = fig : GA]popsize : = desired population sizeP : = [ ] for popsize times do P : = P U < new random individuals > Best : = [ ] repeat for each individual Pi in P do AssessFitness(Pi ) if Best = = [ ] or Fitness(Pi ) > Fitness(Best ) then Best : = Pi Q : = [ ] for popsize/2 times do Parent Pa : = SelectwithReplacement(P ) Parent Pb : = SelectwithReplacement(P ) Children Ca , Cb : = Crossover(Copy(Pa ) , Copy(Pb ) ) Q : = P U < Mutate(Ca ) , Mutate(Cb ) > P : = Quntil Best is the ideal solution OR we have run out of timereturn BestlstlistingInitial Populationsec : initpopAs mentioned before , in order to execute a TDS a set of constraints ( corresponding to the IF and WHILE conditions associated to this TDS ) has to be satisfied ."
MPI,multiple parallel instances,TR-16966,table[!htb ] tabularrrrrrr & & nodes & time ( s ) & & 24 & 24 & 1 & 89.9 & 1.0 & 1.0 48 & 48 & 2 & 46.8 & 1.9 & 0.96 72 & 72 & 3 & 33.7 & 2.7 & 0.89 96 & 96 & 4 & 25.1 & 3.6 & 0.89 144 & 144 & 6 & 43.7 & 2.1 & 0.34 192 & 192 & 8 & 13.5 & 6.7 & 0.83 tabular [ Time necessary for writing the trajectory segments ] The wall - clock time spent for writing trajectory segments using processes using MPI on SDSC Comet .
MPI,message passing interface,TR-17120,figure[!htb ] subfigure.4 figures / main - RMSD - t_total - Bridges.pdf Scaling total fig : MPIscaling - Bridges subfigure subfigure.4 figures / main - RMSD - speed_up - Bridges.pdf Speed - up fig : MPIspeedup - Bridges subfigure subfigure.4 figures / main - RMSD - time_comp_IO_comparison - Bridges.pdf format = hang Scaling for different components fig : ScalingComputeIO - Bridges subfigure subfigure .5 figures / main - RMSD - BarPlot - rank - comparison_72_4-Bridges.pdf format = hang Time comparison on different parts of the calculations per MPI rank ( example ) fig : MPIranks - Bridges subfigure PSC Bridges : Performance of the RMSD task .
RV,random vaccination,TR-17167,"List of SymbolsList of abbreviationslongtablecc Acronym & Descriptions ADN & Activity driven network modelling AV & Acquaintance vaccination APV & Absolute percentage variation CIP & co - location interaction parameters CN & Common neighbours DST network & Dense SPST network DDT network & Dense SPDT network DDT1 & Vaccinating neighbours in DDT network with direct linksDDT2 & Vaccinating neighbours in DDT network with any linksGDT & Generated SPDT network with 364 K nodes GST & Generated SPST network with 364 K nodes IMV & Individual movement based vaccination strategy IMVE & Individual movement based vaccination strategy with exact information IMVT & Individual movement based vaccination strategy with temporal information LST & SPDT network with the same number of links that of DDT network LST & SPST network with the same number of links that of DST network MLE & Maximum likelihood estimator OSN & Online social network PFU & Plaque - forming unit RV & Random vaccination DV & Degree vaccination RSE & Root squared error SPST & Same place same time transmission SPDT & Same place different time transmission SIR & Susceptible - infected - recovered SDT network & Sparse SPDT network of links having direct and indirect components SST network & Sparse SPST network of links having indirect component onlySPDT graph & graph based on SPDT diffusion longtableList of symbolslongtablecc Symbol & Descriptions A & Set of active copies of nodes in SPDT graph b & active particle decay rates from an area of interaction & activity potential of node & active periods of a node C & Particle concentration in interaction area C , C , C & scenario 1 , 2 and 3 d & Activation degree - number of SPDT links created during an activation E & Intake dose or exposure of infectious particles & Average volume fraction of room air introduced by exhaled breath f & distribution function F & Disease spreading force in the network at the current day of simulation & Average disease spreading force in the network & Graph & Dynamic graph g & Particle generation rate by an infected individual h & Activation frequency I & Number of infected individuals in the system & Number of infected individuals at a simulation day & Number of infected individuals in the system at the current time that disease prevalence & Number of infected individuals up to a simulation day L & links set N & Total number of individuals , nodes , users p & Pulmonary rate of susceptible individual & Infection probability for an intake dose & Probability of creating a link during an activation & Probability of breaking a created link Q & air exchange rate from an area q & Transition probability for changing inactive to active state R & disease reproduction ability of an infected individual r & Particle removal rate from interaction area r & Median of particles removal rates S & Number of susceptible individual T & Simulation period or disease observation period & Activation period or period host user or node stays at the interacted location & Link creation delay or delay neighbour user or node arrives at the interacted location & Stay duration of user or node stays at the interacted location V & Air volume of interaction area , , & waiting periods of a node Y & Labelling sets in graph X & updates z & Number of time step Z & set of nodes in the SPDT graph & power law exponent & Infection rate at the current day of simulation in the network for an infected individual & Indirect transmission period & central tendency & Scaling parameter of activation degree distribution & neighbour proportion & average volume fraction of room air that is exhaled by an susceptible individual & links presence function & state probability & nodes presence function & switching probability form active to inactive states & Infectiousness of infection particles & Fraction of dose or exposure reaches to the target infection site & Duration that virus is generated or infectious period of infected individual & inter - event time for node in activity driven networks & lower limit of active degree distribution & activation potential in ADN networks & particle accumulation rate & Transition probability for changing active to inactive state longtable"
CG,conjugate gradient,TR-17299,"algorithmalg : sparseKatzalgorithmic[1]Sparse - KatzGiven Compute with Sort in descending and take nodes Compute with Store s as matrix Take row - wise Pearson Correlation of matrix and form list Change list according the order of listalgorithmicalgorithmfigure*[t ] center [ ] fig : Iter - a IterDBLP.eps [ ] fig : Iter - b IterArxiv.eps [ ] fig : Iter - c IterEmail.eps [ ] fig : Iter - d IterGowalla.eps [ ] fig : Iter - e IterFlickr.eps [ ] fig : Iter - f IterHollywood.eps center The number of iterations required for and CG in computing Katz proximity scores In these experiments , the reported numbers are the averages across randomly chosen query nodes ."
HDT,header dictionary triple,TR-17612,"tab : misccenter tabularlcccccccccccccc Query & Res & 2cRedland & Virtuoso&Stardog & HDT & 4cTripleID & 4cSpeedup 3 - 48 - 15 & & load & query & & & & load & data & join & query & Redland & Virtuoso & Stardog & HDT Q1 & 20,081 & 43.29 & 7.54 & 7.82 & 2.38 & 0.54 & 1.36 & 0.29 & - & 0.36 & 20.94 & 21.72 & 6.61&1.50 Q2 & 784,648 & 43.38 & 40.1 & 670.69&509.90 & 2.56 & 1.25 & 0.29 & - & 0.83 & 48.31 & 808.06&614.33&3.08Q3 & 870,890 & 43.09 & 57.04 & 785.38 & 570.19 & 3.37 & 1.15 & 0.29 & - & 0.86 & 66.33 & 897.67 & 663.01&3.91Q4 & 891,102 & 43.11 & 72.39 & 785.95 & 596.51 & 3.51 & 1.24 & 0.29 & - & 0.83 & 82.36 & 897.67 & 718.68&4.22Q5 & 24 & 43.21 & 5.54 & 0.04 & 0.14 & - & 1.15 & 0.27 & - & 0.32 & 17.31 & - & -&-Q6 & 18 & 43.05 & 5.55 & 0.01 & 0.27 & - & 1.15 & 0.29 & - & 0.32 & 17.34 & - & -&- Q7 & 22 & 43.23 & 11.45 & 3.00 & 0.15 & 0.40 & 1.15 & 0.3 & - & 0.36 & 31.81 & 8.34 & -&1.11 Q8 & 20,370 & 43.23 & 19.49 & 4.87 & 1.49 & 0.80 & 1.11 & 0.28 & - & 0.4 & 48.73 & 12.17 & 3.73&2.00 Q9 & 1 & 43.01 & 47.67 & 2.14 & 0.14 & 0.90 & 0.9 & 0.27 & 0.03 & 0.45 & 105.93 & 4.76 & - & 2.00Q10 & 0 & 43.32 & 48.97 & 0.01 & 0.18 & 1.27 & 1.15 & 0.28 & 0.02 & 0.45 & 108.83 & -&-&2.82Q11 & 98 & 43 & 5.61 & 14.70 & 0.17 & 0.36 & 1.11 & 0.28 & 0 * & 0.34 & 16.50 & 43.24 & -&1.05 Q12 & 1,529 & 43.08 & 6.17 & 16.49 & 0.36 & 0.46 & 1.11 & 0.27 & 0 * & 0.36 & 17.14 & 45.81 & - & 1.27Q13 & 30,427 & 43.16 & 8.23 & 22.97 & 2.62 & 0.53 & 1.14 & 0.27 & 0.02 & 0.38 & 21.66 & 60.45 & 6.89&1.39 Q14 & 144,845 & 43.09 & 15.36 & 58.53 & 20.62 & 0.98 & 1.11 & 0.28 & 0.3 & 0.68 & 22.59 & 86.08&30.32&1.44Q15 & 5,595 & 43.97 & 6.82 & 1.28 & 0.65 & 0.43 & 1.29 & 0.27 & 0.03 & 0.42 & 16.24 & 3.05 & 1.54&1.02 Q16 & 86,824 & N / A & N / A & 1,528.03 & 614.34 & 9.86 & 1.11 & 0.3 & 0.95 & 1.35 & - & 1,131.88&455.07&7.30 tabularcentertable*Table tab : misc2 presents timing results for the larger BTC data set ( 012347 ) containing 7 million triples ."
OCM,oz computation model,TR-17655,"In addition , the Ozy orchestration container also includes the following contributions : A new notion of partial activation as a dual to the existing notion of partial termination A technology neutral orchestration architecture inspired by the elements of a network - based architecture An open source orchestration container that implements the orchestration architecture ( 2 ) , implements an Oz language interpreter based on the OCM , and exploits the notions of partial activation ( 1 ) and partial termination to support a persistent execution stateThis paper describes the Ozy framework with special attention paid to the limitations of current approaches ."
GCN,graph convolution networks,TR-18033,a ) FCN-32 ; ( b ) FCN-16s ; ( c ) ResNet - DUC ; ( d ) E - Net ; ( e ) SegNet ; ( f ) U - Net ; ( g ) FCN-8s ; ( h ) CWGAN - GP ; ( i ) FC - DenseNet ; ( j ) DSFE - CRF ; ( k ) DSFE - GCN ; ( l ) DSFE - GraphSAGE ; ( m ) DSFE - GGNN ; ( n ) DSFE - GGCN ; ( o ) Ground truth ; ( p ) Optical image .
NE,nash equilibrium,TR-18086,"The AlgorithmsWe use two multi - population ( each player has its own population of chromosomes representing its alternative choices at any round ) co - evolutionary genetic algorithms , Vriend 's individual learning algorithm [ 15 ] and co - evolutionary programming , a similar algorithm that has been used for the game of prisoner 's dilemma [ 10 ] and , unsuccessfully , for Cournot Duopoly [ 13]. Since those two algorithms do n't , as it will be seen , lead to convergence to the NE in the models under consideration , we introduce two different versions of the algorithms , as well , which are characterized by the use of opponent choices , when the new generation of each player 's chromosome population is created , and therefore can be regarded as "" socialized "" versions of the two algorithms ."
CAA,civil aviation authority,TR-18106,"tabular@l@ l@ p10.9cm@class & properties & languages 1551 & c t w i & IBM 's EasyEnglish & c w s g & Special English & c w a & E - Prime & c w g & Plain Language 2132 & c s d g & CAA Phraseology , FAA Phraseology , ICAO Phraseology , PoliceSpeak , SEASPEAK 2133 & c w d i & Airbus Warning Language 2541 & f w a & AIDA 2551 & c t w d a i & ALCOGRAM , COGRAM & c t w d a & CLCM & c t w d i & ASD - STE , Avaya CE , Bull GE , CTE , CASL , CE at Douglas , DCE , General Motors GE , PACE , Sun Proof & c t w d & Wycliffe Associates ' EasyEnglish & c t w i & iCE , SMART Controlled English & c w d i & AECMA - SE , CFE , CASE , CE at Clark , CE at IBM , CE at Rockwell , EE , HELP , ILSAM , KISL , NCR FE & c w d g & Massachusetts Legislative Drafting Language & c w i & Boeing Technical English , NSE , SMART Plain English & c w & Basic English & t w d i & MCE , Oce Controlled English & t w a & KCE & t w i & CLOUT 3142 & c f w d i & SLANG & f s d i & Voice Actions 3243 & f w d a & RNLS 3333 & f w a & ClearTalk & f w i & ITA CE 3342 & f w i & CPL 3442 & c f w i & RuleSpeak , SBVR - SE 4143 & f w d a & Drafter Language , MILE Query Language 4144 & f w a & Quelo Controlled English 4153 & t f d a & PILLS Language 4243 & f w d a & Atomate Language & f w a i & Gellish English & f w a & GINO 's Guided English & f w i & CELT 4343 & f w d a & PROSPER CE & f w a & ACE 4353 & f w d a & ICONOCLAST Language 5143 & f w d a & CLEF Query Language & f w a & Ginseng 's Guided English 5144 & f w d a & Coral 's Controlled English & f w a & PathOnt CNL 5145 & f w a & Sowa 's syllogisms 5234 & f w d a i & TBNLS & f w a & OWLPath 's Guided English , SQUALL 5243 & f w a & CPE , CLIP , OWL ACE , SOS 5244 & f w d a & BioQuery - CNL , PERMIS CNL , ucsCNL & f w a & CLOnE , DL - English , E2V , Lite Natural Language , OSE & f w g & Rabbit 5333 & f w d a & CLM , ForTheL , Naproche CNL & f w a & CLCE , PNL 5343 & f w d a & Gherkin & f w a g & RECON & f w a & First Order English , PENG , PENG - D , PENG Light & f w i & iLastic Controlled English 5433 & f w a & FEtabular"
FA,fractional anisotropy,TR-18112,tabularp0.8cmp5.5cmp6 cm 3cAmyloid Load ( PiB Positivity ) Set 1 & PiB Angular L / R & PiB Cingulum Ant L / R & PiB Cingulum Post L / R & PiB Frontal Med Orb L / R & PiB Precuneus L / R & PiB Temporal Sup L / R & PiB Temporal Mid L / R & PiB SupraMarginal L Set 2 & FA Cerebral peduncle R & FA Cerebral peduncle L & MD Corticospinal tract R & MD Corticospinal tract L & Trail - Making Test Part A Score & MD Cerebral peduncle R & PET Cingulum Post R & tabular Group difference across Amyloid Load ( PiB Positivity)tabletable [ ! ]
MPI,message passing interface,TR-18173,table[!htb ] tabularrrrrrr & & nodes & time ( s ) & & 24 & 24 & 1 & 89.9 & 1.0 & 1.0 48 & 48 & 2 & 46.8 & 1.9 & 0.96 72 & 72 & 3 & 33.7 & 2.7 & 0.89 96 & 96 & 4 & 25.1 & 3.6 & 0.89 144 & 144 & 6 & 43.7 & 2.1 & 0.34 192 & 192 & 8 & 13.5 & 6.7 & 0.83 tabular [ Time necessary for writing the trajectory segments ] The wall - clock time spent for writing trajectory segments using processes using MPI on SDSC Comet .
MSE,mean squared error,TR-18231,"NPSS is meant to complement the popular MSE by addressing some of its drawbacks including : a ) a frequency - shift in a predicted sequence , e.g. , walking at a faster or slower rate , compared to ground - truth will be heavily penalized by MSE despite being qualitatively similar , and b ) a phase - shift in predicted sequence , e.g. , if a few frames of motion are missed / skipped by the model , the resulting predicted motion sequence will be phase - shifted but MSE will heavily penalize it despite qualitative similarity with ground - truth ."
RP,reciprocal pagerank,TR-18321,"tableSpearman Correlations to the KORE gold standard with the text - based approach for different Wikipedia dumpstab : bag - of - word - results tabularp5cmlVersion & Correlation Wikipedia at the time when YAGO2 was created & 0.503212 Wikipedia at the time of DBpedia 2009 dump & 0.491440 Wikipedia at the time of DBpedia 2010 dump & 0.502987 tabulartabletable*tableSpearman Correlations to the KORE gold standard comparing text - based approach with graph - based approaches between models without redirects and models with redirects of data from DBpedia 2009 and DBpedia 2010tab : corr-2009 - 2010-redirectsthreeparttable tabularp40mmllll2 * & 2cDbpedia 2009 & 2cDbpedia 2010 2 - 5 & 1cWithout Redirect & 1cWith Redirects & 1cWithout Redirect & 1cWith Redirects TF - IDF & 0.491440 & - & 0.502987 & - Jaccard ( I ) & 0.568509 & 0.575758 & 0.568026 & 0.588351 Jaccard ( O ) & 0.511564 & 0.504138 & 0.559898 & 0.564088 Jaccard ( I+O ) & 0.578706 & 0.580944 & 0.585535 & 0.586593 Extended Jaccard RP ( I ) & 0.585212 & 0.591538 & 0.569857 & 0.616843 Extended Jaccard RP ( O ) & 0.578478 & 0.566556 & 0.576616 & 0.601197 Extended Jaccard RP ( I+O ) & 0.604112 & 0.613450 & 0.599284 & 0.637930 Extended Jaccard RD ( I ) & 0.623768 & 0.634961 & 0.632760 & 0.672512 Extended Jaccard RD ( O ) & 0.588592 & 0.567652 & 0.601755 & 0.609715 Extended Jaccard RD ( I+O ) & 0.639055 & 0.651450 & 0.658647 & 0.696506 tabulartablenotes[para , flushleft ] , , , and mean the Wikipedia article link network with redirects using the Extended Jaccard with Reciprocal Degree Centrality considering both in - links and out - links is significantly better than this result with p - value 0.1 , p - value 0.05 , p - value 0.01 and p - value 0.001 respectively ."
CM,corporate messaging,TR-18427,The combined datasets were : AT and ATIS IntAT and ClassicLitAT and CM and DFG and DT and NYR 10 and SDC and TEATIS Int and ClassicLitATIS Int and CMClassicLit and CMClassicLit and DFGClassicLit and LMRCClassicLit and RSClassicLit and SSTClassicLit and TECM and ATCM and DFGDFG and ATDFG and ATIS IntDT and ATDT and ATIS IntLMRC and ATLMRC and ATIS IntLMRC and RS and SSTLMRC and RS and YTSNYR 10 and ATNYR 10 and ATIS IntNYR 115 and ATNYR 115 and ATIS IntPSC and ATPSC and ATIS IntRS and ATRS and ATIS IntRS and LMRCRS and SSTSDC and ATSDC and ATIS IntSNIPS and ATSNIPS and ATIS IntSNIPS and ATIS Int and ClassicLitSNIPS and ATIS Int and PSCSNIPS and ATIS Int and SSTSST 2 and ATSST 2 and ATIS IntSST and ATSST and ATIS IntSST and ClassicLit and LMRCSST and LMRCSST and SST 2TE and ATTE and ATIS IntTE and NYR 10YTS and ATYTS and ATIS IntYTS and TE and PSC and RS * [ !
RL,reinforcement learning,TR-18644,"Although represents one number , algorithm 's behaviour may turn out to be sensitive to its choosing , and must be designed by engineer as some scheduled motion from something near 0 to 1 , and its well - turned selection may require inaccessible knowledge about how many steps it will take for algorithm to < < warm up>>.Multi - step DQNOne more widespread modification of Q - learning in RL community is substituting one - step approximation present in Bellman optimality equation ( ) with -step:(-step Bellman optimality equation)Indeed , definition of consists of average return and can be viewed as making steps from state after selecting action , while vanilla Bellman optimality equation represents as reward from one next step in the environment and estimation of the rest of trajectory reward recursively ."
ILP,integer linear programming,TR-18738,"The ILP model of the dynamic QoS problem can be formulated as follows : maximize & _ i=1^n_j=1^m_l=1^q E_l x_ijl & & subject to : & x_ijl = 0 & & l 1, ... ,q , i , j i , ju_i cov(s_j ) & _ i=1^n _ l=1^q W_l^kx_ijl c_j^k & & j 1, ... ,m , k 1, ... ,d & _ j=1^m _ l=1^q x_ijl 1 & & i 1, ... ,n & x_ijl 0,1 & & i 1, ... ,n , j 1, ... ,m , l 1, ... ,q eq : ilp_domain_constraint is the binary indicator variable such that , The objective ( ) maximizes the total QoE of all allocated users ."
AP,average precision,TR-18808,tab : state tabularlccccc Model & AP & AP1 & AP0.5 & AP0.25 & AP0.125 MLP baseline & 3.60.5 & 1.50.4 & 0.80.3 & 0.20.1 & 0.00.0 RNN baseline & 4.01.9 & 1.81.2 & 0.90.5 & 0.20.1 & 0.00.0 Ours ( 10 iters ) & 72.82.3 & 59.22.8 & 39.04.4 & 12.42.5 & 1.30.4 Ours ( 20 iters ) & 84.04.5 & 80.04.9 & 57.012.1 & 16.69.0 & 1.60.9 Ours ( 30 iters ) & 85.24.8 & 81.15.2 & 47.417.6 & 10.89.0 & 0.60.7 tabulartableResultsWe show our results in tab : state and give sample outputs in app : outputs .
DC,distributed control,TR-18895,"In this case , input to the global closed - loop voltage dynamics is given by : The steady - state DC microgrid bus voltages are obtained by applying the final value theorem to ( ) : llv^ss= _ s0 sV = _ s0 [ s^2(H^v_cl)^-1+s^2FrY + s^2FrG^vG^avg -s^2FrG^e(I_N - G^avg)MY]^-1s^2v^mg(I_N+FrG^v)1 The steady - state bus voltages can be reached to based on the following limits : ll_s0 s^2G^v = G^vii , where G^vii = diagk_i^vii _ s0 sG^e = G^ei , where G^ei = diagk_i^ei _ s0 sM = M_0 , where M_0=diag - v^mge_i^max _ s0 G^avg = Q , _ s0 Y = Y_0 , _ s0 F = I_N , and _ s0 ( H^v_cl)^-1=I_N thereforellv^ss=[r(G^viiQ - G^ei(I_N - Q)M_0Y_0)]^-1v^mg(rG^vii)1 which yieldsll[(G^ei)^-1G^viiQ-(I_N - Q)M_0Y_0]v^ss = v^mg(G^ei)^-1G^vii)1 Furthermore , as shown in ( ) , without the double - integral gain of the voltage controller , the steady - state response would be dominated by the energy balancing control signal ; to verify that the average steady - state voltage is equal to the reference voltage of the microgrid , each side of ( ) is multiplied by the averaging matrix ."
SVM,support vector machine,TR-18910,"fig : feature_selection_contribution2figurefigure[!h]tikzpicture [ thick , scale=1 , every node/.style = scale=1]box = [ rectangle , draw , thick , align = center , minimum height=10mm];arrow = [ ->,thick ] ; [ ] ( d ) ; [ box , right=5 mm of d.east,anchor=west,fill=yellow ] ( dico ) Sparse Representation;[above=5 mm of dico.north,anchor=south ] ( dicolearn ) ; [ box , right=20 mm of dico.east,anchor=west,fill=cyan ] ( svm ) SVM Classification;[above=5 mm of svm.north,anchor=south ] ( svmlearn ) ; [ right=5 mm of svm.east,anchor=west ] ( dddd ) ; [ arrow ] ( d)-(dico);[arrow ] ( dicolearn)-(dico);[arrow ] ( dico)-(svm ) node[above , pos=0.5 ] ; [ arrow ] ( svm)-(dddd);[arrow ] ( svmlearn)-(svm);tikzpictureProcessing flow of classification over testing set ."
NF,new foundations,TR-18933,"1-May 23 , 2017May 22 , 2018 enumeratei*enumerateicalcmOOOO [ baseline=(tocancel.base ) ] [ inner sep=0pt , outer sep=0pt ] ( tocancel ) # 1 ; [ gray ] ( ) - ( ) ; @R12em@ L16.5em@ L15emboxed figure # 2(N ) fa [ 3]#3 _ # 2^#1 thebibliography[1 ] # 1 1010 [ 1 ] # 1[1]opens(#1)pow _ : : idN of def S @fdsymbol UFdSymbolA fdsymbolsUFdSymbolAmn symbolsboldUFdSymbolAbn UFdSymbolAmn < -7.1 > s * [ ] FdSymbolA- < 7.1- > s * [ ] FdSymbolA- UFdSymbolAbn < -7.1 > s * [ ] FdSymbolA- < 7.1- > s * [ ] FdSymbolA- fdsymbols""C7 fdsymbols""C8 fdsymbols""C9 fdsymbols""CA[Stratified Sets is confluent and normalising]The language of Stratified Sets is confluent and strongly normalising [ M. Gabbay]Murdoch J. GabbayHeriot - Watt University , Scotland , UKwww.gabbay.org.ukThanks to the editor and to the anonymous refereesWe study the properties of the language of Stratified Sets ( first - order logic with and a stratification condition ) as used in TST , TZT , and ( with stratifiability instead of stratification ) in Quine 's NF ."
SAM,semi - autonomous machine,TR-18966,Experiment Results with SAM Wheelchair figure[!t ] [ ] 0.30c6_exp41-eps - converted - to.pdf c6.exp41 [ ] 0.30c6_exp42-eps - converted - to.pdf c6.exp42 [ ] 0.30c6_exp43-eps - converted - to.pdf c6.exp43 [ ] 0.30c6_exp44-eps - converted - to.pdf c6.exp44 [ ] 0.30c6_exp45-eps - converted - to.pdf c6.exp45 Wheelchair performs basic navigation tasks c6.exp4 figure figure[!h ] [ ] 0.30c6_exp51-eps - converted - to.pdf c6.exp51 [ ] 0.30c6_exp52-eps - converted - to.pdf c6.exp52 [ ] 0.30c6_exp53-eps - converted - to.pdf c6.exp53 [ ] 0.30c6_exp54-eps - converted - to.pdf c6.exp54 [ ] 0.30 - 90c6_exp55-eps - converted - to.pdf c6.exp55 Wheelchair avoids random obstacles c6.exp5 figure We first show that the wheelchair with ENA is able to perform basic navigation tasks in a static environment .
LR,logistic regression,TR-18989,"In the bag of words method , we extract the features by using presence / absence of words ( binary ) term frequency of the words ( tf ) inverse document frequency of words ( idf ) product of term frequency and inverse document frequency of words ( tf - idf ) We also evaluate some of the recent state of the art methods for text classification on the above datasets naive bayes features in bag of words followed by Logistic Regression ( NB - LR ) inversion of distributed language representation ( W2V inversion ) ( We use the code available at https://github.com/TaddyLab/deepir which builds on top of gensim toolkit ) Convolutional Neural Networks for text categorization ( CNN ) Paragraph Vectors - Distributed Bag of Words Model ( PV - DBOW)Class Vector method based scoring and feature extraction ."
ILP,inductive logic programming,TR-19015,"table[ht ] Dataset Features tbl : datasets tabular l l l l l l Dataset & Constants & Predicates & Examples & Target Predicate Mutagenesis & 7045 & 20 & 188 & UW - CSE & 7045 & 15 & 16714 & Cora & 3079 & 10 & 70367 & IMDB & 316 & 10 & 14505 & tabulartableAs baseline we are comparing our method with the state of the art algorithms based on Markov Logic Networks such GSLP ( dinh2011generative ) , LSM ( kok2009learning ) , MLN - B ( Boosted MLN ) , B - RLR ( ramanan2018structure ) as well as probabilistic ILP based algorithms such as SleepCover ( bellodi2015structure ) ."
CC,cross - correlation,TR-19196,"R1IR = [ 90.8 ] & & & & & & EO & & & & R1IR = [ 85.6 ] 2cTask & 9lEC : resting state with eyes closed , EO : resting state with eyes open , MI : motor imagery , ERP : event related potential 2c2*Classifier & 9lANN : artificial neural networks , FDA : Fisher discriminant analysis , KNN : k - nearest neighbours , LDA : linear discriminant analysis & & 9lMAP : maximum a posteriori , CC : cross correlation , L1 ( Manhattan ) distance , L2 ( Euclidean ) distance , cosine distance tabular table*Previous protocols sec : previous_protocolTable table : comparison summarises the state - of - the - art of the existing EEG biometrics applications based on multiple data acquisition days ."
NB,naive bayes,TR-19413,"max width=0.75Classification performance of the compared algorithms with NB classifier ( averaged over 40 runs ) ' Mean'and ' SD ' - Mean and standard deviation of classification error over 40 runs ' PI'- Percentage improvement in the classification error relative to the classification error achieved with all features max width=0.75Classification performance of the compared algorithms with k - NN Classifier ( averaged over 40 runs ) ' Mean'and ' SD ' - Mean and standard deviation of classification error over 40 runs ' PI'- Percentage improvement in the classification error relative to the classification error achieved with all features Further , paired t - tests were used to evaluate the statistical significance of the results relative to the compared algorithm ."
PT,proof time,TR-19443,tabularcccccccApplication & Transactions & Robustness & 2cReachability Analysis & 2cCDG Analysis & & & PO & PT & PO & PTAuction & 4 & & 70 & 0.3 & 20 & 0.5Courseware & 5 & & 59 & 0.37 & na & na FusionTicket & 4 & & 72 & 0.3 & 34 & 0.5SmallBank & 5 & & 48 & 0.28 & na & na TPC - C & 5 & & 54 & 0.7 & 82 & 3.7Cassieq - Core & 8 & & 173 & 0.55 & 104 & 2.9Currency - Exchange & 6 & & 88 & 0.35 & 26 & 3.5PlayList & 14 & & 99 & 4.63 & 236 & 7.3 RoomStore & 5 & & 85 & 0.3 & 22 & 0.5Shopping - Cart & 4 & & 58 & 0.25 & T & T tabulartab : expertable
FEC,forward error correction,TR-19582,The parameters are defined as follows : FEC - based : accounts for mechanisms that employ FEC ; ARQ - based : mark mechanisms that use ARQ ; QoE - sensitive data : this parameter demonstrates mechanisms that identity and/or considerate the video content to define the EC policy ; Video - aware : check mark is given to mechanisms that use any video characteristics to define the amount of redundancy and/or retransmission ; High - quality video : it is marked if the mechanisms are using videos equal or higher than 720p ( HD ready ) ; Network status : this parameter defines if the mechanisms use the information about the network healthy to define the redundant data ; UEP - enabled : means that different amounts of redundancy are being added to distinct portions of the video .
CT,computed tomography,TR-19742,figure figure [ ht ] center minipage0.15 ./fig / snapshot0109new2.png minipage minipage0.15 ./fig / snapshot0129new2.png minipage minipage0.15 ./fig / snapshot0139new2.png minipage minipage0.15 ./fig / snapshot0149new2.png minipage minipage0.15 ./fig / snapshot0110new3.png minipage minipage0.15 ./fig / snapshot0130new3.png minipage minipage0.15 ./fig / snapshot0140new3.png minipage minipage0.15 ./fig / snapshot0150new3.png minipage minipage0.15 ./fig / snapshot0106new3.png minipage minipage0.15 ./fig / snapshot0126new3.png minipage minipage0.15 ./fig / snapshot0136new3.png minipage minipage0.15 ./fig / snapshot0146new3.png minipage minipage0.15 ./fig / snapshot0103new3.png minipage minipage0.15 ./fig / snapshot0123new3.png minipage minipage0.15 ./fig / snapshot0133new3.png minipage minipage0.15 ./fig / snapshot0143new3.png minipage center 5fig : vis2_2 Visualizations for five anatomies on the first four holdout CT images .
SL,strictly local,TR-19922,table*[t]Accuracy on Target SL Stringsets Early Stoppingtab : resultsSLES4.5pttabularcccccccccc2c2Training & 2Test & 3cLSTM & 3cs - RNN & 2RPNI & & & 10 & 30 & 100 & 10 & 30 & 100 & 6SL2 & 21k & 1 & 0.818 ( 0.03 ) & 0.843 ( 0.05 ) & 0.923 ( 0.02 ) & 0.848 ( 0.06 ) & 0.904 ( 0.03 ) & 0.930 ( 0.03 ) & 0.855 & & 2 & 0.780 ( 0.06 ) & 0.820 ( 0.07 ) & 0.905 ( 0.04 ) & 0.871 ( 0.10 ) & 0.980 ( 0.02 ) & 0.992 ( 0.01 ) & 0.844 3 - 10 & 210k & 1 & 0.925 ( 0.07 ) & 0.851 ( 0.04 ) & 0.875 ( 0.04 ) & 0.936 ( 0.05 ) & 0.884 ( 0.07 ) & 0.729 ( 0.12 ) & 1.000 & & 2 & 0.919 ( 0.09 ) & 0.836 ( 0.06 ) & 0.835 ( 0.10 ) & 0.964 ( 0.07 ) & 0.868 ( 0.11 ) & 0.753 ( 0.15 ) & 1.0003 - 10 & 2100k & 1 & 0.737 ( 0.14 ) & 0.711 ( 0.14 ) & 0.730 ( 0.03 ) & 0.869 ( 0.15 ) & 0.767 ( 0.17 ) & 0.625 ( 0.01 ) & 1.000 & & 2 & 0.727 ( 0.15 ) & 0.698 ( 0.15 ) & 0.711 ( 0.04 ) & 0.885 ( 0.17 ) & 0.766 ( 0.19 ) & 0.605 ( 0.01 ) & 1.0006SL4 & 21k & 1 & 0.898 ( 0.01 ) & 0.939 ( 0.01 ) & 0.945 ( 0.01 ) & 0.908 ( 0.01 ) & 0.945 ( 0.01 ) & 0.958 ( 0.01 ) & 0.918 & & 2 & 0.829 ( 0.01 ) & 0.888 ( 0.01 ) & 0.887 ( 0.00 ) & 0.840 ( 0.01 ) & 0.883 ( 0.01 ) & 0.898 ( 0.01 ) & 0.8133 - 10 & 210k & 1 & 0.953 ( 0.05 ) & 0.956 ( 0.04 ) & 0.997 ( 0.00 ) & 0.976 ( 0.03 ) & 0.982 ( 0.00 ) & 0.981 ( 0.00 ) & 0.995 & & 2 & 0.934 ( 0.06 ) & 0.932 ( 0.05 ) & 0.995 ( 0.01 ) & 0.973 ( 0.04 ) & 0.989 ( 0.00 ) & 0.990 ( 0.00 ) & 0.978 3 - 10 & 2100k & 1 & 0.994 ( 0.00 ) & 0.973 ( 0.07 ) & 1.000 ( 0.00 ) & 0.990 ( 0.00 ) & 0.990 ( 0.00 ) & 0.987 ( 0.00 ) & 1.000 & & 2 & 0.996 ( 0.00 ) & 0.975 ( 0.07 ) & 1.000 ( 0.00 ) & 0.996 ( 0.00 ) & 0.996 ( 0.00 ) & 0.995 ( 0.00 ) & 1.0006SL8 & 21k & 1 & 0.966 ( 0.02 ) & 0.983 ( 0.01 ) & 0.995 ( 0.00 ) & 0.962 ( 0.02 ) & 0.969 ( 0.01 ) & 0.971 ( 0.01 ) & 0.991 & & 2 & 0.971 ( 0.01 ) & 0.980 ( 0.02 ) & 0.994 ( 0.00 ) & 0.969 ( 0.02 ) & 0.974 ( 0.01 ) & 0.977 ( 0.01 ) & 0.9663 - 10 & 210k & 1 & 0.990 ( 0.01 ) & 0.996 ( 0.00 ) & 0.999 ( 0.00 ) & 0.998 ( 0.00 ) & 0.999 ( 0.00 ) & 0.999 ( 0.00 ) & 0.998 & & 2 & 0.994 ( 0.00 ) & 0.995 ( 0.00 ) & 0.998 ( 0.00 ) & 0.997 ( 0.00 ) & 0.998 ( 0.00 ) & 0.998 ( 0.00 ) & 0.9943 - 10 & 2100k & 1 & 0.993 ( 0.01 ) & 0.998 ( 0.00 ) & 1.000 ( 0.00 ) & 0.999 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 & & 2 & 0.994 ( 0.01 ) & 0.999 ( 0.00 ) & 1.000 ( 0.00 ) & 0.999 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000tabulartable *
CNN,convolutional neural network,TR-20022,table[H ] tabularcccccccccc & 3c80 & 3c160 & 3c320 & SRP & GMBF & CNNf15 + 11+static & SRP & GMBF & CNNf15 + 11+static & SRP & GMBF & CNNf15 + 11+statu 2 * 01 & & & & & & & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & & & & & & & 2*Average & & & & & & & & & & & & & & & & & & tabular Results for the standard SRP - PHAT strategy ( columns SRP ) ; the one invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with the sequences described in Table tab : fine - tuning - material ( columns CNNf15 + 11+static ) tab : baselineResults+ft15 + 1 + 2 + 3table
AE,autoencoder,TR-20091,"Different architectures were tested for the DAEs : [ 513 , 128 , , 128 , 513 ] , [ 513 , 256 , , 256 , 513 ] and [ 513 , 256 , 128 , , 128 , 256 , 513]. Concerning the LSTM - AE , our implementation used two vanilla forward LSTM layers ( one for the encoder and one for the decoder ) with non - linear activation functions giving the following architecture : [ 513 , , 513]. Both LSTM layers were designed for many - to - many sequence learning , meaning that a sequence of inputs , i.e. of spectral magnitude vectors , is encoded into a sequence of latent vectors of same temporal size and then decoded back to a sequence of reconstructed spectral magnitude vectors ."
DA,data augmentation,TR-20131,"Binary Classification based NID*[t ] ML based 2-class NID with LR and SVM(Mean Std - Dev Percent ) * 6p2 cm p2.0 cm c c c c 0em 1lIntrusion Type & 1lNID Model Name & 1cAccuracy & 1cPrecision & 1cRecall & 1cF1-Score 0em 0em 4*'apache2 ' & NID - LR & 98.51 0.42 & 0.00 & 0.00 & - & NID - DA - LR & 99.53 0.05 & 77.29 1.21 & 90.70 3.41 & 83.45 2.14 0em 2 - 6 0em & NID - SVM & 98.97 0.02 & 55.73 0.56 & 99.32 0.38 & 71.39 0.49 & NID - DA - SVM & 99.94 0.01 & 95.61 0.87 & 99.70 0.07 & 97.61 0.44 0em 0em 4*'mailbomb ' & NID - LR & 91.93 0.44 & 0.00 & 0.00 & - & NID - DA - LR & 97.84 0.41 & 78.11 3.37 & 99.89 0.14 & 87.63 2.08 0em 2 - 6 0em & NID - SVM & 93.04 0.03 & 80.38 1.58 & 11.53 0.33 & 20.16 0.50 & NID - DA - SVM & 99.34 0.34 & 92.29 3.65 & 99.78 0.16 & 95.86 2.05 0em 0em 4*'processtable ' & NID - LR & 99.33 0.05 & 64.98 1.78 & 98.74 2.03 & 78.37 1.72 & NID - DA - LR & 99.53 0.04 & 72.46 1.81 & 100.00 0.00 & 84.02 1.21 0em 2 - 6 0em & NID - SVM & 99.87 0.06 & 90.79 3.86 & 99.60 0.56 & 94.96 2.14 & NID - DA - SVM & 99.90 0.04 & 92.60 2.17 & 99.71 0.15 & 96.02 1.22 0em 0em 4*'mscan ' & NID - LR & 97.78 0.11 & 42.58 1.35 & 84.90 1.48 & 56.71 1.42 & NID - DA - LR & 99.01 0.06 & 64.81 1.48 & 92.29 0.46 & 76.14 1.14 0em 2 - 6 0em & NID - SVM & 99.61 0.12 & 85.10 4.34 & 94.11 0.18 & 89.32 2.73 & NID - DA - SVM & 99.73 0.06 & 90.03 4.12 & 95.12 1.04 & 92.44 1.65 0em 0em 4*'saint ' & NID - LR & 98.22 0.22 & 40.17 2.89 & 96.47 1.45 & 56.65 2.81 & NID - DA - LR & 98.47 0.04 & 43.78 0.75 & 96.93 0.65 & 60.32 0.83 0em 2 - 6 0em & NID - SVM & 98.56 0.03 & 45.39 0.50 & 96.41 0.75 & 61.72 0.46 & NID - DA - SVM & 98.60 0.01 & 46.08 0.24 & 97.47 0.33 & 62.58 0.27 0em 0em 4*'guesspasswd ' & NID - LR & 88.59 0.75 & 34.21 1.50 & 75.10 2.87 & 46.98 1.44 & NID - DA - LR & 89.07 0.24 & 35.65 0.59 & 77.74 0.19 & 48.89 0.57 0em 2 - 6 0em & NID - SVM & 94.59 0.27 & 89.06 1.93 & 22.21 4.10 & 35.42 5.19 & NID - DA - SVM & 98.95 0.10 & 90.57 2.17 & 94.29 0.22 & 92.38 1.19 0em 0em 4*'snmpgetattack ' & NID - LR & 88.67 0.00 & 0.00 & 0.00 & - & NID - DA - LR & 80.42 0.58 & 36.61 0.69 & 99.43 0.07 & 53.51 0.72 0em 2 - 6 0em & NID - SVM & 88.65 0.02 & 0.00 & 0.00 & - & NID - DA - SVM & 82.42 0.03 & 39.13 0.03 & 99.39 0.21 & 56.15 0.04 0em 0em 4*'snmpguess ' & NID - LR & 98.84 0.15 & 78.61 2.51 & 95.93 0.05 & 86.39 1.55 & NID - DA - LR & 99.07 0.06 & 82.66 1.17 & 95.84 0.00 & 88.76 0.68 0em 2 - 6 0em & NID - SVM & 96.18 0.00 & 0.00 & 0.00 & - & NID - DA - SVM & 81.20 0.04 & 16.85 0.02 & 99.72 0.10 & 28.83 0.03 0em In this subsection , we evaluate binary classification performance of the DA enhanced NID framework ."
RL,reinforcement learning,TR-20157,Initialize weights of neural net arbitrary Initialize Precompute support grid On each interaction step : select observe transition construct -step transition and add it to experience replay with priority sample batch of size from experience replay using probabilities compute weights for the batch ( where is the size of experience replay memory ) for each transition from the batch compute target ( detached from computational graph to prevent backpropagation ) : project on support update transition priorities compute loss : make a step of gradient descent using if : Policy Gradient algorithmsPolicy Gradient theoremAlternative approach to solving RL task is direct optimization of objectiveas a function of .
ADN,activity driven networks,TR-20159,"List of SymbolsList of abbreviationslongtablecc Acronym & Descriptions ADN & Activity driven network modelling AV & Acquaintance vaccination APV & Absolute percentage variation CIP & co - location interaction parameters CN & Common neighbours DST network & Dense SPST network DDT network & Dense SPDT network DDT1 & Vaccinating neighbours in DDT network with direct linksDDT2 & Vaccinating neighbours in DDT network with any linksGDT & Generated SPDT network with 364 K nodes GST & Generated SPST network with 364 K nodes IMV & Individual movement based vaccination strategy IMVE & Individual movement based vaccination strategy with exact information IMVT & Individual movement based vaccination strategy with temporal information LST & SPDT network with the same number of links that of DDT network LST & SPST network with the same number of links that of DST network MLE & Maximum likelihood estimator OSN & Online social network PFU & Plaque - forming unit RV & Random vaccination DV & Degree vaccination RSE & Root squared error SPST & Same place same time transmission SPDT & Same place different time transmission SIR & Susceptible - infected - recovered SDT network & Sparse SPDT network of links having direct and indirect components SST network & Sparse SPST network of links having indirect component onlySPDT graph & graph based on SPDT diffusion longtableList of symbolslongtablecc Symbol & Descriptions A & Set of active copies of nodes in SPDT graph b & active particle decay rates from an area of interaction & activity potential of node & active periods of a node C & Particle concentration in interaction area C , C , C & scenario 1 , 2 and 3 d & Activation degree - number of SPDT links created during an activation E & Intake dose or exposure of infectious particles & Average volume fraction of room air introduced by exhaled breath f & distribution function F & Disease spreading force in the network at the current day of simulation & Average disease spreading force in the network & Graph & Dynamic graph g & Particle generation rate by an infected individual h & Activation frequency I & Number of infected individuals in the system & Number of infected individuals at a simulation day & Number of infected individuals in the system at the current time that disease prevalence & Number of infected individuals up to a simulation day L & links set N & Total number of individuals , nodes , users p & Pulmonary rate of susceptible individual & Infection probability for an intake dose & Probability of creating a link during an activation & Probability of breaking a created link Q & air exchange rate from an area q & Transition probability for changing inactive to active state R & disease reproduction ability of an infected individual r & Particle removal rate from interaction area r & Median of particles removal rates S & Number of susceptible individual T & Simulation period or disease observation period & Activation period or period host user or node stays at the interacted location & Link creation delay or delay neighbour user or node arrives at the interacted location & Stay duration of user or node stays at the interacted location V & Air volume of interaction area , , & waiting periods of a node Y & Labelling sets in graph X & updates z & Number of time step Z & set of nodes in the SPDT graph & power law exponent & Infection rate at the current day of simulation in the network for an infected individual & Indirect transmission period & central tendency & Scaling parameter of activation degree distribution & neighbour proportion & average volume fraction of room air that is exhaled by an susceptible individual & links presence function & state probability & nodes presence function & switching probability form active to inactive states & Infectiousness of infection particles & Fraction of dose or exposure reaches to the target infection site & Duration that virus is generated or infectious period of infected individual & inter - event time for node in activity driven networks & lower limit of active degree distribution & activation potential in ADN networks & particle accumulation rate & Transition probability for changing active to inactive state longtable"
PSL,probabilistic soft logic,TR-20270,"Here we highlight our contributions : i ) we present a novel reasoning component that successfully infers answers from various ( noisy ) knowledge sources for ( primarily what and which ) questions posed on unconstrained images ; ii ) The reasoning component is an augmentation of the PSL engine to reason using phrasal similarities , which by its nature can be used for other language and vision tasks ; iii ) we annotate a subset of Visual Genome captions with word - pairs and open - ended relations , which can be used as the seed data for semi - supervised semantic parsing of captions ."
MPC,model predictive control,TR-20402,"In this context , there are some alternatives already available in the literature to suppress the disturbance while tracking the commanded trajectories including MPC [ 28],[29 ] , dynamic inversion control [ 30],[31 ] , and adaptive control [ 32],[33]. Among them , SMC is one of the strong candidates to handle internal and/or external disturbances and the system uncertainties [ 34],[35]. In this context , SMC - based approaches are investigated in a simulated environment for the attitude and the position loops of the UAVs [ 36]. As a real - time experiment , a finite time stabilizing SMC is applied on a ground - based experimental setup [ 37]. However , the evaluation is restricted to the attitude channel of the system ."
CPI,consumer price index,TR-20420,"Sources : BOE ( ID : IUQLBEDR , XUQLBK82 , IUQLBEDR , LPQAUYN ) , ONS ( ID : D7BT , UKEA , PGDP , PRDY , MGSX ) , BIS ( US private sector debt : Q : US : P : A : M : XDC : A , UK : ERI , GBP / USD ( 1955 only ) ) , OECD ( US CPI , US M3 , US GDP , US Unemployment , US CA ) , FRED ( ID : RNUSBIS , FEDFUNDS , PRS85006163 , A229RX0 ) , ( UK private sector debt , M4 , labour productivity)[h ! ]"
PF,propagation fusion,TR-20488,"For evaluating structure fusion generalization , we compare structure fusion based graph convolutional networks ( SF - GCN ) , propagation fusion based graph convolutional networks ( PF - GCN ) and structure propagation fusion based graph convolutional networks ( SPF - GCN).In Table , we observe that the performance of SPF - GCN is better than that of other method , and the least improvement of SPF - GCN respectively is for Cora , for Citeseer and for PubMed , while the performance of SP is superior to that of PF - GCN , and the improvement of SF - GCN respectively is for Cora , for Citeseer and for PubMed Therefore , PF and SF both are benefit for further mining the structure information and the role of SF is more important than that of PF ."
ECC,error correcting code,TR-20601,"tabularl cc ccc cc ccc & 5cTesla K40c ( ECC on ) & 5cGeForce GTX 1080 ( r)2 - 6(l)7 - 11Graph Name & Near - Far & Bucketing & 3cMultisplit - SSSP & Near - Far & Bucketing & 3cMultisplit - SSSP ( r)1 - 1 ( r)2 - 2 ( r)3 - 3 ( r)4 - 6 ( l)7 - 7 ( l)8 - 8 ( l)9 - 11- & time ( ms ) & time ( ms ) & time ( ms ) & MTEPS & speedup & time ( ms ) & time ( ms ) & time ( ms ) & MTEPS & speedup cit - Patents & 458.4 & 3375.9 & 343.1 & 96.3 & 1.34x & 444.2 & 3143.0 & 346.8 & 95.2 & 1.28x flickr & 96.0 & 163.0 & 64.5 & 305.2 & 1.49x & 66.7 & 111.1 & 36.5 & 539.0 & 1.83x belgiumosm & 561.4 & 3588.0 & 604.5 & 5.12 & 0.93x & 443.8 & 3014.2 & 427.0 & 7.3 & 1.04x rmat & 20.9 & 28.7 & 13.2 & 727.3 & 1.58x & 12.17 & 14.9 & 5.8 & 1655.2 & 2.17x tabularNear - Far , Bucketing , and our new Multisplit - SSSP methods over various datasets ."
ATE,absolute trajectory error,TR-20616,"tabularcY1cmY1cmY1cmY1cmY1cmY1cmY1cmY1cmY1 cm Sequence & 3cfr1-xyz & 3cfr2 - 360-hs & 3cfr3-walk - xyz 2*Method & ATE & RPE & RPE & ATE & RPE & RPE & ATE & RPE & RPE & ( m ) & ( m ) & ( ) & ( m ) & ( m ) & ( ) & ( m ) & ( m ) & ( ) LSD - SLAMengel2014lsd & 0.090 & - & - & - & - & - & 0.124 & - & - ORB - SLAMmur2015orb & 0.009 & 0.007 & 0.645 & - & - & - & 0.012 & 0.013 & 0.694 DeMoN(10)UZUMIDB17 & 0.178 & 0.021 & 1.193 & 0.601 & 0.035 & 2.243 & 0.265 & 0.049 & 1.447 DeMoN(1)UZUMIDB17 & 0.183 & 0.037 & 3.612 & 0.669 & 0.032 & 3.233 & 0.279 & 0.040 & 3.174 Ours(fully connected ) & 0.169 & 0.028 & 1.887 & 0.883 & 0.030 & 1.799 & 0.268 & 0.044 & 1.698 Ours(iterative ) & 0.071 & 0.024 & 1.237 & 0.461 & 0.020 & 0.736 & 0.240 & 0.026 & 0.811tabulartab : rgbd_posetableAblation Experimentssubsec : ablation_studiesIn order to examine the contribution of using each component of our pose estimation network , we compare the pose estimates under various configurations on sequences 09 and 10 of the KITTI odometry datasetGeiger2013IJRR , summarised in Table tab : kitti_confidence ."
NN,neural network,TR-20685,"& 3cLAN ( it / sec ) & 3cWAN ( it / min ) 3 - 8 & & 1cB-128 & 1cB-256 & 1cB-512 & 1cB-128 & 1cB-256 & 1cB-512 2*NN & ABY3 & & & & & & & This & & & & & & 2*CNN & ABY3 & & & & & & & This & & & & & & tabular Comparison of ABY3 ( Malicious ) and This for NN and CNN ( higher = better).tab : NNBenchtableWe consider a NN with two hidden layers , each having 128 nodes followed by an output layer of 10 nodes ."
RPE,relative pose error,TR-20767,"tabularcY1cmY1cmY1cmY1cmY1cmY1cmY1cmY1cmY1 cm Sequence & 3cfr1-xyz & 3cfr2 - 360-hs & 3cfr3-walk - xyz 2*Method & ATE & RPE & RPE & ATE & RPE & RPE & ATE & RPE & RPE & ( m ) & ( m ) & ( ) & ( m ) & ( m ) & ( ) & ( m ) & ( m ) & ( ) LSD - SLAMengel2014lsd & 0.090 & - & - & - & - & - & 0.124 & - & - ORB - SLAMmur2015orb & 0.009 & 0.007 & 0.645 & - & - & - & 0.012 & 0.013 & 0.694 DeMoN(10)UZUMIDB17 & 0.178 & 0.021 & 1.193 & 0.601 & 0.035 & 2.243 & 0.265 & 0.049 & 1.447 DeMoN(1)UZUMIDB17 & 0.183 & 0.037 & 3.612 & 0.669 & 0.032 & 3.233 & 0.279 & 0.040 & 3.174 Ours(fully connected ) & 0.169 & 0.028 & 1.887 & 0.883 & 0.030 & 1.799 & 0.268 & 0.044 & 1.698 Ours(iterative ) & 0.071 & 0.024 & 1.237 & 0.461 & 0.020 & 0.736 & 0.240 & 0.026 & 0.811tabulartab : rgbd_posetableAblation Experimentssubsec : ablation_studiesIn order to examine the contribution of using each component of our pose estimation network , we compare the pose estimates under various configurations on sequences 09 and 10 of the KITTI odometry datasetGeiger2013IJRR , summarised in Table tab : kitti_confidence ."
SE,small enough,TR-20781,table[ht]tabularrlrrrr & Method & & & & 1 & Laplace & -18560 & 0.3 & -18560 & -18561 & ADVI(FR ) & -18562 & 1.0 & -18559 & -18560 & ADVI(MF ) & -18564 & 2.1 & -18559 & -18558 & MCMC & -18559 & 0.3 & -18560 & -18559 2 & Laplace & -17058 & 31.9 & -17049 & -17142 & ADVI(MF ) & -17068 & 50.1 & -17059 & -17105 & MCMC & -17064 & 30.7 & -17067 & -17110 3 & Laplace & -17035 & 33.0 & -17017 & -17117 & ADVI(MF ) & -17097 & 20.7 & -17090 & -17090 & MCMC & -17068 & 17.4 & -17085 & -17102 4 & Laplace & -17003 & 66.6 & -16866 & -17057 & ADVI(MF ) & -16990 & 19.8 & -17013 & -17034 & MCMC & -17043 & 19.5 & -17022 & -17049 5 & ADVI(MF ) & -18223 & 37.6 & -18225 & -18285 & MCMC & -18295 & 52.2 & -18253 & -18308 6 & ADVI(MF ) & -16656 & 90.8 & -16603 & -16869 & MCMC & -16835 & 63.2 & -16798 & -16864 7 & Laplace & -17096 & 45.8 & -17063 & -17140 & ADVI(MF ) & -16996 & 25.4 & -16957 & -17035 & MCMC & -17126 & 27.3 & -17136 & -17050 tabularThe estimated using a subsample of size and its standard error ( SE ) .
HDT,header dictionary triple,TR-20854,"tab : misc2 tabularlcccccccccccccc Query & Res & 2cRedland & Virtuoso&Stardog & HDT & 4cTripleID & 4cSpeedup 3 - 48 - 15 & & load & query & & & & load & data & join & query & Redland & Virtuoso & Stardog & HDT Q1 & 20,977 & 62.78 & 10.45 & 15.76 & 1.62 & 0.65 & 1.86 & 0.29 & - & 0.41 & 25.49 & 38.43 & 3.95&1.58 Q2 & 1,119,681 & 62.40 & 57.34 & 1173.10 & 821.28 & 3.80 & 1.7 & 0.29 & - & 0.97 & 59.11 & 1209.38 & 846.68&3.92Q3 & 1,220,456 & 62.52 & 80.9 & 1391.87 & 880.19 & 4.67 & 1.69 & 0.31 & - & 1.02 & 79.31 & 1364.57 & 862.93&4.44 Q4 & 1,242,627 & 62.72 & 102.09 & 1399.37 & 908.90 & 5.22 & 1.71 & 0.30 & - & 1.06 & 96.31 & 1320.16 & 857.45&4.92Q5 & 24 & 62.19 & 7.78 & 0.85 & 0.18 & 0 & 1.68 & 0.27 & - & 0.33 & 23.58 & 2.58 & - & -Q6 & 18 & 62.35 & 7.75 & 0.61 & 0.07 & 0 & 1.69 & 0.29 & - & 0.33 & 23.48 & 1.85 & - & -Q7 & 23 & 62.28 & 15.96 & 1.86 & 0.13 & 0.58 & 1.68 & 0.3 & - & 0.38 & 42.00 & 4.89 & -&1.52Q8 & 26,307 & 62.42 & 27.92 & 61.20 & 1.67 & 1.15 & 1.68 & 0.3 & - & 0.45 & 62.04 & 136.00 & 3.71&2.55Q9 & 10 & 62.15 & 50.57 & 0.53 & 0.09&1.31 & 1.68 & 0.29 & 0 * & 0.46 & 109.93 & - & -&2.85Q10 & 0 & 62.61 & 51.72 & 3.39 & 0.14 & 1.84 & 1.71 & 0.31 & 0 * & 0.49 & 105.56 & 10.93 & -&3.75Q11 & 98 & 61.59 & 7.73 & 0.55 & 0.11 & 0.53 & 1.71 & 0.29 & 0 * & 0.35 & 22.09 & 1.58&-&1.5Q12 & 1,542 & 61.72 & 8.46 & 11.24 & 0.58 & 0.66 & 1.73 & 0.28 & 0 * & 0.39 & 21.69 & 28.82 & 1.49&1.69Q13 & 31,863 & 62.33 & 11.58 & 24.71 & 2.99 & 0.73 & 1.72 & 0.3 & 0.03 & 0.41 & 28.24 & 60.27&7.29&1.78Q14 & 148,213 & 61.55 & 19.18 & 73.01 & 20.62 & 1.20 & 1.7 & 0.3 & 0.11 & 0.52 & 36.88 & 140.41&39.65&2.31Q15 & 5,715 & 63.38 & 9.31 & 1.12 & 0.59 & 0.61 & 1.71 & 0.29 & 0.01 & 0.39 & 23.87 & 2.88&1.51&1.56Q16 & 90,504 & N / A & N / A & 2140.42 & 645.56 & 10.67 & 1.72 & 0.31 & 0.32 & 0.76 & - & 2,816.35 & 849.42&14.03tabulartable *"
CC,corpus callosum,TR-20866,"Along - fiber GFA visualization and cosine similarity between pairs of fibers from three prominent bundles : a ) CST ( R ) , b ) CC , c ) IFOF ( R ) , using framework of varifolds ( Var ) and functional varifolds ( fVar ) ( top left ) , and Comparing variation of cosine similarity for the select fiber pairs over kernel bandwidth parameters and for the framework of functional varifolds ( top right : CST ( R ) , middle left : CC , middle right : IFOF ( R ) ) ; Impact of on clustering consistency ( measured using Average Silhouette ) for for functional Varifolds vs Varifolds ( bottom left ) , and functional Varifolds vs GFA only ( bottom right ) Figure ( top left ) shows GFA color - coded fiber pairs ."
ACE,average causal effect,TR-20874,"Infant Health and Development Program : BaselinesFor our experiments , we compare the performance of CEIB for predicting the ACE against several existing baselines as intLouizos : OLS-1 is a least squares regression ; OLS-2 uses two separate least squares regressions to fit the treatment and control groups respectively ; TARnet is a feedforward neural network fromtpmlr - v70-shalit17a ; KNN is a -nearest neighbours regression ; RF is a random forest ; BNN is a balancing neural networkpJohanssonTAR ; BLR is a balancing linear regressionpJohanssonTAR , and CFRW is a counterfactual regression that using the Wasserstein distanceppmlr - v70-shalit17a ."
AIDA,"atomic , independent , declarative , and absolute",TR-21034,"tabular@l@ l@ p10.9cm@class & properties & languages 1551 & c t w i & IBM 's EasyEnglish & c w s g & Special English & c w a & E - Prime & c w g & Plain Language 2132 & c s d g & CAA Phraseology , FAA Phraseology , ICAO Phraseology , PoliceSpeak , SEASPEAK 2133 & c w d i & Airbus Warning Language 2541 & f w a & AIDA 2551 & c t w d a i & ALCOGRAM , COGRAM & c t w d a & CLCM & c t w d i & ASD - STE , Avaya CE , Bull GE , CTE , CASL , CE at Douglas , DCE , General Motors GE , PACE , Sun Proof & c t w d & Wycliffe Associates ' EasyEnglish & c t w i & iCE , SMART Controlled English & c w d i & AECMA - SE , CFE , CASE , CE at Clark , CE at IBM , CE at Rockwell , EE , HELP , ILSAM , KISL , NCR FE & c w d g & Massachusetts Legislative Drafting Language & c w i & Boeing Technical English , NSE , SMART Plain English & c w & Basic English & t w d i & MCE , Oce Controlled English & t w a & KCE & t w i & CLOUT 3142 & c f w d i & SLANG & f s d i & Voice Actions 3243 & f w d a & RNLS 3333 & f w a & ClearTalk & f w i & ITA CE 3342 & f w i & CPL 3442 & c f w i & RuleSpeak , SBVR - SE 4143 & f w d a & Drafter Language , MILE Query Language 4144 & f w a & Quelo Controlled English 4153 & t f d a & PILLS Language 4243 & f w d a & Atomate Language & f w a i & Gellish English & f w a & GINO 's Guided English & f w i & CELT 4343 & f w d a & PROSPER CE & f w a & ACE 4353 & f w d a & ICONOCLAST Language 5143 & f w d a & CLEF Query Language & f w a & Ginseng 's Guided English 5144 & f w d a & Coral 's Controlled English & f w a & PathOnt CNL 5145 & f w a & Sowa 's syllogisms 5234 & f w d a i & TBNLS & f w a & OWLPath 's Guided English , SQUALL 5243 & f w a & CPE , CLIP , OWL ACE , SOS 5244 & f w d a & BioQuery - CNL , PERMIS CNL , ucsCNL & f w a & CLOnE , DL - English , E2V , Lite Natural Language , OSE & f w g & Rabbit 5333 & f w d a & CLM , ForTheL , Naproche CNL & f w a & CLCE , PNL 5343 & f w d a & Gherkin & f w a g & RECON & f w a & First Order English , PENG , PENG - D , PENG Light & f w i & iLastic Controlled English 5433 & f w a & FEtabular"
CNN,convolutional neural network,TR-21047,"table[H ] tabularcccccccccc & 3c80 & 3c160 & 3c320 & SRP & GMBF & CNN & SRP & GMBF & CNN & SRP & GMBF & CNN 2 * 01 & & & & & & & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & & & & & & & 2*Average & & & & & & & & & & & & & & & & & & tabular Baseline results for the SRP - PHAT strategy ( columns SRP ) ; the one invelasco2012-F ( columns GMBF ) , and the CNN trained with synthetic data without applying the fine - tuning procedure ( columns CNN ) for sequences 01 , 02 and 03 for different window sizes ."
CPM,completely positive maps,TR-21060,"The compact structure for is given by the doubles of the cups and caps of , while the adjoint of a process in the form of Diagram is given by first taking the adjoint in , and then using the following equation for the adjoint of the discarding map : Because the doubled processes and the discarding maps are well - defined CP maps , it is legitimate to rephrase the very definition of the CPM category by saying that its processes are exactly those in the following form : This means that doubled processes and discarding maps are enough to express all CP maps , but to prove results about CP maps we need a graphical axiom relating a generic CPM category to the corresponding original category ."
PSO,particle swarm optimization,TR-21124,"return NULL ; Agent * a = NULL ; a = ( Agent * ) malloc(sizeof(Agent ) ) ; a->v = NULL ; a->strength = NULL ; / * > > > NEW LINE HERE < < < * / switch ( opt_id ) case _ PSO _ : a->v = ( double * ) malloc(n*sizeof(double ) ) ; break ; ... case _ BSO _ : / * > > > NEW CASE HERE < < < * / a->strength = ( double * ) malloc(n*sizeof(double ) ) ; break ; default : free(a ) ; fprintf(stderr,""optimization identifier @CreateAgent "" ) ; return NULL ; break ; a->x = ( double * ) malloc(n*sizeof(double ) ) ; return a ; In function DestroyAgent ( LibOPT / src / common.c ) , you should deallocate your new variable : / * It deallocates an agentParameters : a : address of the agent to be deallocatedopt_id : identifier of the optimization technique * /void DestroyAgent(Agent * * a , int opt_id ) Agent * tmp = NULL ; tmp = * a ; if(!tmp ) fprintf(stderr,""not allocated @DestroyAgent . "" ) ;"
OP,old persian,TR-21146,"[ noitemsep]*huarnah-/*farnah- MP xwarrah NP farr ' glory'*uarna - ka- ' wool ' Phl wlk ' , MMP wrg /warrag/ NP barrah ' lamb'*parna- Phl pl , MMP pr /parr/ NP par(r ) ' feather'OP krnuvaka < k - r - nu - u - v - k - a > ' stonemason ' Phl < k(y)lwk ' > , MMP < qrwg > ' artisan ' , generally transcribed as kirrog on the basis of Armenian krogpet , though late OP * uva would seem to yield MP u*d(a)r - n- Phl dl- /darr/ NP darr- ' to rend , tear up'darrah*us - prna- Phl spwl , MMP ' spwr /aspurr/ ' accomplished'It has been suggested that the changes * rn rr and * rn l are interconnected , and that l(l ) r(r ) variation in reflexes of * rn represents dialectal variation within West Iranian ( [ 292 , fn ."
NE,nash equilibrium,TR-21159,"The AlgorithmsWe use two multi - population ( each player has its own population of chromosomes representing its alternative choices at any round ) co - evolutionary genetic algorithms , Vriend 's individual learning algorithm [ 15 ] and co - evolutionary programming , a similar algorithm that has been used for the game of prisoner 's dilemma [ 10 ] and , unsuccessfully , for Cournot Duopoly [ 13]. Since those two algorithms do n't , as it will be seen , lead to convergence to the NE in the models under consideration , we introduce two different versions of the algorithms , as well , which are characterized by the use of opponent choices , when the new generation of each player 's chromosome population is created , and therefore can be regarded as "" socialized "" versions of the two algorithms ."
CNL,certain natural language,TR-21172,"tabular@l@ l@ p10.9cm@class & properties & languages 1551 & c t w i & IBM 's EasyEnglish & c w s g & Special English & c w a & E - Prime & c w g & Plain Language 2132 & c s d g & CAA Phraseology , FAA Phraseology , ICAO Phraseology , PoliceSpeak , SEASPEAK 2133 & c w d i & Airbus Warning Language 2541 & f w a & AIDA 2551 & c t w d a i & ALCOGRAM , COGRAM & c t w d a & CLCM & c t w d i & ASD - STE , Avaya CE , Bull GE , CTE , CASL , CE at Douglas , DCE , General Motors GE , PACE , Sun Proof & c t w d & Wycliffe Associates ' EasyEnglish & c t w i & iCE , SMART Controlled English & c w d i & AECMA - SE , CFE , CASE , CE at Clark , CE at IBM , CE at Rockwell , EE , HELP , ILSAM , KISL , NCR FE & c w d g & Massachusetts Legislative Drafting Language & c w i & Boeing Technical English , NSE , SMART Plain English & c w & Basic English & t w d i & MCE , Oce Controlled English & t w a & KCE & t w i & CLOUT 3142 & c f w d i & SLANG & f s d i & Voice Actions 3243 & f w d a & RNLS 3333 & f w a & ClearTalk & f w i & ITA CE 3342 & f w i & CPL 3442 & c f w i & RuleSpeak , SBVR - SE 4143 & f w d a & Drafter Language , MILE Query Language 4144 & f w a & Quelo Controlled English 4153 & t f d a & PILLS Language 4243 & f w d a & Atomate Language & f w a i & Gellish English & f w a & GINO 's Guided English & f w i & CELT 4343 & f w d a & PROSPER CE & f w a & ACE 4353 & f w d a & ICONOCLAST Language 5143 & f w d a & CLEF Query Language & f w a & Ginseng 's Guided English 5144 & f w d a & Coral 's Controlled English & f w a & PathOnt CNL 5145 & f w a & Sowa 's syllogisms 5234 & f w d a i & TBNLS & f w a & OWLPath 's Guided English , SQUALL 5243 & f w a & CPE , CLIP , OWL ACE , SOS 5244 & f w d a & BioQuery - CNL , PERMIS CNL , ucsCNL & f w a & CLOnE , DL - English , E2V , Lite Natural Language , OSE & f w g & Rabbit 5333 & f w d a & CLM , ForTheL , Naproche CNL & f w a & CLCE , PNL 5343 & f w d a & Gherkin & f w a g & RECON & f w a & First Order English , PENG , PENG - D , PENG Light & f w i & iLastic Controlled English 5433 & f w a & FEtabular"
CRF,conditional random field,TR-21348,"A CRF model was used by with the following features : tokens , POS tags , syntactic dependency ( if the opinion target has a relation with the opinionated word ) , word distance ( the distance between the word in the closest noun phrase and the opinionated word ) , and opinion sentences ( each token in the sentence containing an opinionated expression is labeled by this feature ) , the input of this method is also the opinionated expressions , they use these expressions for predicting the opinion target polarity using the dependency parsing for retrieving the pair target - expression from the training set ."
SVM,support vector machine,TR-21365,"dev None & 0.4212 & ( + /- 0.06 ) & 0.7572 & ( + /- 0.02 ) Sub & 0.4050 & ( + /- 0.06 ) & 0.6922 & ( + /- 0.03 ) Super & 0.4418 & ( + /- 0.05 ) & 0.7106 & ( + /- 0.02 ) Sub+Super & 0.4807 & ( + /- 0.09 ) & 0.6658 & ( + /- 0.03 ) tabular svm ' sample results tab : svm_sampletableFirst off , even though the sub- and super - sampling techniques yielded similar SDQC distributions , we see a clear advantage for the SVM with more data points , scoring 0.4050 with "" Sub "" and 0.4418 with Super , the latter improving over the original result with ."
CT,class table,TR-21369,"figure[t ] gather * [ field - lookup ] C.f_i : C_i fields(C , CT ) field(f_i , C , CT ) = C_i [ extends ] ( C.extends = D ) CT extends(C , CT ) = D [ S - Extend ] ( C.extends = D ) CT CT satisfy ( C.extends : D , cond ) [ S - Constructor ] fields(C , CT ) = C.f : C_f CT satisfy ( C.init(C_f ) , cond ) [ S - Field ] field(f , C , CT ) = C ' CT satisfy ( C.f : C ' , cond ) [ S - Method ] if mtype(m , C , CT ) = CC ' CT satisfy ( C.m : CC ' , cond ) [ Satisfy ] ( cond hold CT satisfy ( CReq , cond ) ) & ( CReq , cond ) CR CT satisfy CR gather * Judgment for Satisfy ."
SR,success rate,TR-21488,tab : table20.950.5mmtabularlcccc1 - 5Category & Kitchen & Living room & Bedroom & Bathroom & P=15.2 & P=15.6 & P=20.0&P=20.0Random & 0.0 / 0.0 & 1.6 / 1.0 & 2.0 / 1.1 & 1.2 / 0.7 TD - A3C & 17.4 / 3.1 & 13.2 / 2.1 & 16.9 / 1.9 & 32.4 / 9.0 TD - A3C(BC ) & 21.3 / 7.8 & 18.2 / 5.2 & 22.4 / 8.1 & 30.1 / 10.4Ours & 42.6 / 23.6 & 36.7 / 19.6 & 40.6 / 21.8 & 62.7 / 38.1 Ours - FroView & 34.8 / 11.2 & 17.6 / 5.0 & 28.0 / 9.2 & 48.8 / 16.0 Ours - NoGen & 38.8 / 22.0 & 28.8 / 15.4 & 38.8/ 22.7 & 58.4 / 35.2 Ours - VallinaGen & 47.2 / 24.0 & 15.6 / 6.1 & 34.8 / 13.5 & 52.4 / 27.3 tabulartabletableAverage navigation performance ( SR and SPL in ) comparisons on unseen scenes from AI2-THOR without stop action .
TA,threshold algorithm,TR-21630,figure*[!ht ] minipage[t]0.23 figures / TimeVsDimensionalityFullSpace.pdf Varying query size fig : algorithms minipage minipage[t]0.23 figures / TimeVsDimensionality.pdf Varying query size fig : syn_TimeVsDimension_Dist minipage minipage[t]0.23 figures / TimeVsN.pdf Varying number of tuples fig : syn_TimeVsN minipage minipage[t]0.25 figures / TimeVsCardinality.pdf Varying cardinality fig : syn_TimeVsC minipagefigure*figure*[!ht ] minipage[t]0.25 figures / TimeVsOutputTuple.pdf Time vs number of skylines returned fig : syn_TimeVsNumberOfSkylines minipage minipage[t]0.25 figures / NumberOfTuplesVsOutputTuples.pdf Tuples accessed vs number of skylines returned fig : syn_NumberOfTuplesVsSkylines minipage minipage[t]0.21 figures / TimeVsDimensionalityAirbnb.pdf AirBnB : Varying query size fig : algorithmsAirbnb minipage minipage[t]0.23 figures / TimeVsDimensionalityAirbnbTASky.pdf AirBnB : TA - SKY performance v.s .
FA,fractional anisotropy,TR-21683,table tabular[t]ll 2cGender Set 1 & RAVLT Total ( 1 - 5 ) & FA Cingulum L Set 2 & FA Medial lemniscus L & FA Cingulum ( hippocampus ) L & FA Post thalamic radiation L Set 3 & FA Corticospinal tract R & FA Superior O.F. fasciculus R tabular tabular[t]ll 2cGenotype : APOE4 Digit Span Backward Raw Score & Stroop Color - word Score PiB Cingulum Post L & PiB Cingulum Post R PiB Frontal Med Orb L & PiB Frontal Med Orb R PiB Precuneus L & PiB Precuneus R PiB SupraMarginal & PiB Temporal Mid R tabular Group difference across Gender ( left ) and Genotype APOE4 expression ( right ) .
ACI,adjacent channel interference,TR-21720,"Computation of and sec : S : I : computationIt follows from the scheduling constraints constraintnRBsForAVUE and constraintnVUEsInAnRB that the desired signal power and interference power needed in the SINR constraint constraintMatrix2 can be computed asalign S_j , f , t & = _ i T_j X_i , f , t P_i , t H_i , j definitionS , I_j , f , t & = _ k T_j X_k , f , t P_k , t H_k , j & + Ff ' = 1 f ' f _ k N X_k , f',t P_k , t H_k , j , definitionI alignWe note that the first term in definitionI is CCI from VUEs not in and that the second term is ACI from all transmitting VUEs ."
CS,centralized solution,TR-21754,"In particular , we now make the following assumptions , that are common in most PEV - CS assignment problems : We use a Poisson process to model a new PEV requiring charging ( as in and);We assume that the time between the PEV charging request and the recommendation of the optimal CS is short enough so that the optimal solution is computed upon updated information ( this implies that the assignment optimization problem is solved in a very short time , e.g. , 1 s);We assume that the time required for charging is proportional to the required energy ( as in , and similar to and with constant charge step);Finally , we assume that once a vehicle accepts to get charged to the recommended CS , it will in fact drive towards the CS ."
HMC,hybrid monte carlo,TR-21920,XXXXXXX HMC Properties & Acceptance Rate ( ) & IACF equilibration & IACF production & Acceptance Rate ( ) & IACF equilibration & IACF production ( ps ) & 3c25 fs 1000 = 25 ps & 3c25 fs 2000 = 50 ps VV & 42.92 & 105.50 & 102.76 & 39.34 & 226.57 & FAILED BCSS & 17.64 & 259.38 & FAILED & 6.01 & 373.43 & FAILED AIA & 41.30 & 107.04 & 105.94 & 41.10 & 101.02 & 141.67 Acceptance rate and IACF for equilibration and production with HMC for the biggest simulated time step and two choices of lengths of MD trajectories .
LD,large deviation,TR-21987,"equationAs for the distribution of the survival probability , one may obtain a LD form for it in the following way : eqnarraysurv - prob - distrProb(P ) & = & dL Prob(L)(L - P)&=&d(L / m ) Prob(L / m)(L / m - P ) & & d(L / m)(-mI(L / m))(L / m - P ) ) & & ( -m min_L : L = mPI(L / m)),eqnarraywhere in the third step we have considered large and have used ( prob_log_surv ) , while in the last step we have used the saddle point method to evaluate the integral ."
RF,register file,TR-22025,"decorations.markings , shapes.geometric , arrows.meta , backgrounds , positioning , patterns , calc , short - format = TTA short = TTA , long = transport triggered architectureVLIWshort = VLIW , long = very long instruction wordFU short = FU , long = functional unitRF short = RF , long = register fileLSU short = LSU , long = load - store unitGCU short = GCU , long = general control unitTCE short = TCE , long = TTA - based Co - Design EnvironmentTUT short = TUT , long = Tampere University of TechnologyAG short = AG , long = address generatorTFG short = TFG , long = twiddle factor generatorCADDshort = CADD , long = complex adderFFT short = FFT , long = fast Fourier transformLSB short = LSB , long = least significant bitMSB short = MSB , long = most significant bitLUT short = LUT , long = lookup tableROM short = ROM , long = read - only memoryOFDMshort = OFDM , long = orthogonal frequency division multiplexingASICshort = ASIC , long = application specific integrated circuitFPGAshort = FPGA , long = field - programmable gate arrayCGRAshort = CGRA , long = coarse - grained reconfigurable arrayGPP short = GPP , long = general purpose processorGPU short = GPU , long = graphics processung unitPE short = PE , long = processing elementDIT short = DIT , long = decimation - in - timeDIF short = DIF , long = decimation - in - frequencySNR short = SNR , long = signal - to - noise ratioLow - Power Programmable Processor for Fast Fourier Transform Based on Transport Triggered ArchitectureJakub Žádník and Jarmo Takala Faculty of Information Technology and Communication Sciences , Tampere University , Finland jakub.zadnik , jarmo.takala@tuni.fiThis paper describes a low - power processor tailored for fast Fourier transform computations where transport triggering template is exploited ."
SCS,spoken conversational search,TR-22194,landscapetable[]tabularlllll1c & 4c[HTML]FFFC9EUser 1c & 2c[HTML]FFFC9EQuery & 2c[HTML]FFFC9EFeedback 1c-3*Models & 1c[HTML]FFFC9EInformation & 1c[HTML]FFFC9EPrompt & 1c[HTML]FFFC9EPositive & 1c[HTML]FFFC9ENegative COR & request & withdraw & accept & reject 10 labels & & & be contented & be discontented CS DBLP : conf / chiir / RadlinskiC17 & rt & & rr & 10 labels & & & rp & & & & rnp & & & & & rc SCS & Initial information request & SERP information request & Confirms & 13 labels & Intent clarification & & & & Query repeat & & & & Query embellishment & & & ODE & set(keywords ) & question(data ) & confirm ( ) & reject ( ) 17 labels & & more ( ) & success ( ) & & & prompt(link ) & & & & verify ( ) & & DSTC1 & inform & restart & affirm & negate 32 labels & nextbus & repeat & & & prevbus & tellchoices & & & & goback & & DSTC2 & inform & & ack & negate 22 labels & request & & affirm & & confirm & & thankyou & & deny & & & & repeat & & & & reqalts & & & tabularSchema alignments ( Part 1 ) .
GMM,gaussian mixture model,TR-22381,"Given the LTI system eq : LTI driven by system and sensor noises whose probability density functions can be expressed as the Gaussian mixture models in eq : noiseGMM_eta and eq : noiseGMM_v , the probability density function of the residual at time can be written as a Gaussian mixture model of Gaussian modeswhere represents the mixture probabilities of the Gaussian modes , are the means , and are the covariances , [ ] [ ] [ ] [ ] [ ] where captures all the possible permutations of combining terms from the system and measurement noise GMM modes , following the rules:[leftmargin=1.3 cm ] [ Init : ] for [ Add : ] [ Wrap : ] if [ ] then and ."
MER,maximum entropy regularizer,TR-22613,"tabularcccccccccUnit ( ) & Accuracy & A10 & F10 & I10 & SDF10 & SDI10 & & Baseline & 62.640.26 & 64.400.29 & 0.210.32 & 25.010.33 & 10.170.36 & 18.800.31 & 6.490.10 & 8.420.11 w/o MER , DOS & 68.320.09 & 67.160.42 & 1.590.37 & 16.030.28 & 5.960.27 & 20.250.46 & 3.720.11 & 9.690.13 w/o DOS & 70.070.19 & 70.420.07 & 0.310.16 & 12.780.18 & 5.510.18 & 16.610.55 & 3.170.05 & 8.550.13 w/o MER & 69.650.18 & 70.410.21 & -0.140.35 & 13.290.15 & 5.380.05 & 16.710.46 & 3.560.08 & 8.190.12 ( Full Model ) & 72.510.17 & 73.350.35 & 0.130.16 & 9.680.44 & 5.050.10 & 13.140.28 & 2.960.06 & 6.550.26 tabularAblation Study ."
FEC,forward error correction,TR-22621,The parameters are defined as follows : FEC - based : accounts for mechanisms that employ FEC ; ARQ - based : mark mechanisms that use ARQ ; QoE - sensitive data : this parameter demonstrates mechanisms that identity and/or considerate the video content to define the EC policy ; Video - aware : check mark is given to mechanisms that use any video characteristics to define the amount of redundancy and/or retransmission ; High - quality video : it is marked if the mechanisms are using videos equal or higher than 720p ( HD ready ) ; Network status : this parameter defines if the mechanisms use the information about the network healthy to define the redundant data ; UEP - enabled : means that different amounts of redundancy are being added to distinct portions of the video .
SCM,semantic correlation maximization,TR-22788,"& CMFH & 0.5644 & 0.5648 & 0.5666 & 0.3384 & 0.3407 & 0.3452 2 - 8 & LSSH & 0.5950 & 0.6043 & 0.6117 & 0.4616 & 0.4606 & 0.4800 2 - 8 & DCH & 0.5991 & 0.6025 & 0.6009 & 0.4275 & 0.4402 & 0.4526 2 - 8 & SCM & 0.5821 & 0.5726 & 0.5672 & 0.3858 & 0.3744 & 0.3666 2 - 8 & SePH & 0.6534 & 0.6665 & 0.7277 & 0.6056 & 0.6223 & 0.6674 2 - 8 & DCMH & 0.7050 & 0.7194 & 0.7366 & 0.6195 & 0.6472 & 0.6721 2 - 8 & FDCH & 0.7762 & 0.7877 & 0.7897 & 0.6861 & 0.7003 & 0.7139 3 * & CMFH & 0.5632 & 0.5631 & 0.5639 & 0.3377 & 0.3401 & 0.3424 2 - 8 & LSSH & 0.5893 & 0.5988 & 0.5950 & 0.4481 & 0.4464 & 0.4568 2 - 8 & DCH & 0.5804 & 0.5981 & 0.5968 & 0.4003 & 0.4179 & 0.4428 2 - 8 & SCM & 0.5976 & 0.5826 & 0.5724 & 0.3858 & 0.3708 & 0.3576 2 - 8 & SePH & 0.6135 & 0.6238 & 0.6768 & 0.5876 & 0.5747 & 0.6314 2 - 8 & DCMH & 0.7038 & 0.7048 & 0.7291 & 0.6147 & 0.6432 & 0.6651 2 - 8 & FDCH & 0.7504 & 0.7555 & 0.7552 & 0.6518 & 0.6712 & 0.6834 Experimental DatasetsMIRFLICKR-25 K consists of 25,000 instances collected from Flickr website ."
ECC,error correcting code,TR-22865,tabularlll cccccccc & & & 8cThroughput ( speedup against CUB 's ) 4 - 11 & & & 8cNumber of bits in each key 4 - 11 & & Method & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 6*turn90K40c ( ECC on)turn & 3*turn0key - onlyturn & WMS & 14.08 ( 1.12 x ) & 13.88 ( 1.12 x ) & 12.17 ( 0.99 x ) & 10.12 ( 0.94 x ) & 7.67 ( 1.05 x ) & - & - & - & & BMS & 13.84 ( 1.10 x ) & 12.94 ( 1.04 x ) & 12.10 ( 0.99 x ) & 10.88 ( 1.01 x ) & 9.45 ( 1.29 x ) & 6.89 ( 1.18 x ) & 4.55 ( 0.79 x ) & 2.69 ( 0.50 x ) & & CUB & 12.56 ( 1x ) & 12.45 ( 1x ) & 12.28 ( 1x ) & 10.75 ( 1x ) & 7.33 ( 1x ) & 5.83 ( 1x ) & 5.74 ( 1x ) & 5.42 ( 1x ) 2 - 11 & 3*turn0key - valueturn & WMS & 8.62 ( 1.13 x ) & 7.94 ( 1.04 x ) & 6.58 ( 0.96 x ) & 5.37 ( 0.94 x ) & 4.56 ( 0.95 x ) & - & - & - & & BMS & 7.79 ( 1.02 x ) & 7.85 ( 1.03 x ) & 7.55 ( 1.10 x ) & 7.16 ( 1.25 x ) & 6.76 ( 1.41 x ) & 5.23 ( 1.60 x ) & 3.41 ( 1.09 x ) & 1.92 ( 0.67 x ) & & CUB & 7.61 ( 1x ) & 7.60 ( 1x ) & 6.87 ( 1x ) & 5.72 ( 1x ) & 4.79 ( 1x ) & 3.27 ( 1x ) & 3.12 ( 1x ) & 2.86 ( 1x ) 6*turn90K40c ( ECC off)turn & 3*turn0key - onlyturn & WMS & 17.57 ( 1.35 x ) & 16.47 ( 1.26 x ) & 13.36 ( 1.04 x ) & 10.18 ( 0.80 x ) & 7.80 ( 0.68 x ) & - & - & - & & BMS & 15.26 ( 1.17 x ) & 13.89 ( 1.06 x ) & 12.76 ( 0.99 x ) & 10.91 ( 0.85 x ) & 9.49 ( 0.82 x ) & 6.85 ( 1.06 x ) & 4.53 ( 0.70 x ) & 2.68 ( 0.42 x ) & & CUB & 13.05 ( 1x ) & 13.06 ( 1x ) & 12.86 ( 1x ) & 12.76 ( 1x ) & 11.54 ( 1x ) & 6.50 ( 1x ) & 6.46 ( 1x ) & 6.44 ( 1x ) 2 - 11 & 3*turn0key - valueturn & WMS & 10.31 ( 1.13 x ) & 9.67 ( 1.07 x ) & 7.73 ( 0.86 x ) & 6.21 ( 0.70 x ) & 4.53 ( 0.59 x ) & - & - & - & & BMS & 9.01 ( 0.99 x ) & 9.00 ( 1.00 x ) & 8.50 ( 0.94 x ) & 7.69 ( 0.86 x ) & 6.78 ( 0.88 x ) & 5.20 ( 1.16 x ) & 3.31 ( 0.74 x ) & 1.92 ( 0.43 x ) & & CUB & 9.14 ( 1x ) & 9.00 ( 1x ) & 9.00 ( 1x ) & 8.92 ( 1x ) & 7.69 ( 1x ) & 4.50 ( 1x ) & 4.46 ( 1x ) & 4.47 ( 1x ) 6*turn90GTX 1080 turn & 3*turn0key - onlyturn & WMS & 18.87 ( 1.15 x ) & 17.70 ( 1.07 x ) & 16.76 ( 1.03 x ) & 16.33 ( 0.96 x ) & 12.56 ( 0.71 x ) & - & - & - & & BMS & 19.37 ( 1.18 x ) & 19.15 ( 1.15 x ) & 18.99 ( 1.17 x ) & 18.61 ( 1.09 x ) & 17.89 ( 1.02 x ) & 16.83 ( 0.93 x ) & 13.41 ( 0.92 x ) & 8.17 ( 0.94 x ) & & CUB & 16.35 ( 1x ) & 16.59 ( 1x ) & 16.24 ( 1x ) & 17.05 ( 1x ) & 17.62 ( 1x ) & 18.05 ( 1x ) & 14.50 ( 1x ) & 8.65 ( 1x ) 2 - 11 & 3*turn0key - valueturn & WMS & 11.39 ( 1.01 x ) & 11.04 ( 1.00 x ) & 10.73 ( 0.96 x ) & 10.05 ( 0.90 x ) & 8.29 ( 0.76 x ) & - & - & - & & BMS & 11.68 ( 1.03 x ) & 11.61 ( 1.05 x ) & 11.58 ( 1.04 x ) & 11.41 ( 1.02 x ) & 11.18 ( 1.03 x ) & 10.89 ( 1.07 x ) & 10.20 ( 1.23 x ) & 6.42 ( 1.20 x ) & & CUB & 11.30 ( 1x ) & 11.06 ( 1x ) & 11.18 ( 1x ) & 11.14 ( 1x ) & 10.88 ( 1x ) & 10.20 ( 1x ) & 8.30 ( 1x ) & 5.34 ( 1x ) tabular Multisplit with identity buckets .
RV,random vaccination,TR-22889,"List of SymbolsList of abbreviationslongtablecc Acronym & Descriptions ADN & Activity driven network modelling AV & Acquaintance vaccination APV & Absolute percentage variation CIP & co - location interaction parameters CN & Common neighbours DST network & Dense SPST network DDT network & Dense SPDT network DDT1 & Vaccinating neighbours in DDT network with direct linksDDT2 & Vaccinating neighbours in DDT network with any linksGDT & Generated SPDT network with 364 K nodes GST & Generated SPST network with 364 K nodes IMV & Individual movement based vaccination strategy IMVE & Individual movement based vaccination strategy with exact information IMVT & Individual movement based vaccination strategy with temporal information LST & SPDT network with the same number of links that of DDT network LST & SPST network with the same number of links that of DST network MLE & Maximum likelihood estimator OSN & Online social network PFU & Plaque - forming unit RV & Random vaccination DV & Degree vaccination RSE & Root squared error SPST & Same place same time transmission SPDT & Same place different time transmission SIR & Susceptible - infected - recovered SDT network & Sparse SPDT network of links having direct and indirect components SST network & Sparse SPST network of links having indirect component onlySPDT graph & graph based on SPDT diffusion longtableList of symbolslongtablecc Symbol & Descriptions A & Set of active copies of nodes in SPDT graph b & active particle decay rates from an area of interaction & activity potential of node & active periods of a node C & Particle concentration in interaction area C , C , C & scenario 1 , 2 and 3 d & Activation degree - number of SPDT links created during an activation E & Intake dose or exposure of infectious particles & Average volume fraction of room air introduced by exhaled breath f & distribution function F & Disease spreading force in the network at the current day of simulation & Average disease spreading force in the network & Graph & Dynamic graph g & Particle generation rate by an infected individual h & Activation frequency I & Number of infected individuals in the system & Number of infected individuals at a simulation day & Number of infected individuals in the system at the current time that disease prevalence & Number of infected individuals up to a simulation day L & links set N & Total number of individuals , nodes , users p & Pulmonary rate of susceptible individual & Infection probability for an intake dose & Probability of creating a link during an activation & Probability of breaking a created link Q & air exchange rate from an area q & Transition probability for changing inactive to active state R & disease reproduction ability of an infected individual r & Particle removal rate from interaction area r & Median of particles removal rates S & Number of susceptible individual T & Simulation period or disease observation period & Activation period or period host user or node stays at the interacted location & Link creation delay or delay neighbour user or node arrives at the interacted location & Stay duration of user or node stays at the interacted location V & Air volume of interaction area , , & waiting periods of a node Y & Labelling sets in graph X & updates z & Number of time step Z & set of nodes in the SPDT graph & power law exponent & Infection rate at the current day of simulation in the network for an infected individual & Indirect transmission period & central tendency & Scaling parameter of activation degree distribution & neighbour proportion & average volume fraction of room air that is exhaled by an susceptible individual & links presence function & state probability & nodes presence function & switching probability form active to inactive states & Infectiousness of infection particles & Fraction of dose or exposure reaches to the target infection site & Duration that virus is generated or infectious period of infected individual & inter - event time for node in activity driven networks & lower limit of active degree distribution & activation potential in ADN networks & particle accumulation rate & Transition probability for changing active to inactive state longtable"
CT,computed tomography,TR-22995,"The total loss can be formulated asequation5eq : hybridlossalignedTP_p(c ) = & _ n=1^Np_n(c)g_n(c ) FN_p(c ) = & _ n=1^N(1-p_n(c))g_n(c ) FP_p(c ) = & _ n=1^Np_n(c)(1-g_n(c ) ) L = & L_Dice + L_Focal = & C - _ c=0^C-1TP_p(c)TP_p(c ) + FN_p(c ) + FP_p(c ) & - 1N_c=0^C-1_n=1^Ng_n(c)(1-p_n(c))^2(p_n(c ) ) , alignedequationwhere , and are the true positives , false negatives and false positives for class calculated by prediction probabilities respectively , is the predicted probability for voxel being class , is the ground truth for voxel being class , is the total number of anatomies plus one ( background ) , is the trade - off between dice loss and focal loss , and are the trade - offs of penalties for false negatives and false positives which are set as 0.5 here , is the total number of voxels in the CT images ."
CST,corticospinal tract,TR-23051,"Along - fiber GFA visualization and cosine similarity between pairs of fibers from three prominent bundles : a ) CST ( R ) , b ) CC , c ) IFOF ( R ) , using framework of varifolds ( Var ) and functional varifolds ( fVar ) ( top left ) , and Comparing variation of cosine similarity for the select fiber pairs over kernel bandwidth parameters and for the framework of functional varifolds ( top right : CST ( R ) , middle left : CC , middle right : IFOF ( R ) ) ; Impact of on clustering consistency ( measured using Average Silhouette ) for for functional Varifolds vs Varifolds ( bottom left ) , and functional Varifolds vs GFA only ( bottom right ) Figure ( top left ) shows GFA color - coded fiber pairs ."
SO,smart object,TR-23096,"table[H ] tabularccc Scenario & Simple & Complex & queries & queries Consumer SO 2 & 0.18 ms & 0.44 ms tabularVarying query complexity - Consumer SO tab : lev2tab3tabletable * tabularccccccccccc Queries & Q1 & Q2 & Q3 & Q4 & Q5 & Q6 & Q7 & Q8 & Q9 & Q10 Query selectivity & 3,5 & 1,84 & 0,85 & 0,72 & 0,55 & 0,37 & 0,28 & 0,2 & 0,15 & 0,13 Extra bits per output tuple & 98 & 140 & 210 & 245 & 294 & 336 & 392 & 455 & 483 & 546 Bandwidth overload per hour & 3440 & 2576 & 1785 & 1764 & 1617 & 1243 & 1097 & 910 & 724 & 709 tabularVarying query complexity - SO network - bandwidth overhead tab : bandwidthtable*Smart Object network : We simulate a smart object network via a Streambase query , where each single Streambase operator acts as a smart object ."
SVM,support vector machine,TR-23202,"However , the results show : ( i ) the size of the corpus has a very positive impact on the accuracy for the content - based deep learning approaches and those models perform best in the larger datasets , since the results were statistical significant over the baseline for the multibotwoz dataset ( ) ; and ( ii ) if the dialogue dataset is small and topic - oriented ( but with few topics ) , which is the case of finch dataset , it is sufficient to use an agent - only MLE or SVM models , although slightly higher accuracies can be achieved with the use of the content of the utterances with a CNN model ( ) ."
PC,principal component,TR-23228,"@X rr rr @ & 2 cCNS & 2 cMDC ( lr)2 - 3(lr)4 - 5 & PC 0 & PC 1 & PC 0 & PC 1 Social circle size , & 0.41 & 0.18 & -0.36 & 0.04 Activity space size , & 0.42 & -0.23 & -0.40 & -0.05 New ties / week , & 0.33 & 0.27 & -0.24 & -0.35 New locations / week , & 0.39 & -0.11 & -0.37 & -0.22 Social circle entropy , & 0.29 & 0.33 & -0.36 & -0.23 Activity space entropy , & 0.38 & -0.10 & -0.35 & 0.13 Social circle stability , & -0.12 & -0.50 & -0.11 & 0.56 Activity space stability , & -0.06 & -0.50 & -0.03 & 0.62 Social circle rank turnover , & -0.17 & 0.26 & 0.28 & -0.19 Activity space rank turnover , & -0.35 & 0.37 & 0.42 & -0.18 T=30 , Principal Components ."
PSD,power spectral density,TR-23275,"Client & 233 & 37 & 270 & 86.3 & & Imposter & 37 & 3743 & 3780&99.0 & & Imposter & 90 & 1260 & 1350 & 93.3 2 - 7 & 3*LDA & Client & 200 & 70 & 270 & 74.1 & & Imposter & 87 & 3693 & 3780&97.7 & & Imposter & 152 & 1198 & 1350 & 88.7 2 - 7 & 3*SVM & Client & 241 & 29&270&89.3 & & Imposter & 11 & 3769 & 3780 & 99.7 & & Imposter & 50 & 1300 & 1350 & 96.3 7c Always the same result regardless of the data tabulartableClient - Imposter verification results per subject sec : results5Table table : Result_subjects ( middle columns ) summarises the subject- and day - wise validation results with PSD and AR features from [ 60]s segments in Setup - R , which corresponds to Table table : Result_segment_size ( upper - panel ) for [ 60]s ."
AM,arithmetic mean,TR-23284,& & & & & 0.2 & 0.812 0.033 & 0.8195 0.03 & 0.782 0.034 & 0.8136 0.028 & 0.78 0.041 & 0.816 0.024 0.3 & 0.831 0.0270 & 0.839 0.0281 & 0.828 0.032 & 0.830 0.036 & 0.829 0.031 & 0.840 0.034 0.4 & 0.842 0.021 & 0.847 0.018 & 0.849 0.016 & 0.821 0.017 & 0.827 0.020 & 0.827 0.019 0.5 & 0.850 0.0170 & 0.848 0.016 & 0.848 0.014 & 0.831 0.019 & 0.837 0.0186 & 0.837 0.018 0.6 & 0.841 0.023 & 0.850 0.024 & 0.849 0.019 & 0.846 0.023 & 0.840 0.026 & 0.842 0.023 0.7 & 0.859 0.025 & 0.862 0.019 & 0.867 0.018 & 0.838 0.018 & 0.843 0.031 & 0.844 0.027 0.8 & 0.845 0.031 & 0.846 0.032 & 0.850 0.029 & 0.844 0.024 & 0.841 0.030 & 0.843 0.024 Averaged AM over 10 trials of the linear classifiers obtained from based regularized ERM on two synthetic datasets .
AE,absolute error,TR-23331,"C > p6.2emg>[RGB]195 , 195 , 195Ck > GrayC*[htbp ] The SE ( ) , AE ( ) and HD ( ) calculated using the results of different methods ( Dufour 's method , OCTRMIA3D and GDM ) and the ground truth manual segmentation , for the IS - OS ( ) surface in each of the 10 OCT volumesC > p6.2emg>[RGB]195 , 195 , 195Ck > GrayC*[htbp ] The OSE ( ) , OAE ( ) and OHD ( ) calculated using the results of different methods ( Dufour 's method , OCTRMIA3D and GDM ) and the ground truth manual segmentation , for overall retina surfaces in each of the 10 OCT volumes[h ! ]"
RD,reciprocal degree,TR-23423,"table*[!h ] tableThe comparison of Spearman Correlations to the KORE gold standard of different methods from each dataset over time - varying graphs and aggregated graphs with redirects tab : corr - all threeparttable tabularllllllllll & Jaccard ( I+O ) & tabular[c]@l@ Extended Jaccard RP ( I+O ) tabular & tabular[c]@l@ Extended Jaccard RD ( I+O ) tabular 2007 & 0.522472 & 0.530393 & 0.540804 2008 & 0.532665 & 0.529899 & 0.580926 2009 & 0.580944 & 0.613450 & 0.651450 2010 & 0.586593 & 0.637930 & 0.696506 2011 & 0.561152 & 0.619950 & 0.669867 2012 & 0.537278 & 0.582106 & 0.634255 2013 & 0.543986 & 0.589968 & 0.640649 2014 & 0.483211 & 0.530217 & 0.589831 2015 & 0.506170 & 0.529598 & 0.579989 2016 & 0.494577 & 0.520003 & 0.544250 Intersection & 0.470378 & 0.465568 & 0.487342 Union & 0.531830 & 0.652417 & 0.708271 tabular tablenotes[para , flushleft ] , , , and mean the Union model using the Extended Jaccard with Reciprocal Degree Centrality considering both in - links and out - links is significantly better than this result with p - value 0.1 , p - value 0.05 , p - value 0.01 and p - value 0.001 respectively ."
OP,old persian,TR-23538,"[ 196ff.]Lipp2009 states that OP -st- ( found as a reflex of PIE * -k - t- , * -g - t- ) is due to analogy , while other developments are due to a phonological change predating Middle Persian:[noitemsep]PIE * hreg - to- PIr * rasta- OP rasta- ' right ' MP rast NP rastPIr * musti- ' fist ' MP must , must NP mustPIr * -ista- ( superlative suffix ) MP -ist ; e.g. , Phl balist , MMP barist ' highest ' * barjista- ; Phl xwalist , MMP xwarist ' sweetest ' * huarjista- ( cf ."
RL,reinforcement learning,TR-23604,"figure[ht]centertikzpicture[scale=0.74 , every node/.style = scale=0.74]hidden = [ rectangle , minimum width=0.8 cm , minimum height=0.8 cm , text centered , draw = black]state = [ circle , radius=0.8 , text centered , draw = black]arrow = [ ->,>=stealth][dashed ] ( -2.5 , 0 ) - ( -2.5 , -2.1 ) ; [ dashed ] ( -2.5 , 0 ) - ( -1.5 , 0 ) ; at ( -0.9 , 0)Agent ; [ dashed ] ( -0.2 , 0 ) - ( 17.8 , 0 ) ; [ dashed ] ( -2.5 , -2.1 ) - ( 17.8 , -2.1 ) ; [ dashed ] ( 17.8 , 0 ) - ( 17.8 , -2.1 ) ; [ dashed ] ( -2.5 , -2.3 ) - ( -2.5 , -7.2 ) ; [ dashed ] ( -2.5 , -2.3 ) - ( 17.8 , -2.3 ) ; [ dashed ] ( -2.5 , -7.2 ) - ( -1.5 , -7.2 ) ; at ( -0.2 , -7.2 ) MDP ; [ dashed ] ( 1.2 , -7.2 ) - ( 17.8 , -7.2 ) ; [ dashed ] ( 17.8 , -2.3 ) - ( 17.8 , -7.2 ) ; ( h0 ) [ hidden ] at ( 0 , -1 ) ; ( h1 ) [ hidden , right of = h0 , xshift=1.8 cm ] ; ( h2 ) [ hidden , right of = h1 , xshift=1.8 cm ] ; ( h3 ) [ hidden , right of = h2 , xshift=1.8 cm ] ; ( h4 ) [ hidden , right of = h3 , xshift=1.8 cm ] ; ( h5 ) [ hidden , right of = h4 , xshift=1.8 cm ] ; ( h6 ) [ hidden , right of = h5 , xshift=1.8 cm ] ; ( x1 ) [ state , below of = h0 , xshift=1.3 cm , yshift=-3 cm ] ; ( x0 ) [ state , left of = x1 , xshift=-1.8 cm ] ; ( x2 ) [ state , right of = x1 , xshift=1.8 cm ] ; ( x3 ) [ state , right of = x2 , xshift=1.8 cm ] ; ( x4 ) [ state , right of = x3 , xshift=1.8cm];(x5 ) [ state , right of = x4 , xshift=1.8 cm ] ; ( x6 ) [ state , right of = x5 , xshift=1.8cm];[arrow ] ( h0 ) - ( h1);[arrow ] ( h1 ) - ( h2);[arrow ] ( h2 ) - ( h3);[arrow ] ( h3 ) - ( h4);[arrow ] ( h4 ) - ( h5);[arrow ] ( h5 ) - ( h6);[arrow ] ( x0 ) - ( h0 ) node[near start , below , sloped , xshift=0.4 cm ] , , ; [ arrow ] ( x1 ) - ( h1 ) node[near start , below , sloped , xshift=0.4 cm ] , , ; [ arrow ] ( x2 ) - ( h2 ) node[near start , below , sloped , xshift=0.4 cm ] , , ; [ arrow ] ( x3 ) - ( h3 ) node[near start , below , sloped , xshift=0.4 cm ] , , ; [ arrow ] ( x4 ) - ( h4 ) node[near start , below , sloped , xshift=0.4 cm ] , , ; [ arrow ] ( x5 ) - ( h5 ) node[near start , below , sloped , xshift=0.4 cm ] , , ; [ arrow ] ( x6 ) - ( h6 ) node[near start , below , sloped , xshift=0.4 cm ] , , ; [ arrow ] ( h0 ) - ( x1 ) node[near start , right , yshift=0.4 cm , xshift=-0.1 cm ] ; [ arrow ] ( h1 ) - ( x2 ) node[near start , right , yshift=0.4 cm , xshift=-0.1 cm ] ; [ arrow ] ( h2 ) - ( x3 ) node[near start , right , yshift=0.4 cm , xshift=-0.1 cm ] ; [ arrow ] ( h3 ) - ( x4 ) node[near start , right , yshift=0.4 cm , xshift=-0.1 cm ] ; [ arrow ] ( h4 ) - ( x5 ) node[near start , right , yshift=0.4 cm , xshift=-0.1 cm ] ; [ arrow ] ( h5 ) - ( x6 ) node[near start , right , yshift=0.4 cm , xshift=-0.1 cm ] ; [ arrow ] ( x0 ) - ( x1);[arrow ] ( x1 ) - ( x2);[arrow ] ( x2 ) - ( x3);[arrow ] ( x3 ) - ( x4);[arrow ] ( x4 ) - ( x5);[arrow ] ( x5 ) - ( x6);[decorate , decoration = brace , amplitude=5pt , mirror , raise=4pt , xshift=-4pt , yshift=-5pt](-2,-5.4 ) - ( 7.7,-5.4 ) node [ black , midway , yshift=-0.8cm]Episode 1;[decorate , decoration = brace , amplitude=5pt , mirror , raise=4pt , xshift=-4pt , yshift=-5pt](9.3,-5.4 ) - ( 16.1,-5.4 ) node [ black , midway , yshift=-0.8cm]Episode 2;tikzpictureA graphic representation , inspired by the RL illustration RL2 , of our deep meta - reinforcement learning framework ."
CNL,controlled natural language,TR-23678,"tabular@l@ l@ p10.9cm@class & properties & languages 1551 & c t w i & IBM 's EasyEnglish & c w s g & Special English & c w a & E - Prime & c w g & Plain Language 2132 & c s d g & CAA Phraseology , FAA Phraseology , ICAO Phraseology , PoliceSpeak , SEASPEAK 2133 & c w d i & Airbus Warning Language 2541 & f w a & AIDA 2551 & c t w d a i & ALCOGRAM , COGRAM & c t w d a & CLCM & c t w d i & ASD - STE , Avaya CE , Bull GE , CTE , CASL , CE at Douglas , DCE , General Motors GE , PACE , Sun Proof & c t w d & Wycliffe Associates ' EasyEnglish & c t w i & iCE , SMART Controlled English & c w d i & AECMA - SE , CFE , CASE , CE at Clark , CE at IBM , CE at Rockwell , EE , HELP , ILSAM , KISL , NCR FE & c w d g & Massachusetts Legislative Drafting Language & c w i & Boeing Technical English , NSE , SMART Plain English & c w & Basic English & t w d i & MCE , Oce Controlled English & t w a & KCE & t w i & CLOUT 3142 & c f w d i & SLANG & f s d i & Voice Actions 3243 & f w d a & RNLS 3333 & f w a & ClearTalk & f w i & ITA CE 3342 & f w i & CPL 3442 & c f w i & RuleSpeak , SBVR - SE 4143 & f w d a & Drafter Language , MILE Query Language 4144 & f w a & Quelo Controlled English 4153 & t f d a & PILLS Language 4243 & f w d a & Atomate Language & f w a i & Gellish English & f w a & GINO 's Guided English & f w i & CELT 4343 & f w d a & PROSPER CE & f w a & ACE 4353 & f w d a & ICONOCLAST Language 5143 & f w d a & CLEF Query Language & f w a & Ginseng 's Guided English 5144 & f w d a & Coral 's Controlled English & f w a & PathOnt CNL 5145 & f w a & Sowa 's syllogisms 5234 & f w d a i & TBNLS & f w a & OWLPath 's Guided English , SQUALL 5243 & f w a & CPE , CLIP , OWL ACE , SOS 5244 & f w d a & BioQuery - CNL , PERMIS CNL , ucsCNL & f w a & CLOnE , DL - English , E2V , Lite Natural Language , OSE & f w g & Rabbit 5333 & f w d a & CLM , ForTheL , Naproche CNL & f w a & CLCE , PNL 5343 & f w d a & Gherkin & f w a g & RECON & f w a & First Order English , PENG , PENG - D , PENG Light & f w i & iLastic Controlled English 5433 & f w a & FEtabular"
EO,eyes open,TR-23776,"& Task&Segment & Feature Extraction&Classifier&Performance 4*Biased & 2*Abdullah2010 & 2 * 10 & 2 * 5 & 2*[2]weeks & 2 * 4 & EC & 2*[5]s & 2*AR & 2*ANN&CRR = [ 97.0 ] & & & & & & EO & & & & CRR = [ 96.0]2 - 11 & Riera2008&51&4 & 34 [ 74]days & 2 & EC & [ 4]s & AR , PSD , MuI , COH , CC & FDA & EER = [ 3.4 ] 2 - 11 & Su2010&40&2 & - & 1 & EC & [ 180]s & AR , PSD & KNN , LDA & CRR = [ 97.5 ] 8*Rigorous&Marcel2007&9&3&[3]days & 8 & MI & [ 1]s & PSD & MAP model & HTER = [ 19.3 ] 2 - 11 & Lee2013&4&2 & [ 10]days - [ 5]months & 1 & EC & [ 50]s & PSD & LDA & AC = [ 100 ] 2 - 11 & 2*Rocca2013 & 2 * 9 & 2 * 2 & 2 * 1 - [ 3]weeks & 3 & 2*EC & 2*[1]s & 2*AR & 2*Linear classier & CRR = [ 100 ] & & & & & 5 & & & & & CRR = [ 100 ] 2 - 11 & 2*Armstrong2015 & 15 & 2 & 5 - [ 40]days & 2 * 1 & 2*ERP & 2*[1.1]s & 2*Time - series & 2*CC & CRR = [ 89.0 ] & & 9 & 3 & 134 - [ 188]days & & & & & & CRR = [ 93.0 ] 2 - 11 & 2*Maiorana2016a & 2 * 50 & 2 * 3 & 2*Ave . ["
CT,computed tomography,TR-23813,figure figure [ ht ] center minipage0.15 ./fig / snapshot0109new2.png minipage minipage0.15 ./fig / snapshot0129new2.png minipage minipage0.15 ./fig / snapshot0139new2.png minipage minipage0.15 ./fig / snapshot0149new2.png minipage minipage0.15 ./fig / snapshot0110new3.png minipage minipage0.15 ./fig / snapshot0130new3.png minipage minipage0.15 ./fig / snapshot0140new3.png minipage minipage0.15 ./fig / snapshot0150new3.png minipage minipage0.15 ./fig / snapshot0106new3.png minipage minipage0.15 ./fig / snapshot0126new3.png minipage minipage0.15 ./fig / snapshot0136new3.png minipage minipage0.15 ./fig / snapshot0146new3.png minipage minipage0.15 ./fig / snapshot0103new3.png minipage minipage0.15 ./fig / snapshot0123new3.png minipage minipage0.15 ./fig / snapshot0133new3.png minipage minipage0.15 ./fig / snapshot0143new3.png minipage center 5fig : vis2_2 Visualizations for five anatomies on the first four holdout CT images .
TD,temporal dimension,TR-24042,"[ ( Y ) ] Yes[(N ) ] NoCoding Field 4 : Dimensions of Trade - Off[(cost ) ] Cost ( Often in monetary terms)[(func ) ] Functionality[(mait ) ] Software Maintainability[(qual ) ] Software Quality[(risk ) ] Risk[(time ) ] Time ( Includes scheduling , delivery and release)[(value ) ] Value ( As in terms of monetary value , "" business value "" or in some cases , in terms of benefit)[(other ) ] See Appendix Overlap between main query and TD papers Overview of search results for main searchOverview of search results for Technical Debt ancillary searchAnalysis of Extracted DataThe secondary researchers extracted data and analyzed the results included below ."
GCN,graph convolution networks,TR-24054,"table*[h ] threeparttable tabularlS[round - mode = places , round - precision=2]@S@S[round - mode = places , round - precision=2]@S@S[round - mode = places , round - precision=2]@S@S[round - mode = places , round - precision=2]@S@S[round - mode = places , round - precision=2]@S@S[round - mode = places , round - precision=2]@S@ 2*Method & 2cTechnology & 2cCulture / Recreation & 2cLife / Arts & 2cScience & 2cProfessional / Business & Acc ( ) & MRR&Acc ( ) & MRR&Acc ( ) & MRR&Acc ( ) & MRR&Acc ( ) & MRR RF BurelMA16 , TianZL13 & 66.78 0.023 & 0.6834 0.043 & 72.5 0.018 & 0.626 0.050 & 72.71 0.049 & 0.6283 0.089 & 68.09 0.024 & 0.6923 0.049 & 74.72 0.044 & 0.5951 0.081 FF JendersKN16 & 67.31 0.027 & 0.7860 0.022 & 72.22134095 0.020 & 0.782138231 0.023 * & 73.57771352 0.049 & 0.7801657493 0.034 & 67.86881023 0.024 & 0.7997881343 0.028 & 74.63275816 0.040 & 0.7595471154 0.049 DGCN DualGCN & 70.70 0.022 & 0.7819 0.017 & 75.22 0.017 & 0.7715263633 0.028 & 76.73 0.034 & 0.7840080442 0.038 & 71.45 0.023 * & 0.7915379815 0.035 & 76.86 0.031 & 0.7511047996 0.046 RGCN relationalGCN & 54.40 0.045 & 0.6726 0.045 & 60.39 0.016 & 0.6455423723 0.042 & 59.97 0.043 & 0.654424422 0.054 & 58.65 0.054 & 0.6825181603 0.042 & 63.02 0.038 & 0.6568131822 0.061 ( lr)1 - 1(lr)2 - 11 AS - GCN & 67.76 0.032 & 0.7746 0.015 & 73.05 0.021 & 0.7630910769 0.025 & 73.79 0.048 & 0.7765385098 0.042 & 66.93 0.045 & 0.7876480266 0.028 & 74.99 0.045 & 0.7424021656 0.047 TS - GCN & 66.87 0.032 & 0.7786 0.018 & 72.16 0.023 & 0.7642533243 0.023 & 72.02 0.061 & 0.7655344376 0.048 & 65.90 0.042 & 0.7896596329 0.031 & 74.17 0.046 & 0.746963993 0.044 C - GCN & 71.64 0.022 * & 0.7902 0.015 * & 76.18 0.017 * & 0.781 0.024 & 77.37 0.034 * & 0.7883908504 0.040 * & 70.81 0.042 & 0.8002268874 0.032 * & 77.57 0.038 * & 0.7679999323 0.034 * IR - GCN & 73.96 0.023 & 0.7939 0.014 & 78.61 0.018 & 0.7905241843 0.025 & 79.21 0.032 & 0.7995996677 0.037 & 74.98 0.021 & 0.8085288947 0.028 & 80.17 0.026 & 0.7849464934 0.032 tabular tablenotes [ * ] DGCN stands for DualGCN , RGCN stands for RelationalGCN , and IR - GCN stands for Induced Relational GCN ."
PCA,principal component analysis,TR-24076,"frische & aufrechen & liebe & brummen veilchen & alsbald & freundschaft & krähen niedersinken & billigkeit & lust & rasseln duftig & erzeigen & treue & rum jenseits & unterstehen & trieb & bock zauber & betragen & seligkeit & dum entgleiten & stracks & hoffnung & prasseln künden & zuerkennen & glaube & trommel hoffend & hierin & keusch & säbel efeu & schmeissen & treu & traben enthüllen & anlaß & erkalten & bellen erfüllung & jederzeit & wahr & block heimat & muhen & immerdar & bügel trübe & schimpfen & regung & gaul gloria & stecken & gegenliebe & grasen tabularsmallTop 15 words per dimension for ' love ' tropes from PCA extremes , plotted in figures 4 , 5 , 6 and 7 ."
PC,principal component,TR-24161,"@X rrr rrr @ & 3 c[c ] PC 0 , , & 3c[c]PC 1 , , ( lr)2 - 4(lr)5 - 7 & coeff & p val & LMG & coeff & p val & LMG extraversion & & & 0.8 & & 0.03 & 0.38 openness & & 0.01 & 0.07 & & 0.5 & 0.08 neuroticism & & 0.7 & 0.07 & & 0.01 & 0.48 agreeableness & & 0.5 & 0.03 & & 1.0 & 0.01 conscientiousness & & 1.0 & 0.03 & & 0.6 & 0.04 T=30 , Extraversion , openness , and neuroticism explain spatial behaviour ."
FEC,forward error correction,TR-24165,"figure[!htb ] center ./loss_distribution_MINT.eps center MINT - FEC 's experiment PLR distribution fig : MINT : lossDistfiguretable[!ht ] MINT - FEC Simulation parameters center tabularll Parameters & Value Display sizes & 1920x1080 , 1280x720 , and 800x600 Frame rate mode & Constant Frame rate & 29.970 fps GoP & 19:2 Video format & H.264 Codec & x264 Container & MP4 Propagation model & FriisPropagationLossModel Mobility model & Gauss - Markov UAV velocity & 45 - 65 km / h ( 28 - 40 mph ) LTE Frequency band & 800MHz LTE Mode & FDD LTE Bandwidth & 5 MHz eNodeB Operating Power & 22 dBm Antenna Gain & 16 dBi tabular tab : MINT : parameters center tableFive different schemes were simulated as follows : ( 1 ) without any FEC mechanism ."
BS,base station,TR-24195,"OAM beam has divergence in its high - intensity region which caused attenuation [ 15 - 17]. By using Fresnel - zone - plate lenses antenna at BS , this issue can be mitigated without affecting the helical phase profile of OAM beam [ 15 - 17].The normalized channel matrix can be expressed as follows [ 16 ] where is the index of the receiving antenna and is the total number of receiving antennas [ 16]. Since one OAM beam is considered here to transmit by OAM , so there are no possibilities of intersymbol interference or intermodal interference among the OAM beams for LOS system [ 16]. Relay TransmissionIn the second time slot , Link 4 is transmitted with total transmit power ."
FEC,forward error correction,TR-24271,"Fn algorithm[!htb ] RuleBlock * block = new RuleBlock ( ) ; block addRule ( new MamdaniRule ( "" if ( SpatialComplexity is HIGH and PacketLossRate is HIGH and FrameType is I ) then RedundancyAmount is HIGH "" , engine ) ) block addRule ( new MamdaniRule ( "" if ( TemporalIntensity is HIGH and PacketLossRate is HIGH and FrameType is I or P ) then RedundancyAmount is HIGH "" , engine ) ) Packet loss x video characteristics rulesalgo : MINT : lossVideoRules algorithmMINT - FEC utilises the same core structure of uavFEC , so once all the fuzzy rules and sets are defined , they are employed in real - time in the fuzzy logic controller ."
ASA,adaptive segmentation algorithm,TR-24592,"[ ] [ ASA][][Deep learning ] [ ] [ ASA][][Deep learning ] [ ] [ ASA][][Deep learning ] Examples from the test set , where a , c , and d are the ASA mask contours overlaid on manual annotation images ( counted neurons have blue marks ) , b , d , and f are the iterative deep learning predicted masks ( iteration 5 ) contours overlaid on manual annotation imageDiscussionThe evidence from this study suggests that the iterative deep learning based unbiased stereology method presented herein is much faster and more accurate than the state - of - the - art stereology since human involvement was mainly reduced ."
OCR,optical character recognition,TR-24598,"For example , in Figure figure:1 the word "" coil "" has been correctly spelled in the OCR text but should have been "" and "" according to the original newspaper article ( real word error ) ; the word "" tnenty "" in the OCR text has a substitution error ( ' n ' should have been ' w ' ) which is actually "" twenty "" according to the original newspaper article ( Non - real word error ) ; the word "" 4anrliteii "" is a combination of alphabets and number and should have been "" confident "" as per the original newspaper article(Non - word error ) ; the word "" ex - ceptionally "" where "" ex "" occurs on one line while "" ceptionally "" in the next and due to no punctuation in the text , they are treated as separate words in OCR text(New Line error ) ; the word "" Thernndldntesnra "" in the OCR text is actually a combination of three words "" The candidates are "" while the words "" v Icrory "" are actually equivalent to a single word "" victory "" when compared with the original news article(Word Split and Join error ) ."
CNN,convolutional neural network,TR-24655,table[H ] tabularcccccccccc & 3c80 & 3c160 & 3c320 & SRP & GMBF & CNNf15 + 11 & SRP & GMBF & CNNf15 + 11 & SRP & GMBF & CNNf15 + 11 2 * 01 & & & & & & & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & & & & & & & 2*Average & & & & & & & & & & & & & & & & & & tabular Results for the SRP - PHAT strategy ( columns SRP ) ; the one invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with sequences 15 and 11 ( columns CNNf15 + 11 ) tab : baselineResults+ft15 + 11table
RL,reinforcement learning,TR-24709,"Although represents one number , algorithm 's behaviour may turn out to be sensitive to its choosing , and must be designed by engineer as some scheduled motion from something near 0 to 1 , and its well - turned selection may require inaccessible knowledge about how many steps it will take for algorithm to < < warm up>>.Multi - step DQNOne more widespread modification of Q - learning in RL community is substituting one - step approximation present in Bellman optimality equation ( ) with -step:(-step Bellman optimality equation)Indeed , definition of consists of average return and can be viewed as making steps from state after selecting action , while vanilla Bellman optimality equation represents as reward from one next step in the environment and estimation of the rest of trajectory reward recursively ."
IP,inductive programming,TR-24765,"& Louis Johnson , PhD & PhD & red Lou & verde PhD & Robert Mills & & red Rob & verde 3rAccuracy : & 0.72 & 1 4 * 20 & 235 - 7654 Taiwan & ( 886 ) 235 - 7654 & & & 17 - 455 - 81 - 39 Spain & ( 34 ) 17 - 455 - 81 - 39 & red ( 886 ) 17 - 455 - 81 - 39 & verde ( 34 ) 17 - 455 - 81 - 39 & 618 - 4390 Panama & ( 507 ) 618 - 4390 & red ( 886 ) 618 - 4390 & verde ( 507 ) 618 - 4390 & 25 - 613 - 24 - 50 Chile & ( 56 ) 25 - 613 - 24 - 50 & red ( 886 ) 25 - 613 - 24 - 50 & verde ( 56 ) 25 - 613 - 24 - 50 3rAccuracy : & 0 & 1 4 * 23 & 23/11/18 425 - 785 - 4210 & 425 - 785 - 4210 & & & 425 - 613 - 2450 000 - 000 & 425 - 613 - 2450 & red 2450 000 - 000 & verde 425 - 613 - 2450 & [ TS]865 - 000 - 0000 - 06 - 23 - 09 & 865 - 000 - 0000 & red 06 - 23 - 2009 & verde 865 - 000 - 0000 & 17:58 - 19:29 , 425 - 743 - 1650 & 425 - 743 - 1650 & verde 425 - 743 - 1650 & verde 425 - 743 - 1650 3rAccuracy : & 0.36 & 1 4 * 25 & 08:55 PM CET & 08:55 & & & 20:15:00 & 20:15:00 & verde 20:15:00 & verde 20:15:00 & 10:05:00 AM & 10:05:00 & verde 10:05:00 & verde 10:05:00 & UTC 21:20 & 21:20 & red UTC 21:20 & verde 21:20 3rAccuracy : & 0.91 & 1 4 * 28 & 01:34:00 5 & 06:34:00 & & & 01:55 5 & 06:55 & verde 06:55 & verde 06:55 & 16:15:12 5 & 21:15:12 & red 06:15:12 & verde 21:15:12 & 21:20 5 & 02:20 & red 06:20 & verde 02:20 3rAccuracy : & 0.10 & 1 4 * 30 & 56.77cl & cl & & & 84Kg & Kg & verde Kg & verde Kg & 39.88 A & A & verde A & verde A & 1 nm & nm & verde nm & verde nm 3rAccuracy : & 1 & 1 4 * 31 & 56.77cl & Volume & & & 84Kg & Mass & red Volume & verde Mass & 39.88 A & Electricity & red Volume & verde Electricity & 1 nm & Length & red Volume & verde Length 3rAccuracy : & 0.10 & 1 tabular Example of results obtained ( using MagicHaskeller as IP core ) compared with FlashFill ."
MD,mean diffusivity,TR-24787,table [ ] tabularlll 3cAmyloid Load ( PiB Positivity ) Set 1 & PiB Angular L / R & PiB Cingulum Ant L / R & PiB Cingulum Post L / R & PiB Frontal Med Orb L / R & PiB Precuneus L / R & PiB Temporal Sup L / R & PiB Temporal Mid L / R & PiB SupraMarginal L Set 2 & FA Cerebral peduncle R & FA Cerebral peduncle L & MD Corticospinal tract R & MD Corticospinal tract L & Trail - Making Test Part A Score & MD Cerebral peduncle R & PET Cingulum Post R & tabular Group difference across Amyloid Load ( PiB Positivity ) tab : wrapPIBtableC ) Graph Scan Statistics on slope differences across amyloid load positivity .
PE,processing element,TR-24939,"hls type= , name = HLS , description = high - level synthesis , first = high - level synthesis ( HLS)hlsg , see=[Glossary:]hlsgrtl type= , name = RTL , description = register - transfer level , first = register - transfer level ( RTL)rtlg , see=[Glossary:]rtlgasic type= , name = ASIC , description = application - specific integrated circuit , first = application - specific integrated circuit ( ASIC)asicg , see=[Glossary:]asicgeda type= , name = EDA , description = electronic design automation , first = electronic design automation ( EDA)edag , see=[Glossary:]edagfpga type= , name = FPGA , description = field - programmable gate array , first = field - programmable gate array ( FPGA)fpgag , plural = FPGAs , see=[Glossary:]fpgaglut type= , name = LUT , description = lookup table , first = lookup table ( LUT)lutg , plural = LUTs , see=[Glossary:]lutgpar type= , name = PAR , description = place and route , first = place and route ( PAR)parg , see=[Glossary:]pargff type= , name = FF , description = flip - flop , first = flip - flop ( FF)ffg , plural = FFs , see=[Glossary:]ffgbram type= , name = BRAM , description = block RAM , first = block RAM ( BRAM)bramg , plural = BRAMs , see=[Glossary:]bramgrom type= , name = ROM , description = Read - only Memory , first = Read - only Memory ( ROM)romg , plural = ROMs , see=[Glossary:]romgpe type= , name = PE , description = Processing Element , first = Processing Element ( PE)peg , plural = PEs , see=[Glossary:]pegfft type= , name = FFT , description = Fast Fourier Transform , first = Fast Fourier Transform ( FFT)fftg , plural = FFTs , see=[Glossary:]fftgdft type= , name = DFT , description = Discrete Fourier Transform , first = Discrete Fourier Transform ( DFT)dftg , plural = DFTs , see=[Glossary:]dftgcrs type= , name = CRS , description = compressed row storage , first = compressed row storage ( CRS)crsg , see=[Glossary:]crsgssa type= , name = SSA , description = static single assigment , first = static single assignment ( SSA)ssag , see=[Glossary:]ssag"
MAP,maximum a posteriori,TR-24954,"equationThen , for each sensor , at each time instant the minimization of the following MH - MAP cost function is addressed eqnarrayeq : cost_step1J_t^i ( X_t - N : t^i ) & = & _ t - N^i - _ t - N^i^2 _ + _ k = t - N^t_k+1^i - A _ k^i ^2_Q&- & _ k = t - N^t [ y_k^i F^i ( ^i- C _ k^i ) + ( 1-y_k^i ) ^i ( ^i- C _ k^i ) ] , & & eqnarraywhere and is the estimate of the local state at time computed at the previous iteration ."
PA,peano 's arithmetics,TR-24990,"* [ ( L / L ) ] z ' z = z ' , ( z ) [ ( _ ) ] n = z ' , B(c , 0 , s ) , B(c , z ' , t ) , u z ' ( u ) [ ( L / L ) ] n = z ' , B(c , 0 , s ) , B(c , z ' , t ) , ( z ' ) , u z ' ( u ) [ ( R / R ) ] ( z')z , m m < n ( s = z A(m , s , z ) ) ^ zx , ty [ ( L ) ] ( z')z ' < n [ ( PA - Ax)](z')z ' < z ' & ( 10,0)(0,7)8 ."
BS,base station,TR-25043,"The received signal at and from the BS by different OAM modes are given belowAccordingly , the received SINR for symbol at can be expressed as [ 27 - 31]where is the singular value of the channel response matrix of CNOMA - SWIPT - PS - OAM system [ 22,27,30]. OAM beam has divergence in its high - intensity region which caused attenuation [ 27 - 30]. By using Fresnel - zone - plate lenses antenna at BS , this issue can be mitigated without affecting the helical phase profile of OAM beam [ 27 - 30]. So , similarly the received SINR for symbol at can be expressed as [ 29 - 31]where similarly as before is the singular value of the channel response matrix of the proposed system [ 22,27,30]. Achievable capacity analysisBy considering normalized total time duration ( ) ."
BR,best response,TR-25115,"( C2 ) If P1 uses CRL1 with a rate of learning faster than P2 who learns with RL2 , then the ODE is given by(Explicit Solutions of Smooth BR Equation ) : Given P2 's trajectory and an initial condition the smooth best response equation in ( ) has a unique solution given by the vectorial functionwhere In particular , if P2 is a slow learner i.e. , constant in time , then the smooth best response equation of P1 converges to which goes to when ( Explicit Solutions of Replicator Equation ) : Given P2 's trajectory and an interior initial condition the replicator equation in ( ) has a unique solution given by the vectorial function , with a normalization factor In particular , if P2 is a slow learner , i.e. , constant in time , then the replicator equation of P1 converges toNote that these solutions are in the interior of the simplex for finite , but the trajectory can be arbitrarily close to the boundary when goes to infinity ."
RNN,random neural networks,TR-25129,"For all the given values , iterate through the RNN recurrent network learning algorithm till convergence , updating at each step the weights and , , so as to minimize the following error function : Dataset DescriptionHere we give a brief description of the four datasets used for evaluation:[leftmargin=*,labelsep=4.9mm]Iris dataset : Each instance is described by four plants attributes ( sepal length and width , and petal length and width ) all are real numbers and the task is to recognize which class of Iris plants ( Iris Setosa , Iris Versicolour , or Iris Virginica ) a given test instance belongs to ."
SD,secure digital,TR-25174,"Across the 31 sensors under study , this consists of 462.1 million rows of telemetry data , across 10 variables:[leftmargin=*,labelsep=5 mm ] CPU load ( /1 min ) : mean CPU usage over a 1 min period across all four 900 MHz CPU cores ; CPU load ( /15 min ) : mean CPU usage over a 15 min period across all four 900 MHz CPU cores CPU temp ( C ) : core CPU temperature in degrees Celsius ; RAM usage ( ): usage of 925 MB RAM ; Wi - Fi signal strength ( ): measure of Wi - Fi signal strength ; Wi - Fi signal quality ( ): measure that factors in signal - to - noise ratio ( SNR ) and signal strength ; Data usage ( ): usage of 12 GB SD card data partition ; TMP usage ( ): usage of 50 MB RAM disk partition used for fast temporary I / O operations ; Var - log usage ( ): usage of 50 MB RAM disk log partition where all log files are written to ; Running processes : count of running processes ."
PR,pagerank,TR-25219,"For the KG embeddings , we report only the approaches that performed best across different test splits.(The full result table is available on - line : https://github.com/vendi12/semantic_coherence/blob/master/results/results.xls)table[!t]adjustboxwidth=.98tabularp2cmlllllllllllll & & 12cAccuracy 3 - 14 & Data & & 10cTNeg & 4 - 13Embeddings & split & TPos & RUf & Avg & VoD & Avg & SqD & Avg & VSp & Avg & HSp & Avg & Avg Word2Vec & RUf & 0.99 & 0.99 & 0.99 & 0.02 & 0.50 & 0.02 & 0.50 & 0.01 & 0.50 & 0.01 & 0.50 & 0.60 & VoD & 0.89 & 0.62 & 0.75 & 0.90 & 0.89 & 0.53 & 0.71 & 0.18 & 0.54 & 0.20 & 0.54 & 0.69 & SqD & 0.75 & 0.65 & 0.70 & 0.88 & 0.81 & 0.81 & 0.78 & 0.27 & 0.51 & 0.29 & 0.52 & 0.66 & VSp & 0.59 & 0.50 & 0.55 & 0.82 & 0.71 & 0.41 & 0.50 & 0.59 & 0.59 & 0.61 & 0.60 & 0.59 & HSp & 0.62 & 0.39 & 0.50 & 0.71 & 0.66 & 0.38 & 0.50 & 0.55 & 0.58 & 0.63 & 0.63 & 0.58 GloVe & RUf & 0.99 & 0.99 & 0.99 & 0.00 & 0.50 & 0.01 & 0.50 & 0.00 & 0.50 & 0.00 & 0.50 & 0.60 & VoD & 0.93 & 0.38 & 0.66 & 0.93 & 0.93 & 0.39 & 0.66 & 0.19 & 0.56 & 0.08 & 0.51 & 0.66 & SqD & 0.76 & 0.71 & 0.73 & 0.91 & 0.84 & 0.82 & 0.79 & 0.16 & 0.46 & 0.15 & 0.45 & 0.66 & VSp & 0.60 & 0.25 & 0.42 & 0.92 & 0.76 & 0.43 & 0.51 & 0.65 & 0.62 & 0.66 & 0.63 & 0.59 & HSp & 0.71 & 0.34 & 0.52 & 0.81 & 0.76 & 0.30 & 0.50 & 0.55 & 0.63 & 0.66 & 0.68 & 0.62 rdf2vec PRS & RUf & 0.98 & 0.99 & 0.99 & 0.02 & 0.50 & 0.02 & 0.50 & 0.02 & 0.50 & 0.01 & 0.50 & 0.60 & VoD & 0.79 & 0.68 & 0.73 & 0.83 & 0.81 & 0.34 & 0.57 & 0.36 & 0.57 & 0.35 & 0.57 & 0.65 & SqD & 0.59 & 0.48 & 0.54 & 0.72 & 0.66 & 0.67 & 0.63 & 0.43 & 0.51 & 0.40 & 0.50 & 0.56 rdf2vec PR & HSp & 0.57 & 0.59 & 0.58 & 0.72 & 0.64 & 0.43 & 0.50 & 0.59 & 0.58 & 0.67 & 0.62 & 0.58 KGloVe Uni & RUf & 0.92 & 0.97 & 0.94 & 0.11 & 0.51 & 0.09 & 0.50 & 0.08 & 0.50 & 0.07 & 0.50 & 0.59 & VoD & 0.54 & 0.88 & 0.71 & 0.73 & 0.64 & 0.61 & 0.58 & 0.51 & 0.52 & 0.52 & 0.53 & 0.60 & SqD & 0.55 & 0.62 & 0.58 & 0.64 & 0.59 & 0.63 & 0.59 & 0.47 & 0.51 & 0.45 & 0.50 & 0.56 KGloVe PrO & HSp & 0.31 & 0.81 & 0.56 & 0.75 & 0.53 & 0.69 & 0.50 & 0.77 & 0.54 & 0.70 & 0.51 & 0.53 KGloVe PR & HSp & 0.47 & 0.69 & 0.58 & 0.61 & 0.54 & 0.54 & 0.50 & 0.57 & 0.52 & 0.65 & 0.56 & 0.54 tabularadjustboxAccuracy on the test set across different embedding and sampling approaches ."
LSC,leicester scientific corpus,TR-25514,"acetosa & 10 & A plant specie Rumex Acetosa ( another usage is ‘ sorreal ’ appearing in 13 documents ) ansdic & 1 & An abbreviation for Ammonium Nitrate and Sodium Salt of Dichloroisocyanuric 18 cm & 10 & Non - word 000009sl & 1 & Non - word ( from the expression ‘ DW=0.000009SL(3.047 ) ’ ) limite & 8 & French word resultan & 1 & French word resultadoscon & 1 & Spanish word with error : ResultadosCon ( ‘ resultado ’ means result in English and appears 90 times in the LSC ) In order to mitigate this issue , we set a minimum cut ( 10 ) so that words appearing in less than the cut - off will not be included in further analysis ."
CNN,convolutional neural network,TR-25552,"Our evaluation shows : ( I ) A sub - pixel cardiac MR image segmentation approach that , in contrast to previous CNN approaches , is robust against slice misalignment and coverage problems ; ( II ) An implicit statistical parametrisation of the left ventricular shape via NNs for pathology classification ; ( III ) An image SR technique that extends previous work and that is robust against slice misalignments ; our approach is computationally more efficient than the state - of - the - art SR - CNN model as the feature extraction is performed in the low - dimensional image space . ("
SP,strictly piecewise,TR-25613,table*[t]Accuracy on Target SP Stringsets Early Stoppingtab : resultsSPES4.5pttabularcccccccccc2c2Training & 2Test & 3cLSTM & 3cs - RNN & 2RPNI & & & 10 & 30 & 100 & 10 & 30 & 100 & 6SP2 & 21k & 1 & 0.871 ( 0.04 ) & 0.954 ( 0.05 ) & 0.992 ( 0.00 ) & 0.910 ( 0.05 ) & 0.994 ( 0.01 ) & 0.992 ( 0.01 ) & 1.000 & & 2 & 0.960 ( 0.03 ) & 0.989 ( 0.02 ) & 0.998 ( 0.00 ) & 0.976 ( 0.01 ) & 0.998 ( 0.01 ) & 1.000 ( 0.00 ) & 1.000 3 - 10 & 210k & 1 & 0.890 ( 0.07 ) & 0.941 ( 0.04 ) & 0.977 ( 0.02 ) & 0.995 ( 0.01 ) & 0.981 ( 0.05 ) & 0.999 ( 0.00 ) & 1.000 & & 2 & 0.979 ( 0.02 ) & 0.990 ( 0.01 ) & 0.994 ( 0.01 ) & 1.000 ( 0.00 ) & 0.984 ( 0.05 ) & 1.000 ( 0.00 ) & 1.000 3 - 10 & 2100k & 1 & 0.833 ( 0.14 ) & 0.819 ( 0.12 ) & 0.890 ( 0.08 ) & 0.997 ( 0.01 ) & 0.999 ( 0.00 ) & 0.997 ( 0.00 ) & 1.000 & & 2 & 0.838 ( 0.16 ) & 0.805 ( 0.13 ) & 0.872 ( 0.09 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 6SP4 & 21k & 1 & 0.881 ( 0.06 ) & 0.946 ( 0.04 ) & 0.963 ( 0.03 ) & 0.887 ( 0.05 ) & 0.966 ( 0.02 ) & 0.979 ( 0.01 ) & 1.000 & & 2 & 0.950 ( 0.03 ) & 0.960 ( 0.03 ) & 0.983 ( 0.01 ) & 0.883 ( 0.05 ) & 0.975 ( 0.01 ) & 0.979 ( 0.01 ) & 1.000 3 - 10 & 210k & 1 & 0.899 ( 0.11 ) & 0.958 ( 0.07 ) & 0.991 ( 0.01 ) & 0.935 ( 0.08 ) & 0.968 ( 0.04 ) & 0.999 ( 0.00 ) & 1.000 & & 2 & 0.926 ( 0.09 ) & 0.971 ( 0.05 ) & 0.991 ( 0.01 ) & 0.954 ( 0.07 ) & 0.984 ( 0.02 ) & 1.000 ( 0.00 ) & 1.000 3 - 10 & 2100k & 1 & 0.943 ( 0.08 ) & 0.940 ( 0.08 ) & 0.920 ( 0.06 ) & 0.942 ( 0.09 ) & 0.958 ( 0.09 ) & 0.973 ( 0.07 ) & 1.000 & & 2 & 0.928 ( 0.08 ) & 0.930 ( 0.09 ) & 0.911 ( 0.07 ) & 0.951 ( 0.09 ) & 0.962 ( 0.08 ) & 0.974 ( 0.08 ) & 1.000 6SP8 & 21k & 1 & 0.884 ( 0.02 ) & 0.884 ( 0.02 ) & 0.903 ( 0.02 ) & 0.861 ( 0.01 ) & 0.878 ( 0.02 ) & 0.857 ( 0.02 ) & 0.817 & & 2 & 0.733 ( 0.03 ) & 0.643 ( 0.06 ) & 0.688 ( 0.04 ) & 0.730 ( 0.01 ) & 0.681 ( 0.06 ) & 0.625 ( 0.04 ) & 0.587 3 - 10 & 210k & 1 & 0.934 ( 0.05 ) & 0.921 ( 0.05 ) & 0.959 ( 0.03 ) & 0.908 ( 0.02 ) & 0.952 ( 0.03 ) & 0.991 ( 0.00 ) & 0.873 & & 2 & 0.637 ( 0.08 ) & 0.659 ( 0.10 ) & 0.704 ( 0.11 ) & 0.600 ( 0.08 ) & 0.640 ( 0.10 ) & 0.837 ( 0.05 ) & 0.634 3 - 10 & 2100k & 1 & 0.977 ( 0.04 ) & 0.975 ( 0.04 ) & 0.980 ( 0.02 ) & 0.964 ( 0.05 ) & 0.990 ( 0.03 ) & 1.000 ( 0.00 ) & 1.000 & & 2 & 0.881 ( 0.11 ) & 0.865 ( 0.13 ) & 0.864 ( 0.08 ) & 0.890 ( 0.08 ) & 0.942 ( 0.09 ) & 0.984 ( 0.03 ) & 1.000 tabulartable *
FEC,federal election candidate,TR-25673,"table[ht ] tabularllll & Facebook & Google & Twitter Ads & All Candidates , & Federal & Federal Included & Issue ads & candidate & candidate , & & related & Issue ads Sponsor & Name & Name , & Name , Info & & FEC / EIN & billing info Ad Contents & Ranges & Ranges & Exact Viewed & Gender , age , & N / A & Gender , age , Audience & geolocation & & geolocation Targeting & N / A & Age , gender , & Age , gender , Info & & geolocation & geolocation & & keywords & Data & Portal , & Portal , & Portal Availability & API ( w / NDA ) & Database & API tabular Transparency Implementations tab : transparency_implementationstableWhat is an ad ?"
FPS,frame per second,TR-25874,"table*[!t]tabularlcccccAlgorithm & tabular[c]@c@YOLOv3 ( 320x320)tabular & tabular[c]@c@YOLOv3 ( 416x416)tabular & tabular[c]@c@YOLOv3 ( 608x608)tabular & tabular[c]@c@Faster R - CNN ( Inception v2)tabular & tabular[c]@c@Faster R - CNN ( ResNet 50)tabular FP & 40 & 66 & 51 & 164 & 137 TP & 159 & 171 & 165 & 228 & 195 FN & 225 & 213 & 219 & 156 & 189 Precision & 0.8058 & 0.7288 & 0.7735 & 0.6091 & 0.6022 Recall & 0.4349 & 0.4577 & 0.4557 & 0.5929 & 0.5042 Quality & 0.3905 & 0.3849 & 0.3923 & 0.4172 & 0.3744 F1score & 0.5541 & 0.5546 & 0.5546 & 0.5887 & 0.5446 mAP & 0.3999 & 0.4152 & 0.4214 & 0.5162 & 0.4317 mIoU & 0.6352 & 0.5988 & 0.6192 & 0.5710 & 0.5850 FPS & 91.28 & 65.31 & 43.84 & 3.35 & 3.8 tabulartab : Evaluation metrics of FASTER R - CNN and YOLOV3 over classesEvaluation metrics of FASTER R - CNN and YOLOV3 over classestable*FN , TP and FPFigure 3 shows that when we used the YOLOv3 , the number of false negatives is much higher than the number of false positives on over classes , and also much higher than the number of true positives , which indicates that most instances go undetected ."
CRF,conditional random field,TR-25912,a ) FCN-32 ; ( b ) FCN-16s ; ( c ) ResNet - DUC ; ( d ) E - Net ; ( e ) SegNet ; ( f ) U - Net ; ( g ) FCN-8s ; ( h ) CWGAN - GP ; ( i ) FC - DenseNet ; ( j ) DSFE - CRF ; ( k ) DSFE - GCN ; ( l ) DSFE - GraphSAGE ; ( m ) DSFE - GGNN ; ( n ) DSFE - GGCN ; ( o ) Ground truth ; ( p ) Optical image .
RG,reber grammar,TR-25942,"We repeated the extraction process on 5000 different patterns 10 times , and we obtained the following results ( average values ) : For RG context : for k 50 , average value of silhouette coefficient is 0.80 and the percentage of accepted sequences is between [ 70 ; 100 ] for k 100 , average value of silhouette coefficient is 0.875 and the percentage of accepted sequences is between [ 85 ; 100 ] For ERG context : for k 50 , average value of silhouette coefficient is 0.750 and the percentage of accepted sequences is between [ 85;100 ] for k 100 , average value of silhouette coefficient is 0.825 and the percentage of accepted sequences is between [ 87;100 ] These results show that it is possible , using the hidden representation of the hidden layer of a RNN - LSTM , to extract a knowledge representation of the hidden rules that is close to the original grammar by 80 in the worst case ."
MDC,metering data collector,TR-26086,"Depending on the choice on the encryption approach in the MDC , as discussed above , there are two choices : ( i ) if the MDC encryption key was negotiated with the SCBR , SCBR would decrypt the data and this data would be disseminated unencrypted ; ( ii ) if the encryption key is negotiated with the trusted parties , the sensitive information ( e.g. , the measurements is kept encrypted even within the SCBR enclave ) , this could be useful for connecting systems that store information , even if the systems themselves can not read it ."
PR,preference ratio,TR-26161,"figure[h ] plots_v3/cs_pr_1.png Output preference ratio for Crime and Sci - Fi Output preference ratio for the Crime and Sci - Fi movie , this is for the whole population and subgroup populations in unbalanced dataset fig : cs_prfigurefigure[h ] plots_v3/cs_bd_1.png Bias disparity for Crime and Sci - Fi Bias disparity for the Crime and Sci - Fi movie , this is for the whole population and subgroup populations in unbalanced dataset fig : cs_bdfigureStep 3 : Users with Extreme Preferences We had 37 users with PR value of zero on Crime movies , meaning that they only watched Sci - Fi movies ."
CN,common neighbours,TR-26257,"List of SymbolsList of abbreviationslongtablecc Acronym & Descriptions ADN & Activity driven network modelling AV & Acquaintance vaccination APV & Absolute percentage variation CIP & co - location interaction parameters CN & Common neighbours DST network & Dense SPST network DDT network & Dense SPDT network DDT1 & Vaccinating neighbours in DDT network with direct linksDDT2 & Vaccinating neighbours in DDT network with any linksGDT & Generated SPDT network with 364 K nodes GST & Generated SPST network with 364 K nodes IMV & Individual movement based vaccination strategy IMVE & Individual movement based vaccination strategy with exact information IMVT & Individual movement based vaccination strategy with temporal information LST & SPDT network with the same number of links that of DDT network LST & SPST network with the same number of links that of DST network MLE & Maximum likelihood estimator OSN & Online social network PFU & Plaque - forming unit RV & Random vaccination DV & Degree vaccination RSE & Root squared error SPST & Same place same time transmission SPDT & Same place different time transmission SIR & Susceptible - infected - recovered SDT network & Sparse SPDT network of links having direct and indirect components SST network & Sparse SPST network of links having indirect component onlySPDT graph & graph based on SPDT diffusion longtableList of symbolslongtablecc Symbol & Descriptions A & Set of active copies of nodes in SPDT graph b & active particle decay rates from an area of interaction & activity potential of node & active periods of a node C & Particle concentration in interaction area C , C , C & scenario 1 , 2 and 3 d & Activation degree - number of SPDT links created during an activation E & Intake dose or exposure of infectious particles & Average volume fraction of room air introduced by exhaled breath f & distribution function F & Disease spreading force in the network at the current day of simulation & Average disease spreading force in the network & Graph & Dynamic graph g & Particle generation rate by an infected individual h & Activation frequency I & Number of infected individuals in the system & Number of infected individuals at a simulation day & Number of infected individuals in the system at the current time that disease prevalence & Number of infected individuals up to a simulation day L & links set N & Total number of individuals , nodes , users p & Pulmonary rate of susceptible individual & Infection probability for an intake dose & Probability of creating a link during an activation & Probability of breaking a created link Q & air exchange rate from an area q & Transition probability for changing inactive to active state R & disease reproduction ability of an infected individual r & Particle removal rate from interaction area r & Median of particles removal rates S & Number of susceptible individual T & Simulation period or disease observation period & Activation period or period host user or node stays at the interacted location & Link creation delay or delay neighbour user or node arrives at the interacted location & Stay duration of user or node stays at the interacted location V & Air volume of interaction area , , & waiting periods of a node Y & Labelling sets in graph X & updates z & Number of time step Z & set of nodes in the SPDT graph & power law exponent & Infection rate at the current day of simulation in the network for an infected individual & Indirect transmission period & central tendency & Scaling parameter of activation degree distribution & neighbour proportion & average volume fraction of room air that is exhaled by an susceptible individual & links presence function & state probability & nodes presence function & switching probability form active to inactive states & Infectiousness of infection particles & Fraction of dose or exposure reaches to the target infection site & Duration that virus is generated or infectious period of infected individual & inter - event time for node in activity driven networks & lower limit of active degree distribution & activation potential in ADN networks & particle accumulation rate & Transition probability for changing active to inactive state longtable"
MSR,minimum storage regenerating,TR-26357,"construction Construction : main We are given two codes : itemize An MSR code with length , number of message symbols , number of parity symbols , number of helper nodes to be contacted for exact repair of any single node and sub - packetization is defined by the parity - check matrix equation eq : main_pc1 H = pmatrix H_1 , 1 & H_1 , 2 & & H_1 , n H_2 , 1 & H_2 , 2 & & H_2 , n & & & H_r , 1 & H_r , 2 & & H_r , n pmatrix B^rn ."
MRE,median recovery error,TR-26434,"fig : LBFGS_visual_realfigure0 ToDo itemize look at distribution of attributes in the latent space If we have time : Do a figure with auto - encoder ( if we manage to sample the latent space , for instance by fitting a Gaussian to the encoded images in the latent space ) Show a picture of a few examples of Eiffel tower or london bridge in LSUN that seem to be memorize but are just samples learn from template show ( at least on some images ) that VGG , perceptual loss , L1 or L1 on Laplacian pyramid gives the same results for recovery because GLO reported some mitigated results on this ( sometimes L2 is better , sometimes pyramid : in the end , they mixed the two metrics ) give statistics about the recovery precision with LBFGS : itemize for a given * generated * target image , the distribution of errors using a lot of random initializations ( to demonstrate that the problem is almost convex , or at least ' easy ' to optimize ) show convergence speed vs SGD or other optimization method ( because it is was is generaly used in the literature : this finding makes it possible to experiment more easily ) , for a given distorted target generated image , the average / median error vs the distorsion : : you did that on training image but it would be nice to show it first on generated images , to show robustness itemize discuss difference LBFGS vs SGD : LBFGS is much faster to converge , not prone to gradient step setting , but can suffer from instabilities add experiments on Auto Encoder : visual recovery results , histograms , MRE values , and maybe FID if we have time ( sampling from the latent space ) add a small paragraph to explain experimental settings , i.e. that we reproduce several architectures from the literature and trained again on splitting , ... tell somewhere the difference ( I think in the intro ) between over - fitting and verbatim memorization , for which we provide a solid definition : add more experiments on LSUN : show histograms with PG - GAN without GAP , show recovery failure results add experiments on MNIST , CIFAR ?"
RNN,recurrent neural network,TR-26490,"Our contributions in this paper are specifically : we evaluate several common NN architectures : feed - forward networks , RNN , GRU , and LSTM , in novel settings , and find that they fail to learn general identity rules ; we identify reasons that prevent the learning process from being successful in this context ; we propose the Relation Based Patterns , a new method to enable the learning of identity rules within the regular network structure ; we show in experiments that identity rules can be learnt with RBP structure on artificial data , including mixed rule - based and concrete patterns , and that they improve performance in real - world prediction tasks;The remainder of this paper is structured as follows ."
IR,information retrieval,TR-26505,"tabular c c Segmented query & Quoted versions & we are the people song lyrics & we are the people "" song lyrics "" & we are "" the people "" song lyrics we are the people song lyrics & we are "" the people "" "" song lyrics "" & "" we are "" the people song lyrics & "" we are "" the people "" song lyrics "" & "" we are "" "" the people "" song lyrics & "" we are "" "" the people "" "" song lyrics "" tabular tab : versionstableWe propose an evaluation framework for segmentation algorithms that generates all possible quoted versions of a segmented query ( see Table tab : versions ) and submits each quoted version to the IR engine ."
AFC,atomic function computation,TR-26649,"With each arc outgoing an AFC node , we associate the atomic function , which takesthe length- incoming vectors , , and produces the length- outgoing vector , i.e. : Similarly , with each arc outgoing a source node , the atomic function takes the length- incoming vectors , , as well as the generated symbols = , and produces the length- outgoing vector , i.e. : ( Note that we consider here the most general case in which a source node does not have to lie on the "" bottom - most "" level of the network , i.e. , it can also have some incoming edges . )"
CT,class table,TR-26852,"figure*[t ] gather * [ T - Var ] ( x ) = C CT x C[T - Field ] CT e C_e & field(f_i , C_e , CT ) = C_i CT e.f_iC_i [ T - Invk ] CT e C_e & CT e C & mtype(m , C_e , CT ) = DC C < : D CTe.m(e ) C [ T - New ] CTe C & fields(C , CT ) = CD & C < : D CTnew C(e ) C [ T - UCast ] CTe D D < : C CT(C)e C[T - DCast ] CTe D C < : D C D CT(C)e C [ T - SCast ] CTe D & C : D & D : C CT(C)e C [ T - Method ] x : C ; this : CCT e E_0 & E_0 < : C_0 extends(C , CT)= D if mtype(m , D , CT ) = DD_0 , then C = D ; C _ 0= D_0 CCT C m(C x ) return e OK [ T - Class ] K = C(D ' g , C ' f)super(g ) ; this.f = f & fields(D , CT)=DD ' CCTM OK CTclass C extends D C f ; K M OK [ T - Program ] CT = _ L'L ( addExt(L')addCtor(L')addFs(L ' ) addMs(L ' ) ) ( CTL ' OK)_L'L L OK gather * Typing rules of Featherweight Java ."
EO,eyes open,TR-26909,"R1IR = [ 90.8 ] & & & & & & EO & & & & R1IR = [ 85.6 ] 2cTask & 9lEC : resting state with eyes closed , EO : resting state with eyes open , MI : motor imagery , ERP : event related potential 2c2*Classifier & 9lANN : artificial neural networks , FDA : Fisher discriminant analysis , KNN : k - nearest neighbours , LDA : linear discriminant analysis & & 9lMAP : maximum a posteriori , CC : cross correlation , L1 ( Manhattan ) distance , L2 ( Euclidean ) distance , cosine distance tabular table*Previous protocols sec : previous_protocolTable table : comparison summarises the state - of - the - art of the existing EEG biometrics applications based on multiple data acquisition days ."
ER,experience replay,TR-27069,tab : cifar_er_agem_analysistabularlcccc1lBucket & 2cNumber of Examples ( fraction of corrects ) & 2cAverage Loss & ER - Random & A - GEM & ER - Random & A - GEM ( r)2 - 3 ( l)4 - 5Both and predict correctly & 3129 ( 0.67 ) & 3129 ( 0.70 ) & 0.25 & 0.42 predicts correctly and fails & 1520 ( 0.33 ) & 1520 & 0.41 & 1.74 fails and predicts correctly & 1360 & 1360 ( 0.30 ) & 2.14 & 0.62 tabular table*figure*[tb ] subfigure0.5 center figs / cifar_acc_train.pdf Train center subfigure subfigure0.5 center figs / cifar_acc_mem.pdf Memory center subfigure subfigure1.0 center figs / cifar_acc_test.pdf Test center subfigure CIFAR Analysis : Evolution of accuracy as a function of tasks on Train/ Memory and Test sets .
MVP,mitral valve prolapse,TR-27094,"B. Variation in ECG patterns for leads I and V1 for patients highlighted in red ( B ) and yellow ( C ) in A. In each case , ECGs show substantial temporal changes in complex morphology , particularly in those features important for a PAH diagnosis ( Figure ) * [ ] [ ] Characteristics of Studies Used to Train Left Ventricular Mass Estimation Model * [ ] [ ] Characteristics of Studies Used to Train Left Atrial Volume Estimation Model * [ ] [ ] Characteristics of Studies Used to Train Mitral Valve Annular e ' Estimation Model * [ ] [ ] Characteristics of Studies Used to Train PAH Classification Model * [ ] [ ] Characteristics of Studies Used to Train HCM Classification Model * [ ] [ ] Characteristics of Studies Used to Train Amyloid Classification Model * [ ] [ ] Characteristics of Studies Used to Train MVP Classification Model * [ ] [ ] Characteristics of Studies Used to Train Left Ventricular Hypertrophy Classification Model * [ ] [ ] Characteristics of Studies Used to Train Diastolic Dysfunction Classification Model * [ ] [ ] Characteristics of Studies Used to Train Left Atrial Enlargement Classification Model * [ ] [ ] Variable Importance for GBM Models for Cardiac Structure and Function * [ ] [ ] Variable Importance for GBM Models for Disease Detection"
MAP,maximum a posteriori,TR-27159,"Then , the likelihood of and based on our generative model is equation*P(R , A ) _ oO_sS_o P(v_o^s _ s , _ o ) _ o O_w W_o P(v_o^w _ w,_o)equation*where the probability of generating a claimed value by a source or a worker becomes align P(v_o^s _ s,_o ) & = _ v V_oP(v_o^s _ s , v_o^*=v ) _ o , veq : margin_s P(v_o^w _ w,_o ) & = _ v V_oP(v_o^w _ w , v_o^*=v ) _ o , v.eq : margin_w alignConsequently , the MAP point estimator is obtained by maximizing the log - posterior asequationeq : map = _ P(R , A)+P ( ) = _ F equationwhere the objective function is alignF & = _ oO_sS_o _ v V_oP(v_o^s _ s , v_o^*=v ) _ o , v & + _ oO_wW_o_v V_oP(v_o^w _ w , v_o^*=v ) _ o , v eq : objectiveL & + _ sSp(_s ) _ wWp(_w ) _ oOp(_o_o ) ."
ML,machine learning,TR-27161,"ML - Schema : Exposing the Semantics of Machine Learning with Schemas and OntologiesGustavo Correa Publio AKSW Group University of Leipzig , Germany Diego Esteves SDA Research University of Bonn , Germany Agnieszka awrynowicz Poznan University of TechnologyPoland Pance Panov Jozef Stefan InstituteLjubljana , Slovenia Larisa Soldatova Brunel University , UK Tommaso Soru AKSW Group University of Leipzig , Germany Joaquin Vanschoren Eindhoven University of TechnologyThe Netherlands Hamid Zafar SDA Research University of Bonn , Germany The ML - Schema , proposed by the W3C Machine Learning Schema Community Group , is a top - level ontology that provides a set of classes , properties , and restrictions for representing and interchanging information on machine learning algorithms , datasets , and experiments ."
SE,selective eraser,TR-27163,"A more sensible approach would be to assess a probability within the text preserved by the SEs that define the topic , which would be : multline P_topic(E(a , w_a ) -4ptE(b , w_b ) ) = = _ k_i ( P_topic([E(k_i , w_t)E(a , w_a ) ] -4pt[E(k_i , w_t)E(b , w_b)]))multlineRestricting ourselves to the set of keywords , the maximum value would always be for the topic - defining SE with the same central term as the antecedent SE ( ) , which simplifies the formula to : multlineeq : TopicalUConditional P_topic(E(a , w_a ) -4ptE(b , w_b ) ) = = E(a , w_a)E(b , w_b)E(a , w_t)D+E(a , w_a)E(b , w_b)E(a , w_t)D_avg(E(a , w_a)D+ E(a , w_a)D_avgmultlinefor any , where is the width of the SEs used to define the topic ."
FEC,forward error correction,TR-27209,"The information is collected by the receiver and sent to the transmitter ; Loss Rate Prediction - Using the feedback statistics , the properties of the error probability are estimated on the server side ; Video Characteristics - This module fetches information from the video sequences that are being transmitted to identify video characteristics such as the frame type and size , as well as the motion vectors ; Ant Colony Optimization - The ACO is responsible for making a joint analysis of all the information gathered by the other modules , establishing the most suitable amount of redundancy to each FEC block ; FEC Blocks - The FEC blocks are built and a specific amount of redundancy designed by the ACO is assigned to each one ."
PC,principal component,TR-27320,"@X rr rr @ & 2 cCNS & 2 cMDC ( lr)2 - 3(lr)4 - 5 & PC 0 & PC 1 & PC 0 & PC 1 Social circle size , & 0.41 & 0.18 & -0.36 & 0.04 Activity space size , & 0.42 & -0.23 & -0.40 & -0.05 New ties / week , & 0.33 & 0.27 & -0.24 & -0.35 New locations / week , & 0.39 & -0.11 & -0.37 & -0.22 Social circle entropy , & 0.29 & 0.33 & -0.36 & -0.23 Activity space entropy , & 0.38 & -0.10 & -0.35 & 0.13 Social circle stability , & -0.12 & -0.50 & -0.11 & 0.56 Activity space stability , & -0.06 & -0.50 & -0.03 & 0.62 Social circle rank turnover , & -0.17 & 0.26 & 0.28 & -0.19 Activity space rank turnover , & -0.35 & 0.37 & 0.42 & -0.18 T=30 , Principal Components ."
OCR,optical character recognition,TR-27350,tabular@cccccc@ & Layer & Filters & Size & Input & Output & conv & & & & & max & & & & & conv & & & & & max & & & & & conv & & & & & conv & & & & & conv & & & & & max & & & & & conv & & & & & conv & & & & & conv & & & & & conv & & & & & conv & & & & & conv & & & & & conv & & & & & detection & & & & tabular tab : cr_nettableWe then created an OCR descriptor by combining the textual content extracted from both license plates .
FEC,forward error correction,TR-27373,"The information is collected by the receiver and sent to the transmitter ; Loss Rate Prediction - Using the feedback statistics , the properties of the error probability are estimated on the server side ; Video Characteristics - This module fetches information from the video sequences that are being transmitted to identify video characteristics such as the frame type and size , as well as the motion vectors ; Ant Colony Optimization - The ACO is responsible for making a joint analysis of all the information gathered by the other modules , establishing the most suitable amount of redundancy to each FEC block ; FEC Blocks - The FEC blocks are built and a specific amount of redundancy designed by the ACO is assigned to each one ."
DL,depth loss,TR-27430,stbl : ablation_sdn 2.5pt tabularlcccccc 1c2*Depth Estimation & 3c & 3c 2 - 7 1c & Easy & Moderate & Hard & Easy & Moderate & Hard & 73.9 / - & 54.0 / - & 46.9 / - & 74.9 / 61.9 & 56.8 / 45.3 & 49.0 / 39.0 + DL & 75.8 / - & 56.2 / - & 51.9 / - & 75.7 / 60.5 & 57.1 / 44.8 & 49.2 / 38.4 & 79.7 / - & 61.1 / - & 54.5 / - & 77.0 / 63.2 & 63.7 / 46.8 & 56.0 / 39.8 tabulartable*table*[hbt ! ]
SW,sliding window,TR-27459,"The PoS tagging problem may be formulated as follows : given a text , each word is assigned ( using a lexicon and a morphological analyzer ) an ambiguity class to obtain the ambiguously tagged text ; the task of a PoS tagger is to obtain a tag sequence as correct as possible , that is , the one that maximizes the probability of that tag sequence given the word sequence : The core idea of SW PoS tagging is to use the ambiguity classes of neighboring words to approximate the dependencies locally : where , is the left context of length ( e.g. if , then , and is the left context of length ."
VM,virtual machine,TR-27828,tabular > gray!25 & 2cx86 - 64 & 2cPOWER & 2cARM & 2cGPU & 2cFPGA & 2cASIC gray!25 -2*Provider & BM & VM & BM & VM & BM & VM & BM & VM & BM & VM & BM & VM Amazon aws : iaas & * & & & & & & & & & & & Microsoft azure : iaas & & & & & & & & & & & & Google gcp : iaas & & & & & & & & & & & & IBM ibm : iaas & * & & * & & & & & & & & & Oracle oracle : iaas & * & * & & & & & & & & & & Scaleway scaleway : iaas & * & & & & * & & & & & & & tabular tab : cloudresources table
AV,acquaintance vaccination,TR-28115,"List of SymbolsList of abbreviationslongtablecc Acronym & Descriptions ADN & Activity driven network modelling AV & Acquaintance vaccination APV & Absolute percentage variation CIP & co - location interaction parameters CN & Common neighbours DST network & Dense SPST network DDT network & Dense SPDT network DDT1 & Vaccinating neighbours in DDT network with direct linksDDT2 & Vaccinating neighbours in DDT network with any linksGDT & Generated SPDT network with 364 K nodes GST & Generated SPST network with 364 K nodes IMV & Individual movement based vaccination strategy IMVE & Individual movement based vaccination strategy with exact information IMVT & Individual movement based vaccination strategy with temporal information LST & SPDT network with the same number of links that of DDT network LST & SPST network with the same number of links that of DST network MLE & Maximum likelihood estimator OSN & Online social network PFU & Plaque - forming unit RV & Random vaccination DV & Degree vaccination RSE & Root squared error SPST & Same place same time transmission SPDT & Same place different time transmission SIR & Susceptible - infected - recovered SDT network & Sparse SPDT network of links having direct and indirect components SST network & Sparse SPST network of links having indirect component onlySPDT graph & graph based on SPDT diffusion longtableList of symbolslongtablecc Symbol & Descriptions A & Set of active copies of nodes in SPDT graph b & active particle decay rates from an area of interaction & activity potential of node & active periods of a node C & Particle concentration in interaction area C , C , C & scenario 1 , 2 and 3 d & Activation degree - number of SPDT links created during an activation E & Intake dose or exposure of infectious particles & Average volume fraction of room air introduced by exhaled breath f & distribution function F & Disease spreading force in the network at the current day of simulation & Average disease spreading force in the network & Graph & Dynamic graph g & Particle generation rate by an infected individual h & Activation frequency I & Number of infected individuals in the system & Number of infected individuals at a simulation day & Number of infected individuals in the system at the current time that disease prevalence & Number of infected individuals up to a simulation day L & links set N & Total number of individuals , nodes , users p & Pulmonary rate of susceptible individual & Infection probability for an intake dose & Probability of creating a link during an activation & Probability of breaking a created link Q & air exchange rate from an area q & Transition probability for changing inactive to active state R & disease reproduction ability of an infected individual r & Particle removal rate from interaction area r & Median of particles removal rates S & Number of susceptible individual T & Simulation period or disease observation period & Activation period or period host user or node stays at the interacted location & Link creation delay or delay neighbour user or node arrives at the interacted location & Stay duration of user or node stays at the interacted location V & Air volume of interaction area , , & waiting periods of a node Y & Labelling sets in graph X & updates z & Number of time step Z & set of nodes in the SPDT graph & power law exponent & Infection rate at the current day of simulation in the network for an infected individual & Indirect transmission period & central tendency & Scaling parameter of activation degree distribution & neighbour proportion & average volume fraction of room air that is exhaled by an susceptible individual & links presence function & state probability & nodes presence function & switching probability form active to inactive states & Infectiousness of infection particles & Fraction of dose or exposure reaches to the target infection site & Duration that virus is generated or infectious period of infected individual & inter - event time for node in activity driven networks & lower limit of active degree distribution & activation potential in ADN networks & particle accumulation rate & Transition probability for changing active to inactive state longtable"
AI,artificial intelligence,TR-28122,"Recent analyses summarize the rapidly increasing number of principles and guidelines for ethical AI fjeld2019,jobin2019 , and tactical support for applying these ideas in practice is available in resources such as the Markkula Center Ethics in Technology Practice Framework and Toolkit,(https://www.scu.edu / ethics - in - technology - practice/ ) the Omidyar Ethical OS Toolkit,(https://ethicalos.org/ ) and the Princeton Dialogues on AI and Ethics Case Studies.(https://aiethics.princeton.edu / case - studies/)PrivacyAs discussed in the Findings section , many respondents were concerned about negative impacts of AI on privacy , reinforcing the value of continued emphasis on designing and developing AI with privacy in mind , concordant with discussion of privacy by design in the EU General Data Protection Regulation ( GDPR).(https://eugdpr.org/ ) The privacy discussion continues to evolve quickly , and best practices for AI technologies continue to be actively explored in the academic , legal , and policy communities ."
GMM,gaussian mixture model,TR-28145,"Given the LTI system eq : LTI driven by system and sensor noises whose probability density functions can be expressed as the Gaussian mixture models in eq : noiseGMM_eta and eq : noiseGMM_v , the probability density function of the residual at time can be written as a Gaussian mixture model of Gaussian modeswhere represents the mixture probabilities of the Gaussian modes , are the means , and are the covariances , [ ] [ ] [ ] [ ] [ ] where captures all the possible permutations of combining terms from the system and measurement noise GMM modes , following the rules:[leftmargin=1.3 cm ] [ Init : ] for [ Add : ] [ Wrap : ] if [ ] then and ."
RV,random vaccination,TR-28208,"List of SymbolsList of abbreviationslongtablecc Acronym & Descriptions ADN & Activity driven network modelling AV & Acquaintance vaccination APV & Absolute percentage variation CIP & co - location interaction parameters CN & Common neighbours DST network & Dense SPST network DDT network & Dense SPDT network DDT1 & Vaccinating neighbours in DDT network with direct linksDDT2 & Vaccinating neighbours in DDT network with any linksGDT & Generated SPDT network with 364 K nodes GST & Generated SPST network with 364 K nodes IMV & Individual movement based vaccination strategy IMVE & Individual movement based vaccination strategy with exact information IMVT & Individual movement based vaccination strategy with temporal information LST & SPDT network with the same number of links that of DDT network LST & SPST network with the same number of links that of DST network MLE & Maximum likelihood estimator OSN & Online social network PFU & Plaque - forming unit RV & Random vaccination DV & Degree vaccination RSE & Root squared error SPST & Same place same time transmission SPDT & Same place different time transmission SIR & Susceptible - infected - recovered SDT network & Sparse SPDT network of links having direct and indirect components SST network & Sparse SPST network of links having indirect component onlySPDT graph & graph based on SPDT diffusion longtableList of symbolslongtablecc Symbol & Descriptions A & Set of active copies of nodes in SPDT graph b & active particle decay rates from an area of interaction & activity potential of node & active periods of a node C & Particle concentration in interaction area C , C , C & scenario 1 , 2 and 3 d & Activation degree - number of SPDT links created during an activation E & Intake dose or exposure of infectious particles & Average volume fraction of room air introduced by exhaled breath f & distribution function F & Disease spreading force in the network at the current day of simulation & Average disease spreading force in the network & Graph & Dynamic graph g & Particle generation rate by an infected individual h & Activation frequency I & Number of infected individuals in the system & Number of infected individuals at a simulation day & Number of infected individuals in the system at the current time that disease prevalence & Number of infected individuals up to a simulation day L & links set N & Total number of individuals , nodes , users p & Pulmonary rate of susceptible individual & Infection probability for an intake dose & Probability of creating a link during an activation & Probability of breaking a created link Q & air exchange rate from an area q & Transition probability for changing inactive to active state R & disease reproduction ability of an infected individual r & Particle removal rate from interaction area r & Median of particles removal rates S & Number of susceptible individual T & Simulation period or disease observation period & Activation period or period host user or node stays at the interacted location & Link creation delay or delay neighbour user or node arrives at the interacted location & Stay duration of user or node stays at the interacted location V & Air volume of interaction area , , & waiting periods of a node Y & Labelling sets in graph X & updates z & Number of time step Z & set of nodes in the SPDT graph & power law exponent & Infection rate at the current day of simulation in the network for an infected individual & Indirect transmission period & central tendency & Scaling parameter of activation degree distribution & neighbour proportion & average volume fraction of room air that is exhaled by an susceptible individual & links presence function & state probability & nodes presence function & switching probability form active to inactive states & Infectiousness of infection particles & Fraction of dose or exposure reaches to the target infection site & Duration that virus is generated or infectious period of infected individual & inter - event time for node in activity driven networks & lower limit of active degree distribution & activation potential in ADN networks & particle accumulation rate & Transition probability for changing active to inactive state longtable"
FEC,forward error correction,TR-28325,"equationI_s = 1nF_i=0^nF-1 I_s(i ) eq : MINT : avgFrameIequationequationI_s = I_sI_s + P_s + B_seq : MINT : normFrameIequationtable[!hbt ] MINT - FEC Adopted Notation small center tabularcl Notation & Meaning & Frame size average & Normalised frame size average & Frame size of the frame & Number of frames in the video sequence & Euclidean distance of a motion vector & Euclidean distance of the motion vector & Macroblock height & Macroblock width & Macroblock area & Area of the macroblock & Number of macroblock in the frame & Temporal intensity tabular tab : MINT : notation center small tableOnce all the frame sizes are normalised , it is possible to perform an exploratory analysis to cluster all frames of all video sequences together according to their sizes ."
ATE,absolute trajectory error,TR-28371,We show the strongest results in boldtabularcY1cmY1cmY1cmY1cmY1cmY1cmY1cmY1cmY1 cm Sequence & 3cfr1-xyz & 3cfr2 - 360-hs & 3cfr3-walk - xyz 2*Method & ATE & RPE & RPE & ATE & RPE & RPE & ATE & RPE & RPE & ( m ) & ( m ) & ( ) & ( m ) & ( m ) & ( ) & ( m ) & ( m ) & ( ) LSD - SLAMengel2014lsd & 0.090 & - & - & - & - & - & 0.124 & - & - ORB - SLAMmur2015orb & 0.009 & 0.007 & 0.645 & - & - & - & 0.012 & 0.013 & 0.694 DeMoN(ftf)UZUMIDB17 & 0.183 & 0.037 & 3.612 & 0.669 & 0.032 & 3.233 & 0.279 & 0.040 & 3.174 DeMoN(1 - 10)UZUMIDB17 & 0.178 & 0.021 & 1.193 & 0.601 & 0.035 & 2.243 & 0.265 & 0.049 & 1.447 Ours(iterative ) & 0.071 & 0.024 & 1.237 & 0.461 & 0.020 & 0.736 & 0.240 & 0.026 & 0.811tabulartab : rgbd_posetablecomment
VM,virtual machine,TR-28391,tabular > gray!25 & 2cx86 - 64 & 2cPOWER & 2cARM & 2cGPU & 2cFPGA & 2cASIC gray!25 -2*Provider & BM & VM & BM & VM & BM & VM & BM & VM & BM & VM & BM & VM Amazon aws : iaas & * & & & & & & & & & & & Microsoft azure : iaas & & & & & & & & & & & & Google gcp : iaas & & & & & & & & & & & & IBM ibm : iaas & * & & * & & & & & & & & & Oracle oracle : iaas & * & * & & & & & & & & & & Scaleway scaleway : iaas & * & & & & * & & & & & & & tabular tab : cloudresources table
CT,computed tomography,TR-28504,"51 541 - 578 Rudin L and Osher S and Fatemi E 1992 Nonlinear total variation based noise removal algorithms Physica D 60(1 - 4 ) 259 - 268 Wang Y and Yang J and Yin W and Zhang Y 2008 A new alternating minimization algorithm for total variation image reconstruction SIAM Journal on Imaging Sciences 1(3 ) 248 - 272 Mumcuoglu E U and Leahy R M and Cherry S R 1996 Bayesian reconstruction of PET images : methodology and performance analysis Physics in medicine and Biology 41(9 ) 1777 Chan T and Marquina A and Mulet P 2000 High - order total variation - based image restoration SIAM Journal on Scientific Computing 22(2 ) 503 - 516 Chan R H and Liang H and Wei S and Nikolova M and Tai X C 2015 High - order Total Variation Regularization Approach for Axially Symmetric Object Tomography from a Single Radiograph Inverse Problems and Imaging 9(1 ) 55 - 77 Bredies K and Kunisch K and Pock T 2010 Total generalized variation SIAM Journal on Imaging Sciences 3(3 ) 492 - 526 Liu X and Huang L and Guo Z 2011 Adaptive fourth - order partial differential equation filter for image denoising Applied Mathematics Letters 24(8 ) 1282 - 1288 Papafitsoros K and Schonlieb C - B 2014 A combined first and second order variational approach for image reconstruction Journal of Mathematical Imaging and Vision 48 ( 2 ) 308 - 338 Do S and Karl W C and Kalra M K and Brady T J and Pien H 2010 Clinical low dose CT image reconstruction using high - order total variation techniques In SPIE Medical Imaging , International Society for Optics and Photonics 76225D-76225D Lysaker M and Lundervold A and Tai X C 2003 Noise removal using fourth - order partial differential equation with applications to medical magnetic resonance images in space and time Image Processing , IEEE Transactions on 12(12 ) 1579 - 1590 ."
DBN,deep belief network,TR-28653,"shapesarrows , positioning , patterns shapes , shapes.geometric , shapes.symbols , shapes.arrows , shapes.multipart , shapes.callouts , shapes.misc multi - objective net - works archi - tecture C - MAPSS different classification hyper - parameters pre - process pre - processing ECS - DBN DBNMSA Multi - State Diagnosis and Prognosis Framework with Feature Learning for Tool Condition MonitoringChong Zhang , Student Member , IEEE , Geok Soon Hong , Jun - Hong Zhou , Kay Chen Tan , Fellow , IEEE , Haizhou Li , Fellow , IEEE , Huan Xu , Jihoon Hong , Member , IEEE , and Hian - Leng ChanC. Zhang and H. Li are with the Department of Electrical and Computer Engineering , G. S. Hong and H. Xu are with the Department of Mechanical Engineering , National University of Singapore , 4 Engineering Drive 3 , 117583 , Singapore . ("
FEC,forward error correction,TR-28692,"equationI_s = 1nF_i=0^nF-1 I_s(i ) eq : MINT : avgFrameIequationequationI_s = I_sI_s + P_s + B_seq : MINT : normFrameIequationtable[!hbt ] MINT - FEC Adopted Notation small center tabularcl Notation & Meaning & Frame size average & Normalised frame size average & Frame size of the frame & Number of frames in the video sequence & Euclidean distance of a motion vector & Euclidean distance of the motion vector & Macroblock height & Macroblock width & Macroblock area & Area of the macroblock & Number of macroblock in the frame & Temporal intensity tabular tab : MINT : notation center small tableOnce all the frame sizes are normalised , it is possible to perform an exploratory analysis to cluster all frames of all video sequences together according to their sizes ."
FEC,forward error correction,TR-28702,"equationI_s = 1nF_i=0^nF-1 I_s(i ) eq : MINT : avgFrameIequationequationI_s = I_sI_s + P_s + B_seq : MINT : normFrameIequationtable[!hbt ] MINT - FEC Adopted Notation small center tabularcl Notation & Meaning & Frame size average & Normalised frame size average & Frame size of the frame & Number of frames in the video sequence & Euclidean distance of a motion vector & Euclidean distance of the motion vector & Macroblock height & Macroblock width & Macroblock area & Area of the macroblock & Number of macroblock in the frame & Temporal intensity tabular tab : MINT : notation center small tableOnce all the frame sizes are normalised , it is possible to perform an exploratory analysis to cluster all frames of all video sequences together according to their sizes ."
BR,binary relevance,TR-28723,"fig_gramar5figurefigure[!htbp ] Verbatim[frame = single , samepage = true]<MLC - AA > : : = < ML - DBPNN > < ML - BPNN > < ML - BPNN><ML - BPNN > : : = < ne > < nhu_bpnn > < lr_bpnn > < m_bpnn > # ML - BPNN='Multi - Label Back Propagation # Neural Network'<ne > : : = RANDOM - INT(10 , 1000 ) # ne='number of epochs'<nhu_bpnn > : : = RANDOM - REAL(0.2 , 1.0 ) * n_attributes # nhu_bpnn='number of hidden units , that # is a parameter that dependes on the # number of attributes of the dataset'<lr_bpnn > : : = RANDOM - REAL(0.001 , 0.1 ) # lr_bpnn='learning rate for BPNN / DBPNN'<m_bpnn > : : = RANDOM - REAL(0.2 , 0.8 ) # m_bpnn='momentum for BPNN and DBPNN'<ML - DBPNN > : : = < ne > < nhu_bpnn > < lr_bpnn > < m_bpnn > < rbm > # ML - DBPNN='Deep ML - BPNN ' # MLC - AA='ML - BPNN'<rbm > : : = RANDOM - INT(1 , 5 ) # rbm='number of layers of ( Stacked ) # Restricted Boltzmann Machines'Verbatim 1 mm Defined Grammar - Part 6 : MLC Algorithm Adaptation Methods .. fig_gramar6figurefigure[!htbp ] Verbatim[frame = single , samepage = true]<META - MLC - LEVEL > : : = < META - MLC1 > < META - MLC2 > < META - MLC3 > < META - MLC4 > MBR BR < ALGS - SLC > # MBR='BR method stacked with feature outputs ' # META - MLC 1 - 4='meta MLC algorithms # with different constraints ' < META - MLC1 > : : = ( SM < RSML > < MLC - BMaD > ) ( < ALGS - PT > < ALGS - SLC > < ML - BPNN > ) # SM='Subset Mapper - MLC method as parameter'<RSML > : : = < bsp > < i_metamlc > < ap > # RSML='Random Subspace Multi - Label'<bsp > : : = RANDOM - INT(10 , 100 ) # bsp='bag size percent'<i_metamlc > : : = RANDOM - INT(10 , 50 ) # i_metamlc='number of iterations for # meta MLC methods'<ap > : : = RANDOM - INT(10 , 100 ) # ap='attribute percent'<MLC - BMaD > : : = < s > < tshd > # MLC - BMaD='MLC using Boolean Matrix Decomposition'<s > : : = RANDOM - INT(1 , L ) # s='size of the compressed matrix'<tshd > : : = RANDOM - REAL(0.0 , 1.0 ) # tshd='threshold for the matrix decomposition'<META - MLC2 > : : = < alg - meta - mlc2 > ( ( < ALGS - PT1 > < ALGS - PT2 > < ALGS - PT4 > ) < ALGS - SLC > < ML - BPNN>)<alg - meta - mlc2 > : : = ( ( BaggingML BaggingMLDup < bsp > ) EnsembleML < bsp_ensembleML > ) < i_metamlc > # BaggingML='Bagging of Multi - Label methods ' # BaggingMLDup='BaggingML with duplicates ' # EnsembleML='Ensemble of Multi - Label methods ' # bsp='bag size percent - defined earlier ' < bsp_ensembleML > : : = RANDOM - INT(52 , 72 ) # bsp_ensembleML='specific bsp for EnsembleML'<META - MLC3 > : : = ( ( EM CM ) < i_metamlc > ) ( ( < ALGS - PT1 > < ALGS - PT2 > < ALGS - PT3 > ) < ALGS - SLC > < ML - BPNN > ) # EM='Expectation Maximization ' # CM='Classification Maximization'<META - MLC4 > : : = < HOMER > < ALGS - PT1 > < ALGS - SLC > < HOMER > : : = < t > < k_homer > # HOMER='Hierarchy Of Multi - label classifiERs'<t > : : = BalancedClustering Clustering Random # t='the type of clustering ' < k_homer > : : = RANDOM - INT(2 , L-1 ) # k_hommer='number of clusters to be created'<pred_tshd > : : = PCut1 PCutL RANDOM - REAL(0.001 , 0.999 ) # pred_tshd='prediction threshold ' # PCut1='P - Cut method',PCutL='P - Cut method by Label ' Verbatim 1 mm Defined grammar - Part 7 : MLC Meta - Algorithms ."
NP,new persian,TR-28770,"[ 196ff.]Lipp2009 states that OP -st- ( found as a reflex of PIE * -k - t- , * -g - t- ) is due to analogy , while other developments are due to a phonological change predating Middle Persian:[noitemsep]PIE * hreg - to- PIr * rasta- OP rasta- ' right ' MP rast NP rastPIr * musti- ' fist ' MP must , must NP mustPIr * -ista- ( superlative suffix ) MP -ist ; e.g. , Phl balist , MMP barist ' highest ' * barjista- ; Phl xwalist , MMP xwarist ' sweetest ' * huarjista- ( cf ."
IR,information retrieval,TR-28810,"Based on the hypothesis that both a script and a subtitle exist for the same movie or episode , the method can be described in a pipeline as follows:[(1 ) ] given a monolingual movie / episode script , we identify dialogue boundaries and speaker tags using clues such as format and story structure tags in the script;[(2 ) ] for a bilingual subtitle , we align each sentence with its translation using clues such as format and time information;[(3 ) ] for each utterance in a processed script , we apply IR techniques to match it with the line(s ) in its corresponding processed subtitle according to the shared language;[(4 ) ] for each matched term , we map the useful annotations such as speaker and dialogue boundaries from the script side to the matched line(s ) in its subtitle side ."
DIC,directed interval class,TR-28902,"The features considered important in this work are the following : enumerate fromRoot : the root pitch class of the first chord , toRoot : the root pitch class of the second chord , fromType : the type of the first chord ( GCT base ) , toType : the type of the second chord ( GCT base ) , fromPCs : the pitch classes included in the first chord , toPCs : the pitch classes included in the second chord , DICinfo : the DIC vector of the transition , DIChas0 : Boolean value indicating whether the DIC of the transition has , DIChas1 : Boolean value indicating whether the DIC of the transition has , DIChasMinus1 : Boolean value indicating whether the DIC of the transition has , ascSemZero : Boolean value indicating whether the first chord has the relative pitch class value 11 , descSemZero : Boolean value indicating whether the first chord has the relative pitch class value 1 , semZero : Boolean value indicating whether the first chord has the relative pitch class value 11 or 1 , ascSemNextRoot : Boolean value indicating whether the first chord has a pitch class with ascending semitone relation with the pitch class of the second chord 's root , descSemNextRoot : Boolean value indicating whether the first chord has a pitch class with descending semitone relation with the pitch class of the second chord 's root , semNextRoot : Boolean value indicating whether the first chord has a pitch class with ascending or descending semitone relation with the pitch class of the second chord 's root , and 5thRootRelation : Boolean value indicating whether the first chord 's root note is a fifth above of the second 's ."
RP,reciprocal pagerank,TR-28936,"table*[!h ] tableThe comparison of Spearman Correlations to the KORE gold standard of different methods from each dataset over time - varying graphs and aggregated graphs with redirects tab : corr - all threeparttable tabularllllllllll & Jaccard ( I+O ) & tabular[c]@l@ Extended Jaccard RP ( I+O ) tabular & tabular[c]@l@ Extended Jaccard RD ( I+O ) tabular 2007 & 0.522472 & 0.530393 & 0.540804 2008 & 0.532665 & 0.529899 & 0.580926 2009 & 0.580944 & 0.613450 & 0.651450 2010 & 0.586593 & 0.637930 & 0.696506 2011 & 0.561152 & 0.619950 & 0.669867 2012 & 0.537278 & 0.582106 & 0.634255 2013 & 0.543986 & 0.589968 & 0.640649 2014 & 0.483211 & 0.530217 & 0.589831 2015 & 0.506170 & 0.529598 & 0.579989 2016 & 0.494577 & 0.520003 & 0.544250 Intersection & 0.470378 & 0.465568 & 0.487342 Union & 0.531830 & 0.652417 & 0.708271 tabular tablenotes[para , flushleft ] , , , and mean the Union model using the Extended Jaccard with Reciprocal Degree Centrality considering both in - links and out - links is significantly better than this result with p - value 0.1 , p - value 0.05 , p - value 0.01 and p - value 0.001 respectively ."
DE,differential evolution,TR-29030,"In this work , the main contributions are : ( i ) the objective function in has been reformulated using additional penalty term for optimal performance , ( ii ) two other evolutionary techniques ( DE and GSA ) have been used in the second phase to efficiently deploy the RNs , ( iii ) the effectiveness of the proposed algorithms is compared and contrasted with on the basis of network lifetime enhancement and speed of convergence , and lastly ( iv ) comprehensive experiments have been carried out to show the efficacy ( faster convergence and better optimal solution ) of using DE as opposed to ABC presented in to deploy backbone devices in WSNs ."
AV,acquaintance vaccination,TR-29062,"List of SymbolsList of abbreviationslongtablecc Acronym & Descriptions ADN & Activity driven network modelling AV & Acquaintance vaccination APV & Absolute percentage variation CIP & co - location interaction parameters CN & Common neighbours DST network & Dense SPST network DDT network & Dense SPDT network DDT1 & Vaccinating neighbours in DDT network with direct linksDDT2 & Vaccinating neighbours in DDT network with any linksGDT & Generated SPDT network with 364 K nodes GST & Generated SPST network with 364 K nodes IMV & Individual movement based vaccination strategy IMVE & Individual movement based vaccination strategy with exact information IMVT & Individual movement based vaccination strategy with temporal information LST & SPDT network with the same number of links that of DDT network LST & SPST network with the same number of links that of DST network MLE & Maximum likelihood estimator OSN & Online social network PFU & Plaque - forming unit RV & Random vaccination DV & Degree vaccination RSE & Root squared error SPST & Same place same time transmission SPDT & Same place different time transmission SIR & Susceptible - infected - recovered SDT network & Sparse SPDT network of links having direct and indirect components SST network & Sparse SPST network of links having indirect component onlySPDT graph & graph based on SPDT diffusion longtableList of symbolslongtablecc Symbol & Descriptions A & Set of active copies of nodes in SPDT graph b & active particle decay rates from an area of interaction & activity potential of node & active periods of a node C & Particle concentration in interaction area C , C , C & scenario 1 , 2 and 3 d & Activation degree - number of SPDT links created during an activation E & Intake dose or exposure of infectious particles & Average volume fraction of room air introduced by exhaled breath f & distribution function F & Disease spreading force in the network at the current day of simulation & Average disease spreading force in the network & Graph & Dynamic graph g & Particle generation rate by an infected individual h & Activation frequency I & Number of infected individuals in the system & Number of infected individuals at a simulation day & Number of infected individuals in the system at the current time that disease prevalence & Number of infected individuals up to a simulation day L & links set N & Total number of individuals , nodes , users p & Pulmonary rate of susceptible individual & Infection probability for an intake dose & Probability of creating a link during an activation & Probability of breaking a created link Q & air exchange rate from an area q & Transition probability for changing inactive to active state R & disease reproduction ability of an infected individual r & Particle removal rate from interaction area r & Median of particles removal rates S & Number of susceptible individual T & Simulation period or disease observation period & Activation period or period host user or node stays at the interacted location & Link creation delay or delay neighbour user or node arrives at the interacted location & Stay duration of user or node stays at the interacted location V & Air volume of interaction area , , & waiting periods of a node Y & Labelling sets in graph X & updates z & Number of time step Z & set of nodes in the SPDT graph & power law exponent & Infection rate at the current day of simulation in the network for an infected individual & Indirect transmission period & central tendency & Scaling parameter of activation degree distribution & neighbour proportion & average volume fraction of room air that is exhaled by an susceptible individual & links presence function & state probability & nodes presence function & switching probability form active to inactive states & Infectiousness of infection particles & Fraction of dose or exposure reaches to the target infection site & Duration that virus is generated or infectious period of infected individual & inter - event time for node in activity driven networks & lower limit of active degree distribution & activation potential in ADN networks & particle accumulation rate & Transition probability for changing active to inactive state longtable"
PSC,paper sentence classification,TR-29113,The combined datasets were : AT and ATIS IntAT and ClassicLitAT and CM and DFG and DT and NYR 10 and SDC and TEATIS Int and ClassicLitATIS Int and CMClassicLit and CMClassicLit and DFGClassicLit and LMRCClassicLit and RSClassicLit and SSTClassicLit and TECM and ATCM and DFGDFG and ATDFG and ATIS IntDT and ATDT and ATIS IntLMRC and ATLMRC and ATIS IntLMRC and RS and SSTLMRC and RS and YTSNYR 10 and ATNYR 10 and ATIS IntNYR 115 and ATNYR 115 and ATIS IntPSC and ATPSC and ATIS IntRS and ATRS and ATIS IntRS and LMRCRS and SSTSDC and ATSDC and ATIS IntSNIPS and ATSNIPS and ATIS IntSNIPS and ATIS Int and ClassicLitSNIPS and ATIS Int and PSCSNIPS and ATIS Int and SSTSST 2 and ATSST 2 and ATIS IntSST and ATSST and ATIS IntSST and ClassicLit and LMRCSST and LMRCSST and SST 2TE and ATTE and ATIS IntTE and NYR 10YTS and ATYTS and ATIS IntYTS and TE and PSC and RS * [ !
CA,cardiac amyloidosis,TR-29238,"CA had an AUROC of 0.86 ( 95 CI 0.82 - 0.89 ) , and the strongest predictors in this model were the early portion of the QRS from lead aVR ( 3.0 , segments 4 - 8 ) , which is blunted in voltage in CA patients ( ( Figure I , p=3x10 ) , QRS duration ( 1.3 ) , the middle and early portions of the QRS from lead I ( 1.2 , segments 8 - 12 ; 1.1 , segments 4 - 8 ) , and the earliest portion of the QRS from lead V1 ( 1.1 , segments 0 - 4 ) ( Figure E - F , Table ) ."
SM,scalar multiplication,TR-29408,"ht]Improvements of different coordinate systems & Coordinate formula & Field [ HTML]FFFFFF & Affine & & ( , ) & [ HTML]FFFFFF & Traditional Projective & & ( , ) & [ HTML]FFFFFF & LD - Projective & & ( , ) & [ HTML]FFFFFF & & & & [ HTML]FFFFFF & -Projective & & ( , ) & [ HTML]FFFFFF & & Improved SM execution time & ( , ) & [ HTML]FFFFFF & Traditional Jacobian & 12 m ( PA ) and 4 m ( PD ) & ( , ) & [ HTML]FFFFFF & Chudnovsky Jacobian & 11 m ( PA ) and 5 m ( PD ) & ( , ) & [ HTML]FFFFFF & Modified Jacobian & 13 m ( PA ) and 4 m ( PD ) & ( , , , ) & ( 160,192,224 ) [ HTML]FFFFFF & Affine - Projective & & & [ HTML]FFFFFF & Affine - Jacobian & ) & & [ HTML]FFFFFF & & 11 m ( PA ) and 3 m ( PD ) & ( ) & [ HTML]FFFFFF & & 8 m ( PA ) and 4 m ( PD ) & & ( 224,256 ) [ HTML]FFFFFF & & 8 m ( PA ) and 4 m ( PD ) & & Efficiency Improvement Via Algorithm SimplificationThis section will show some different approaches to improving ECDSA performance ."
CA,current account,TR-29711,"Sources : BOE ( ID : IUQLBEDR , XUQLBK82 , IUQLBEDR , LPQAUYN ) , ONS ( ID : D7BT , UKEA , PGDP , PRDY , MGSX ) , BIS ( US private sector debt : Q : US : P : A : M : XDC : A , UK : ERI , GBP / USD ( 1955 only ) ) , OECD ( US CPI , US M3 , US GDP , US Unemployment , US CA ) , FRED ( ID : RNUSBIS , FEDFUNDS , PRS85006163 , A229RX0 ) , ( UK private sector debt , M4 , labour productivity)[h ! ]"
HPC,high performance computing,TR-29882,"[ 2][1=][linecolor = red , backgroundcolor = red!25,bordercolor = red,#1]#2[2][1=][linecolor = blue , backgroundcolor = blue!25,bordercolor = blue,#1]#2[2][1=][linecolor = OliveGreen , backgroundcolor = OliveGreen!25,bordercolor = OliveGreen,#1]#2[2][1=][linecolor = Plum , backgroundcolor = Plum!25,bordercolor = Plum,#1]#2[2][1=][disable,#1]#22018 2018 rightsretained [ PEARC ' 18]Practice and Experience in Advanced Research ComputingJuly 22 - 26 , 2018Pittsburgh , PA , USAPEARC ' 18 : Practice and Experience in Advanced Research Computing , July 22 - 26 , 2018 , Pittsburgh , PA , USA10.1145/3219104.3219165978 - 1 - 4503 - 6446 - 1/18/07Leveraging OpenStack and Ceph for a Controlled - Access Data CloudEvan F. Bollig MN Supercomputing InstituteUniversity of Minnesota 599 Walter Library110 Pleasant St SE Minneapolis Minnesota 55455bollig@umn.eduGraham T. Allan MN Supercomputing Institute gta@umn.eduBenjamin J. Lynch MN Supercomputing Institute blynch@umn.eduYectli A. Huerta MN Supercomputing Institute yhuerta@umn.eduMathew Mix MN Supercomputing Institute mattmix@umn.eduEdward A. Munsell MN Supercomputing Institute emunsell@umn.eduRaychel M. Benson MN Supercomputing Institute bens0352@umn.eduBrent Swartz MN Supercomputing Institute swartzbr@umn.eduWhile traditional HPC has and continues to satisfy most workflows , a new generation of researchers has emerged looking for sophisticated , scalable , on - demand , and self - service control of compute infrastructure in a cloud - like environment ."
PC,principal component,TR-29892,"@ X rr rr@ & 2 c CNS & 2 c MDC ( lr)2 - 3(lr)4 - 5 & ( PC 0 ) & ( PC 1 ) & ( PC 0 ) & ( PC 1 ) Social circle size , & 0.41 & 0.16 & 0.37 & -0.15 Activity space size , & 0.42 & -0.24 & 0.42 & -0.08 New ties / week , & 0.33 & 0.28 & 0.27 & 0.33 New locations / week , & 0.38 & -0.05 & 0.37 & 0.19 Social circle entropy , & 0.31 & 0.30 & 0.34 & 0.09 Activity space entropy , & 0.38 & -0.16 & 0.30 & -0.07 Social circle stability , & -0.16 & -0.46 & 0.07 & -0.72 Activity space stability , & -0.10 & -0.49 & -0.12 & -0.51 Social circle rank turnover , & -0.20 & 0.28 & -0.33 & 0.10 Activity space rank turnover , & -0.30 & 0.44 & -0.38 & 0.17 Contribution of the original variables to the first principal component ."
SR,success rate,TR-29936,"tab : table30.95tabularcccccc1 - 6 & & 2 c All&2 c 3 - 6 & & SR & SPL & SR & SPL&TD - A3C & 32.4 & 10.2 & 27.1 & 9.8 2 - 6Unseen&TD - A3C(BC ) & 35.9 & 12.4 & 31.3 & 10.1 2 - 6scenes,&Ours & 47.3 & 31.5 & 42.6 & 27.7 2 - 6Known&Ours - FroView & 46.2 & 17.7 & 40.1 & 14.2 2 - 6targets&Ours - NoGen & 47.1 & 30.9 & 41.9 & 28.1 2 - 6P=17.7 & Ours - VallinaGen & 45.5 & 28.9 & 40.7 & 27.3 2 - 6&TD - A3C & 30.4 & 11.9 & 26.9 & 8.2 2 - 6Unseen&TD - A3C(BC ) & 31.7 & 10.3 & 26.9 & 8.7 2 - 6scenes,&Ours & 41.6 & 27.2 & 38.8 & 25.1 2 - 6Novel&Ours - FroView & 39.7 & 16.8 & 37.1 & 14.2 2 - 6targets&Ours - NoGen & 40.8 & 22.0 & 37.2 & 19.2 2 - 6 P=16.0&Ours - VallinaGen & 35.9 & 17.9 & 32.4 & 15.9 2 - 6tabulartabletable*Average navigation performance ( SR and SPL in ) comparisons on unseen scenes from AVD ."
CNN,convolutional neural network,TR-30017,table[H ] tabularccccccc & 2c80 & 2c160 & 2c320 & GMBF & CNNf15+static & GMBF & CNNf15+static & GMBF & CNNf15+static 2 * 01 & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & 2*Average & & & & & & & & & & & & tabular Relative improvements over SRP - PHAT for strategy invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with the sequences described in Table tab : fine - tuning - material ( columns CNNf15+static ) .
NC,node classification,TR-30163,"The reasons for these observations could be : 1 ) DMF and DeepWalk are both network methods and differ only in one aspect - they are different types of methods ( RS versus NC ) ; 2 ) DeepWalk and the non - network method both use the logistic regression to make predictions and differ only in one aspect - DeepWalk uses network features while the non - network method does not ; and 3 ) DMF differs from the non - network method in two ways : the former is a network method and it does not use logistic regression to make predictions , while the latter is a non - network method that uses logistic regression ."
SPF,structure propagation fusion,TR-30186,"For evaluating structure fusion generalization , we compare structure fusion based graph convolutional networks ( SF - GCN ) , propagation fusion based graph convolutional networks ( PF - GCN ) and structure propagation fusion based graph convolutional networks ( SPF - GCN).In Table , we observe that the performance of SPF - GCN is better than that of other method , and the least improvement of SPF - GCN respectively is for Cora , for Citeseer and for PubMed , while the performance of SP is superior to that of PF - GCN , and the improvement of SF - GCN respectively is for Cora , for Citeseer and for PubMed Therefore , PF and SF both are benefit for further mining the structure information and the role of SF is more important than that of PF ."
AP,associated press,TR-30279,"This variance is clear looking at the following examples : Example 1:ORIGINAL : ( Breitbart ) EPA Chief Scott Pruitt Calls for Exit of Paris Climate AgreementCOPY : ( Truthfeed ) BREAKING Trumps EPA Chief Makes Dramatic Announcement Liberals CryingExample 2:ORIGINAL : ( AP ) Absences fitness atmosphere - new ways to track schoolsCOPY : ( PBS ) Beyond test scores here are new ways states are tracking school successExample 3:ORIGINAL : ( USA Today ) Trey Gowdy new Oversight Committee chair plans to deemphasize Russia investigationCOPY : ( Freedom Daily ) BREAKING Trey Gowdy Exposes Massive Scam By Hillary And ObamaExample 4:ORIGINAL : ( NPR ) Republicans Now Control Obamacare - Will Your Coverage ChangeCOPY : ( Salon ) Death by 1000 cuts How Republicans can still alter your health coverageNote , not all title changes are bad ."
CNN,convolutional neural network,TR-30327,"Table presents the results of our CNN - based code generation , in comparison with previous state - of - the - art models : ( 1 ) Latent Predictor Network , an enhanced sequence - to - sequence model with multiple token - level predictors ; ( 2 ) SEQ2TREE , a sequence - to - sequence model based on AST ; ( 3 ) Syntactic Neural Model , an LSTM decoder based on AST structures ; and ( 4 ) Abstract Syntax Networks , another AST - based sequence - to - sequence model , which builds two LSTMs predicting rules in the horizontal and vertical directions , respectively ."
SP,strictly piecewise,TR-30473,table*[t]Accuracy on Target SP Stringsets after 100 Epochstab : resultsSP4.5pttabularcccccccccc2c2Training & 2Test & 3cLSTM & 3cs - RNN & 2RPNI & & & 10 & 30 & 100 & 10 & 30 & 100 & 6SP2 & 21k & 1 & 0.847 ( 0.06 ) & 0.935 ( 0.07 ) & 0.952 ( 0.07 ) & 0.910 ( 0.05 ) & 0.999 ( 0.00 ) & 0.999 ( 0.00 ) & 1.000 & & 2 & 0.873 ( 0.10 ) & 0.951 ( 0.08 ) & 0.947 ( 0.08 ) & 0.976 ( 0.01 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 3 - 10 & 210k & 1 & 0.734 ( 0.12 ) & 0.673 ( 0.04 ) & 0.720 ( 0.03 ) & 0.937 ( 0.13 ) & 0.960 ( 0.08 ) & 0.972 ( 0.06 ) & 1.000 & & 2 & 0.723 ( 0.12 ) & 0.656 ( 0.04 ) & 0.701 ( 0.03 ) & 0.934 ( 0.13 ) & 0.960 ( 0.08 ) & 0.972 ( 0.06 ) & 1.000 3 - 10 & 2100k & 1 & 0.680 ( 0.08 ) & 0.707 ( 0.10 ) & 0.732 ( 0.07 ) & 0.974 ( 0.07 ) & 0.974 ( 0.07 ) & 0.982 ( 0.04 ) & 1.000 & & 2 & 0.665 ( 0.09 ) & 0.697 ( 0.12 ) & 0.716 ( 0.08 ) & 0.977 ( 0.07 ) & 0.974 ( 0.08 ) & 0.986 ( 0.04 ) & 1.000 6SP4 & 21k & 1 & 0.883 ( 0.06 ) & 0.885 ( 0.08 ) & 0.775 ( 0.05 ) & 0.890 ( 0.05 ) & 0.969 ( 0.02 ) & 0.988 ( 0.01 ) & 1.000 & & 2 & 0.943 ( 0.04 ) & 0.840 ( 0.09 ) & 0.749 ( 0.06 ) & 0.885 ( 0.06 ) & 0.975 ( 0.01 ) & 0.985 ( 0.01 ) & 1.000 3 - 10 & 210k & 1 & 0.862 ( 0.13 ) & 0.880 ( 0.14 ) & 0.853 ( 0.08 ) & 0.696 ( 0.05 ) & 0.840 ( 0.15 ) & 0.903 ( 0.12 ) & 1.000 & & 2 & 0.862 ( 0.14 ) & 0.877 ( 0.15 ) & 0.843 ( 0.08 ) & 0.686 ( 0.05 ) & 0.841 ( 0.16 ) & 0.900 ( 0.12 ) & 1.000 3 - 10 & 2100k & 1 & 0.842 ( 0.13 ) & 0.791 ( 0.14 ) & 0.720 ( 0.09 ) & 0.884 ( 0.14 ) & 0.828 ( 0.17 ) & 0.895 ( 0.12 ) & 1.000 & & 2 & 0.831 ( 0.13 ) & 0.785 ( 0.13 ) & 0.716 ( 0.08 ) & 0.900 ( 0.15 ) & 0.827 ( 0.17 ) & 0.902 ( 0.13 ) & 1.000 6SP8 & 21k & 1 & 0.844 ( 0.04 ) & 0.863 ( 0.05 ) & 0.901 ( 0.01 ) & 0.871 ( 0.01 ) & 0.885 ( 0.02 ) & 0.878 ( 0.01 ) & 0.817 & & 2 & 0.699 ( 0.08 ) & 0.627 ( 0.05 ) & 0.692 ( 0.03 ) & 0.719 ( 0.02 ) & 0.663 ( 0.06 ) & 0.668 ( 0.03 ) & 0.587 3 - 10 & 210k & 1 & 0.827 ( 0.15 ) & 0.798 ( 0.11 ) & 0.804 ( 0.04 ) & 0.818 ( 0.12 ) & 0.856 ( 0.10 ) & 0.979 ( 0.02 ) & 0.873 & & 2 & 0.654 ( 0.11 ) & 0.672 ( 0.10 ) & 0.638 ( 0.05 ) & 0.566 ( 0.05 ) & 0.646 ( 0.05 ) & 0.811 ( 0.08 ) & 0.634 3 - 10 & 2100k & 1 & 0.880 ( 0.10 ) & 0.927 ( 0.08 ) & 0.904 ( 0.08 ) & 0.893 ( 0.14 ) & 0.978 ( 0.04 ) & 0.988 ( 0.01 ) & 1.000 & & 2 & 0.760 ( 0.12 ) & 0.802 ( 0.13 ) & 0.739 ( 0.09 ) & 0.825 ( 0.15 ) & 0.909 ( 0.11 ) & 0.907 ( 0.09 ) & 1.000 tabulartable *
MAP,maximum a posteriori,TR-30571,"eqnarrayIn conclusion , the log - likelihood function , natural logarithm of the likelihood function , readsequation35p(Y_tX_t)=_k=0^t _ i=1^p[y_k^i F^i(^i - g^i(x_k))+(1-y_k^i ) ^i ( ^i - g^i(x_k))],equationand the cost function to be minimized in the MAP estimation problem ( 28 ) turns out to be , up to additive constant terms , equal toeqnarray36J_t ( X_t ) & = & x_0-x_0 ^ 2_P+_k=0^tx_k+1-f(x_k , u_k)^2_G&-&_k=0^t _ i=1^p[y_k^i F^i(^i - g^i(x_k))+(1-y_k^i ) ^i(^i - g^i(x_k))].&&eqnarrayUnfortunately , a closed - form expression for the global minimum of ( 36 ) does not exist and , hence , the optimal MAP estimate has to be determined by resorting to some numerical optimization routine ."
SE,smarteda,TR-30573,table[ht]3pttabularllccccccccccccccccc Task type & Task & a & aE & DE & dM & d & EPD & e & eR & fM & i & R & SE & s & v & x 4*Dataset & Variable types & & x & x & x & x & & x & & x & x & & x & x & x & & Dimensions & & x & x & x & x & x & & & x & x & & x & & x & & Other info & & & x & & & & & & & x & & & & x & & Compare datasets & x & & & & & & & & x & x & & & & x & 5*Validity & Missing values & & x & x & x & x & x & x & & x & x & & x & x & x & x & Redundant col . &
TDS,taint dependency sequences,TR-30653,"Application & Name & LoC & Constraints & TDS + GA & coverage + GA & Random inputs 1 & sendmail & mimefromqp & 65 & ' = n ' & 20 & 26 & 243 2 & sendmail & buildfname & 52 & '' and not ( , ; ) & 6 & 10 & 34 3 & edbrowse & ftpls & 49 & ' -- ' ( in the beginning ) & 35 & * & * tabulartab : resulttableIn the table , columns 2 - 3 denote the path of the vulnerable program in the Verisec suite ."
NE,nash equilibrium,TR-30687,"The AlgorithmsWe use two multi - population ( each player has its own population of chromosomes representing its alternative choices at any round ) co - evolutionary genetic algorithms , Vriend 's individual learning algorithm [ 15 ] and co - evolutionary programming , a similar algorithm that has been used for the game of prisoner 's dilemma [ 10 ] and , unsuccessfully , for Cournot Duopoly [ 13]. Since those two algorithms do n't , as it will be seen , lead to convergence to the NE in the models under consideration , we introduce two different versions of the algorithms , as well , which are characterized by the use of opponent choices , when the new generation of each player 's chromosome population is created , and therefore can be regarded as "" socialized "" versions of the two algorithms ."
MRE,median recovery error,TR-30718,"fig : LBFGS_visual_realfigure0 ToDo itemize look at distribution of attributes in the latent space If we have time : Do a figure with auto - encoder ( if we manage to sample the latent space , for instance by fitting a Gaussian to the encoded images in the latent space ) Show a picture of a few examples of Eiffel tower or london bridge in LSUN that seem to be memorize but are just samples learn from template show ( at least on some images ) that VGG , perceptual loss , L1 or L1 on Laplacian pyramid gives the same results for recovery because GLO reported some mitigated results on this ( sometimes L2 is better , sometimes pyramid : in the end , they mixed the two metrics ) give statistics about the recovery precision with LBFGS : itemize for a given * generated * target image , the distribution of errors using a lot of random initializations ( to demonstrate that the problem is almost convex , or at least ' easy ' to optimize ) show convergence speed vs SGD or other optimization method ( because it is was is generaly used in the literature : this finding makes it possible to experiment more easily ) , for a given distorted target generated image , the average / median error vs the distorsion : : you did that on training image but it would be nice to show it first on generated images , to show robustness itemize discuss difference LBFGS vs SGD : LBFGS is much faster to converge , not prone to gradient step setting , but can suffer from instabilities add experiments on Auto Encoder : visual recovery results , histograms , MRE values , and maybe FID if we have time ( sampling from the latent space ) add a small paragraph to explain experimental settings , i.e. that we reproduce several architectures from the literature and trained again on splitting , ... tell somewhere the difference ( I think in the intro ) between over - fitting and verbatim memorization , for which we provide a solid definition : add more experiments on LSUN : show histograms with PG - GAN without GAP , show recovery failure results add experiments on MNIST , CIFAR ?"
ICP,iterative closest point,TR-30746,"tb ] Left : Recovery rate ( colorbar ) vs. missing data ratio ( y - axis ) vs. SNR ( x - axis ) , Middle : Recovery rate ( colorbar ) vs. outlier ratio ( y - axis ) vs. SNR ( x - axis ) , Right : Recovery rate ( y - axis ) vs. outlier ratio ( x - axis ) , blue : rRWOC , red : ICP , yellow : randomized rRWOC ( ) , purple : randomized rRWOC ( ) Randomized approximation algorithm for The exhaustive approach for the dimensional case requires -subset comparisons of , in order to guarantee hitting correct ( in the noiseless case ) or approximately correct ( in the noisy case ) regression coefficients , with complexity ."
HAN,hierarchical attention network,TR-30763,"For BiLSTM , all combinations of the following hyperparameter values were tested before choosing the best combination , which is written in bold in the list below : Batch size : 8 , 16 , 32Learning rates : 0.00005 , 0.0001 , 0.0002 , 0.0004 , 0.0008Word embedding size : 100 , 200 , 400LSTM layer size : 128 , 256Number of LSTM layers : 1 , 2 , 3 , 4Dropout after every LSTM layer : 0.2 , 0.3 , 0.4For HAN , all combinations of the following hyperparameter values were tested ( the best combination is written in bold in the list below):Batch size : 8 , 16 , 32Learning rates : 0.00005 , 0.0001 , 0.0002 , 0.0004 , 0.0008Word embedding size : 100 , 200 , 400Sentence embedding size : 100 , 200 , 400We used the same configuration for all the corpora and performed no corpus specific tweaking of classifier parameters ."
SO,smart object,TR-30829,"table[H ] tabularccc Scenario & Simple & Complex & queries & queries Consumer SO 2 & 0.18 ms & 0.44 ms tabularVarying query complexity - Consumer SO tab : lev2tab3tabletable * tabularccccccccccc Queries & Q1 & Q2 & Q3 & Q4 & Q5 & Q6 & Q7 & Q8 & Q9 & Q10 Query selectivity & 3,5 & 1,84 & 0,85 & 0,72 & 0,55 & 0,37 & 0,28 & 0,2 & 0,15 & 0,13 Extra bits per output tuple & 98 & 140 & 210 & 245 & 294 & 336 & 392 & 455 & 483 & 546 Bandwidth overload per hour & 3440 & 2576 & 1785 & 1764 & 1617 & 1243 & 1097 & 910 & 724 & 709 tabularVarying query complexity - SO network - bandwidth overhead tab : bandwidthtable*Smart Object network : We simulate a smart object network via a Streambase query , where each single Streambase operator acts as a smart object ."
CSD,contextual sentence decomposition,TR-30832,"[ tab2,tabularx = v1.5cmv3.2cmv3.5cmZ , fontupper=]System & Baselines & sentences and domain & Metrics TextRunner & KnowItAll & 400 Web & correct extractions WOE & TextRunner & 300 news 300 Wikipedia 300 Web & precision - recall curve OLLIE & ReVerb WOEparse & 300 news ( from WOE)300 Wikipedia ( from WOE)300 biology & precision - yield curve ReVerb & TextRunner WOEpos WOEparse & 500 Web & precision - recall curve KrakeN & ReVerb & 500 Web ( from ReVerb ) & precisioncompleteness facts extracted per sentence Exemplar & ReVerb OLLIE Sonex Patty TreeKernel SwiRL Lund & 500 Web ( from TextRunner)500 news100 news ( from TreeKernel)222 news & binary : precisionrecallF1-score n - ary : precision over argumentsrecall over arguments PredPatt & OLLIE ClausIE Stanford Open IE OpenIE4 & 13k Web 36k news & precision - recall curve ClausIE & TextRunnerWOEparseReVerbOLLIEKrakeN & 500 Web ( from ReVerb)200 Wikipedia 200 news & precision - yield curve correct extractions Schmid14 & ReVerb Exemplar & 500 Web ( from TextRunner)500 news100 news ( from TreeKernel ) & precisionrecallF1-scoretime per sentence before and after sentence re - structuring OpenIE4 & ReVerb OLLIE & not reported & precisionyield CSD - IE & ReVerbOLLIEClausIE & 200 Wikipedia ( from ClausIE)200 news ( from ClausIE ) & triples labeled accurate correct triples labeled minimalcoverage ( text contained in at least one triple)average triple length NestIE & ReVerbOLLIE ClausIE & 200 Wikipedia ( from ClausIE ) 200 news ( from ClausIE ) & correctness ( 0/1)minimality ( 0/1)informativeness ( 0 - 5 ) MinIE & ClausIE OLLIE Stanford Open IE & 10k news ( from Sandhaus08)200 news ( from ClausIE)200 Wikipedia ( from ClausIE ) & extractions non - redundant extractionsrecallfactual precisionattribution precisionmean word count per triple ( proxy for minimality)Graphene & OLLIEReVerbPropSClausIEStanford Open IEOpenIE4 & 3,200 Wikipedia and news & precision - recall curve Comparison of the intrinsic evaluation approaches applied by the different Open IE systems ."
FEC,forward error correction,TR-30851,"The information is collected by the receiver and sent to the transmitter ; Loss Rate Prediction - Using the feedback statistics , the properties of the error probability are estimated on the server side ; Video Characteristics - This module fetches information from the video sequences that are being transmitted to identify video characteristics such as the frame type and size , as well as the motion vectors ; Ant Colony Optimization - The ACO is responsible for making a joint analysis of all the information gathered by the other modules , establishing the most suitable amount of redundancy to each FEC block ; FEC Blocks - The FEC blocks are built and a specific amount of redundancy designed by the ACO is assigned to each one ."
SVM,support vector machine,TR-30869,"In fact , the interval [ 0 5 [ is divided into two groups : [ 0 2 [ and [ 2 5 [ , and two distinct sub - conditions are thus considered : Sub - condition 1 : The embedding is performed by setting to 1 for [ 0 2 [ and 0.5 for [ 2 5[. Sub - condition 2 : The embedding is performed by setting to 0.5 for [ 0 2 [ and 0.1 for [ 2 5[.The accuracy is not increased by more than 1 in all conditions when SVM is used , while the opposite for NN classifier ."
FEC,forward error correction,TR-30960,"The information is collected by the receiver and sent to the transmitter ; Loss Rate Prediction - Using the feedback statistics , the properties of the error probability are estimated on the server side ; Video Characteristics - This module fetches information from the video sequences that are being transmitted to identify video characteristics such as the frame type and size , as well as the motion vectors ; Ant Colony Optimization - The ACO is responsible for making a joint analysis of all the information gathered by the other modules , establishing the most suitable amount of redundancy to each FEC block ; FEC Blocks - The FEC blocks are built and a specific amount of redundancy designed by the ACO is assigned to each one ."
ODE,ordinary differential equation,TR-30962,"For the second claim , align * x_^ * - x^ * = & _ ^N ( x_^ * ) - x^ * & by x_^ * = _ ^N ( x_^ * ) & _ ^N ( x_^ * ) - _ ^N(x^ * ) + _ ^N ( x^ * ) - x^ * & by triangle inequality & L^(N ) x_^ * - x^ * + _ ^N ( x^ * ) - x^ * & by the definition of & 12 x_^ * - x^ * + _ ^N ( x^ * ) - x^ * & by L^(N ) 1/2align*For the third claim , align * x^ * - _ ^N ( x^ * ) & _ i=0^N-1 _ ^i ( x^ * ) - _ ^(i+1 ) ( x^ * ) & _ i=0^N-1 L^(i ) x^ * - _ ( x^ * ) align*For the last claim , align * & x^ * ( t ) - _ ( x^ * ) ( t ) = & ( x^ * ) ( t ) - _ ( x^ * ) ( t ) = & _ 0^t F ( x^*(s ) , s ) s - _ 0^t _ j=1^D F(x^*(c_j ) , c_j ) _ j(s ) s = & _ 0^t t x^*(s ) s - _ 0^t _ j=1^D t x^*(c_j ) _ j(s ) s & _ 0^t ( t x^*(s ) - q(s ) ) s - _ 0^t _ j=1^D ( t x^*(c_j ) - q(c_j ) ) _ j(s ) s + & _ 0^t q(s ) s - _ 0^t _ j=1^D q(c_j ) _ j(s ) s & _ 0^tt x^*(s)-q(s)s+_j=1^Dtx^*(c_j)-q(c_j)_0^t_j(s)s + 0 & ( 1 + _ ) + 0align*where the first step follows by , the second step follows by the definition of and , the third step follows by is the solution of ODE , the fourth step follows by triangle inequality , the second last step follows by , and the last step follows by and the definition of ."
PDT,pulse discrete time,TR-31007,"These formulations are initially categorized into three groups according to the meaning of variables , as follows : [ Pulse : ] pulse discrete time - PDT ( the most used ) formulation with binary variables , such that if job starts at time , otherwise ; [ Step : ] step discrete time - SDT formulation with binary variables , such that if job starts at time or before , otherwise ; [ On / Off : ] on / off discrete time - OODT ( the lesser used ) formulation with binary variables such that if job is processed at time , otherwise ."
RV,random vaccination,TR-31011,"tikzpicture customlegend[legend columns=4,legend style = at=(0.12,1.02),draw = none , column sep=3ex , line width=2pt , font= , legend entries = RV , AV , IMV , DV , direct , indirect ] solid , color = blue color = red color = green color = magenta color = black dashdotted , color = black customlegend tikzpicture pvan_a.pdfpvan_b.pdfpvan_c.pdfpvan_d.pdfAverage outbreak sizes at various vaccination rates ( percentage of total nodes ) of different strategies : ( A , B ) nodes are vaccinated with contacts created for direct interactions and ( C , D ) comparison of outbreak sizes for vaccinating nodes with contacts based on the direct interactions ( solid lines ) and contact based on any direct or indirect interactions ( dashed lines)-1.5emfig : avacfigurefigure[h ! ]"
CNS,copenhagen networks study,TR-31015,"@X rrr rrr @ Model M1 : Activity space size , & coeff & p val & LMG Social circle size , & & & 0.95 gender & & 0.2 & 0.04 time coverage & & 0.05 & 0.01 [ , , ] Model M2 : Activity space entropy , & & & Social circle entropy , & & & 0.34 gender & & & 0.16 time coverage & & & 0.51 [ , , ] Model M3 : New locations / week , & & & New ties / week , & & & 0.94 gender & & 0.04 & 0.04 time coverage & & 0.5 & 0.01 [ , , ] Model M4 : Activity space stability , & & & Social circle stability , & & & 0.57 gender & & 0.02 & 0.06 time coverage & & & 0.37 [ , , ] Model M5 : Activity space rank turnover , & & & Social circle rank turnover , & & & 0.99 gender & & 0.3 & 0.01 time coverage & & 1.0 & 0.0 [ , , ] T=30 , Linear regression models for the CNS dataset ."
CNN,convolutional neural network,TR-31020,table[H ] tabularcccccccccc & 3c80 & 3c160 & 3c320 & SRP & GMBF & CNNf15+static & SRP & GMBF & CNNf15+static & SRP & GMBF & CNNf15+static 2 * 01 & & & & & & & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & & & & & & & 2*Average & & & & & & & & & & & & & & & & & & tabular Results for the SRP - PHAT strategy ( columns SRP ) ; the one invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with the sequences described in Table tab : fine - tuning - material ( columns CNNf15+static ) .
RP,reciprocal pagerank,TR-31045,"tableSpearman Correlations to the KORE gold standard with the text - based approach for different Wikipedia dumpstab : bag - of - word - results tabularp5cmlVersion & Correlation Wikipedia at the time when YAGO2 was created & 0.503212 Wikipedia at the time of DBpedia 2009 dump & 0.491440 Wikipedia at the time of DBpedia 2010 dump & 0.502987 tabulartabletable*tableSpearman Correlations to the KORE gold standard comparing text - based approach with graph - based approaches between models without redirects and models with redirects of data from DBpedia 2009 and DBpedia 2010tab : corr-2009 - 2010-redirectsthreeparttable tabularp40mmllll2 * & 2cDbpedia 2009 & 2cDbpedia 2010 2 - 5 & 1cWithout Redirect & 1cWith Redirects & 1cWithout Redirect & 1cWith Redirects TF - IDF & 0.491440 & - & 0.502987 & - Jaccard ( I ) & 0.568509 & 0.575758 & 0.568026 & 0.588351 Jaccard ( O ) & 0.511564 & 0.504138 & 0.559898 & 0.564088 Jaccard ( I+O ) & 0.578706 & 0.580944 & 0.585535 & 0.586593 Extended Jaccard RP ( I ) & 0.585212 & 0.591538 & 0.569857 & 0.616843 Extended Jaccard RP ( O ) & 0.578478 & 0.566556 & 0.576616 & 0.601197 Extended Jaccard RP ( I+O ) & 0.604112 & 0.613450 & 0.599284 & 0.637930 Extended Jaccard RD ( I ) & 0.623768 & 0.634961 & 0.632760 & 0.672512 Extended Jaccard RD ( O ) & 0.588592 & 0.567652 & 0.601755 & 0.609715 Extended Jaccard RD ( I+O ) & 0.639055 & 0.651450 & 0.658647 & 0.696506 tabulartablenotes[para , flushleft ] , , , and mean the Wikipedia article link network with redirects using the Extended Jaccard with Reciprocal Degree Centrality considering both in - links and out - links is significantly better than this result with p - value 0.1 , p - value 0.05 , p - value 0.01 and p - value 0.001 respectively ."
PMF,probability mass function,TR-31179,"Require some base graph of arbitrary complexity Given some integer , the set contains all single - component subgraphs formed by nodes Define as an ordered -tuple containing all Define the function to count the number of subgraph isomorphisms of , and a PMF over all elements in Draw structure from this probability distribution and add that structure to the network by some growth rule Repeat steps 4 - 5 until the some termination rule is satisfied The basic steps of the GMM frameworkThe GMM framework brings with it a different set of limitations , many of which will be discussed in the conclusion ."
CNN,convolutional neural network,TR-31186,"However , the results show : ( i ) the size of the corpus has a very positive impact on the accuracy for the content - based deep learning approaches and those models perform best in the larger datasets , since the results were statistical significant over the baseline for the multibotwoz dataset ( ) ; and ( ii ) if the dialogue dataset is small and topic - oriented ( but with few topics ) , which is the case of finch dataset , it is sufficient to use an agent - only MLE or SVM models , although slightly higher accuracies can be achieved with the use of the content of the utterances with a CNN model ( ) ."
SP,strictly piecewise,TR-31343,table*[t]Accuracy on Target SP Stringsets after 100 Epochstab : resultsSP4.5pttabularcccccccccc2c2Training & 2Test & 3cLSTM & 3cs - RNN & 2RPNI & & & 10 & 30 & 100 & 10 & 30 & 100 & 6SP2 & 21k & 1 & 0.847 ( 0.06 ) & 0.935 ( 0.07 ) & 0.952 ( 0.07 ) & 0.910 ( 0.05 ) & 0.999 ( 0.00 ) & 0.999 ( 0.00 ) & 1.000 & & 2 & 0.873 ( 0.10 ) & 0.951 ( 0.08 ) & 0.947 ( 0.08 ) & 0.976 ( 0.01 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 3 - 10 & 210k & 1 & 0.734 ( 0.12 ) & 0.673 ( 0.04 ) & 0.720 ( 0.03 ) & 0.937 ( 0.13 ) & 0.960 ( 0.08 ) & 0.972 ( 0.06 ) & 1.000 & & 2 & 0.723 ( 0.12 ) & 0.656 ( 0.04 ) & 0.701 ( 0.03 ) & 0.934 ( 0.13 ) & 0.960 ( 0.08 ) & 0.972 ( 0.06 ) & 1.000 3 - 10 & 2100k & 1 & 0.680 ( 0.08 ) & 0.707 ( 0.10 ) & 0.732 ( 0.07 ) & 0.974 ( 0.07 ) & 0.974 ( 0.07 ) & 0.982 ( 0.04 ) & 1.000 & & 2 & 0.665 ( 0.09 ) & 0.697 ( 0.12 ) & 0.716 ( 0.08 ) & 0.977 ( 0.07 ) & 0.974 ( 0.08 ) & 0.986 ( 0.04 ) & 1.000 6SP4 & 21k & 1 & 0.883 ( 0.06 ) & 0.885 ( 0.08 ) & 0.775 ( 0.05 ) & 0.890 ( 0.05 ) & 0.969 ( 0.02 ) & 0.988 ( 0.01 ) & 1.000 & & 2 & 0.943 ( 0.04 ) & 0.840 ( 0.09 ) & 0.749 ( 0.06 ) & 0.885 ( 0.06 ) & 0.975 ( 0.01 ) & 0.985 ( 0.01 ) & 1.000 3 - 10 & 210k & 1 & 0.862 ( 0.13 ) & 0.880 ( 0.14 ) & 0.853 ( 0.08 ) & 0.696 ( 0.05 ) & 0.840 ( 0.15 ) & 0.903 ( 0.12 ) & 1.000 & & 2 & 0.862 ( 0.14 ) & 0.877 ( 0.15 ) & 0.843 ( 0.08 ) & 0.686 ( 0.05 ) & 0.841 ( 0.16 ) & 0.900 ( 0.12 ) & 1.000 3 - 10 & 2100k & 1 & 0.842 ( 0.13 ) & 0.791 ( 0.14 ) & 0.720 ( 0.09 ) & 0.884 ( 0.14 ) & 0.828 ( 0.17 ) & 0.895 ( 0.12 ) & 1.000 & & 2 & 0.831 ( 0.13 ) & 0.785 ( 0.13 ) & 0.716 ( 0.08 ) & 0.900 ( 0.15 ) & 0.827 ( 0.17 ) & 0.902 ( 0.13 ) & 1.000 6SP8 & 21k & 1 & 0.844 ( 0.04 ) & 0.863 ( 0.05 ) & 0.901 ( 0.01 ) & 0.871 ( 0.01 ) & 0.885 ( 0.02 ) & 0.878 ( 0.01 ) & 0.817 & & 2 & 0.699 ( 0.08 ) & 0.627 ( 0.05 ) & 0.692 ( 0.03 ) & 0.719 ( 0.02 ) & 0.663 ( 0.06 ) & 0.668 ( 0.03 ) & 0.587 3 - 10 & 210k & 1 & 0.827 ( 0.15 ) & 0.798 ( 0.11 ) & 0.804 ( 0.04 ) & 0.818 ( 0.12 ) & 0.856 ( 0.10 ) & 0.979 ( 0.02 ) & 0.873 & & 2 & 0.654 ( 0.11 ) & 0.672 ( 0.10 ) & 0.638 ( 0.05 ) & 0.566 ( 0.05 ) & 0.646 ( 0.05 ) & 0.811 ( 0.08 ) & 0.634 3 - 10 & 2100k & 1 & 0.880 ( 0.10 ) & 0.927 ( 0.08 ) & 0.904 ( 0.08 ) & 0.893 ( 0.14 ) & 0.978 ( 0.04 ) & 0.988 ( 0.01 ) & 1.000 & & 2 & 0.760 ( 0.12 ) & 0.802 ( 0.13 ) & 0.739 ( 0.09 ) & 0.825 ( 0.15 ) & 0.909 ( 0.11 ) & 0.907 ( 0.09 ) & 1.000 tabulartable *
MS,mobile station,TR-31424,"Additionally , the corresponding terms in eq:19 are defined as follows : 0em : Doppler shift for the th RIS : Doppler shift for the th plain IO : Constant phase shift for the th RIS : Constant phase shift for the th plain IO : Initial radio path distance for the th RIS : Initial radio path distance for the th plain IO : Adjustable phase shift of the th RISHere , the Doppler shifts of the RISs and plain IOs are not only dependent on the speed of the MS , but also on their relative positions with respect to the MS , i.e. , angles of arrival for the incoming signals : and , where and are the angles of arrival for the reflected signals of th RIS and th plain IO , respectively ."
CNN,convolutional neural network,TR-31425,"Compared to all the aforementioned works , we propose an easy to train CNN model , which do not require a lot of images in the training dataset , with combination of transfer learning ( Learning achieved by taking the convolutional base of a pre - trained network , running the new data of 4 traffic categories through it and training a new randomly initialized classifier ) and continuous learning ( Learning achieved by re - training the classifier with wrong predictions till operating period of the system ) capabilities on SOPC without the need of communicating the traffic images to the connected server for further analysis ."
PMF,probability mass function,TR-31494,"Require some base graph of arbitrary complexity Given some integer , the set contains all single - component subgraphs formed by nodes Define as an ordered -tuple containing all Define the function to count the number of subgraph isomorphisms of , and a PMF over all elements in Draw structure from this probability distribution and add that structure to the network by some growth rule Repeat steps 4 - 5 until the some termination rule is satisfied The basic steps of the GMM frameworkThe GMM framework brings with it a different set of limitations , many of which will be discussed in the conclusion ."
DI,direct inspection,TR-31665,"Finding the optimal policy corresponds to solve the following optimization problem [ v01]Resources to sense[v02]Resources to sense at time [ v03]Type I error probabilities[v04]Type II error probabilities[v05]Sensing matrix[v06]Mixing coefficients at time [ v07]Tests for resource [ v08]Cycle for a single test[v09]Set of cycles[v10]Test thresholds[v11]Decision rules[v12]Set of edges[v13]Observations ' distribution parameter[v14]Time horizon[v15]Number of designed tests[v16]Maximum number of mixed sub - bands per test[v17]Resources[v18]Average received noise power[v19]Probability of declaring given [ v20]Reward for resource [ v21]Penalty for resource [ v22]Resources ' binary state[v23]Average received signal power[v24]Observation[v25]Resources ' prior beliefDynamic Design of Sensing MatricesDirect Inspection ( DI ) caseIn the DI case , we limit to have only one non - zero entry , i.e. , ."
ID,item description,TR-31775,table*[htb]tabularlllllllllllView Rate & ID & II & SAGE & SWAG & SAGE ( + ID ) & SWAG ( + ID ) & SAGE ( + II ) & SWAG ( + II ) & SAGE ( + II+ID ) & SWAG ( + II+ID ) Clothing & 16.2 & 10.0 & 10.5 & 10.5 & 22.4 & 23.5 & 16.5 & 20.2 & 22.5 & 23.6 Home & 12.0 & 12.5 & 5.3 & 5.3 & 14.2 & 16.5 & 13.2 & 14.5 & 14.3 & 16.5 Electronic & 20.5 & 20.2 & 7.2 & 7.2 & 21.9 & 25.1 & 20.5 & 21.5 & 22.1 & 25.2 Baby & 12.5 & 13.5 & 3.4 & 3.4 & 14 & 14.5 & 16.8 & 17.5 & 17.0 & 17.6 tabularView rate for different models .
NN,nearest neighbor,TR-31784,"In fact , the interval [ 0 5 [ is divided into two groups : [ 0 2 [ and [ 2 5 [ , and two distinct sub - conditions are thus considered : Sub - condition 1 : The embedding is performed by setting to 1 for [ 0 2 [ and 0.5 for [ 2 5[. Sub - condition 2 : The embedding is performed by setting to 0.5 for [ 0 2 [ and 0.1 for [ 2 5[.The accuracy is not increased by more than 1 in all conditions when SVM is used , while the opposite for NN classifier ."
DC,distributed control,TR-31824,"op - tical net - works semi - conduc - torA Distributed Event - Triggered Control Strategy for DC Microgrids Based on Publish - Subscribe Model Over Industrial Wireless Sensor NetworksSeyed Amir Alavi , Graduate Student Member , IEEE , Kamyar Mehran , Member , IEEE , Yang Hao , Fellow , IEEE , Ardavan Rahimian , Member , IEEE , Hamed Mirsaeedi , Student Member , IEEE , and Vahid Vahidinasab , Senior Member , IEEE Manuscript received December 5 , 2017 ; revised March 5 , 2018 , April 25 , 2018 , and June 30 , 2018 ; accepted July 6 , 2018 ."
LV,left ventricle,TR-31928,"( a ) The MV leaflets were segmented from a stack of MR images of a volunteer at early - diastole , ( b ) positions of the papillary muscle heads and the annulus ring , ( c ) reconstructed MV geometry with chordae , ( d ) a MR image showing the LV and location of the outflow tract ( AV ) and inflow tract ( MV ) , ( e ) the LV wall delineation from short and long axis MR images , ( f ) the reconstructed LV model , in which the LV model is divided into four part : the LV region bellow the LV base , the valvular region , and the inflow and outflow tracts , ( g ) the rule - based fibre orientations in the LV and the MV , and ( h ) the coupled MV - LV model ."
RS,randomly sampled,TR-31940,"Labels & Sampling Percentage 6cStep 2 R1 & & 300 & 5 & 7 & 33 BS - 67 RS R2 & Subset of & 88 & 10 - 20 & 7 & 92 BS - 8 RS VR & & 300 & 5 & 4 & 33 BS - 67 RS 6cStep 3 FR & & 80k & 5 & 4 & 12.5 BS - 87.5 RS Datasets per Round -1emStep 2 : Exploratory Rounds The goal of this step is to tune the crowdsourcing parameters on a smaller dataset , in order to quickly get some insights but minimize the cost while doing so ."
FEC,forward error correction,TR-32065,"figure[!htb ] center ./loss_distribution_MINT.eps center MINT - FEC 's experiment PLR distribution fig : MINT : lossDistfiguretable[!ht ] MINT - FEC Simulation parameters center tabularll Parameters & Value Display sizes & 1920x1080 , 1280x720 , and 800x600 Frame rate mode & Constant Frame rate & 29.970 fps GoP & 19:2 Video format & H.264 Codec & x264 Container & MP4 Propagation model & FriisPropagationLossModel Mobility model & Gauss - Markov UAV velocity & 45 - 65 km / h ( 28 - 40 mph ) LTE Frequency band & 800MHz LTE Mode & FDD LTE Bandwidth & 5 MHz eNodeB Operating Power & 22 dBm Antenna Gain & 16 dBi tabular tab : MINT : parameters center tableFive different schemes were simulated as follows : ( 1 ) without any FEC mechanism ."
BP,belief propagation,TR-32100,"The iterative BP algorithm is applied as follows : all factor nodes associated to direct measurements send messages to corresponding variable nodes ; all variable nodes send messages along incidence edges ( except to an edge towards a factor node associated to direct measurement ) the form of the messages is : messages are equal to the message from step 1 , if variable nodes have direct measurements ; messages take the form of the "" flat start "" given by distribution with means or and variances or , if variable nodes do not have direct measurements ; all factor nodes compute messages to incident variable nodes according to eqn21 ; all variable variable nodes compute messages to incident factor nodes according to eqn16 ; all variable nodes compute corresponding marginal distributions ; repeat steps 3 , 4 , 5 until BP converges ."
RV,random vaccination,TR-32321,"tikzpicture customlegend[legend columns=4,legend style = at=(0.12,1.02),draw = none , column sep=3ex , line width=2pt , font= , legend entries = RV , AV , IMV , DV , direct , indirect ] solid , color = blue color = red color = green color = magenta color = black dashdotted , color = black customlegend tikzpicture pvan_a.pdfpvan_b.pdfpvan_c.pdfpvan_d.pdfAverage outbreak sizes at various vaccination rates ( percentage of total nodes ) of different strategies : ( A , B ) nodes are vaccinated with contacts created for direct interactions and ( C , D ) comparison of outbreak sizes for vaccinating nodes with contacts based on the direct interactions ( solid lines ) and contact based on any direct or indirect interactions ( dashed lines)-1.5emfig : avacfigurefigure[h ! ]"
QA,question answering,TR-32377,"table[!tbh]minipage0.5tabularp2cmlll4cWikiMovies Dataset Perturbations & BLEU & QBLEU & Hit 1 ( ) Original & 100 & 100 & 76.5 Stop Words & 25.4 & 84.0 & 75.6 Relation Words & 29.4 & 64.3 & 54.7 Question Type & 74.0 & 79.3 & 73.5 NER & 41.9 & 48.5 & 17.97 tabularAccuracy reported across different types of questions for WikiMovies Datasetmachine_wikimoviesminipageminipage0.5table - format=-1.2tabularp2cmcccc5cSQuAD Dataset Perturbations & 1lBLEU & 1lQBLEU & 1lF1 & 1lEM Original & 100 & 100 & 76.5 & 66.5 NER & 77.0 & 76.7 & 73.8 & 54.0 Question Type & 80.1 & 70.3 & 69.0 & 59.7 Stop Words & 24.2 & 69.59 & 70.4 & 65.8 Relevant Words & 60.7 & 63.4 & 64.1 & 61.7 tabularAccuracy reported across different types of questions for SQuAD Datasetadv_squadminipageminipage0.5table - format=-1.2tabularp2cmccc4cVQA Dataset Perturbations & 1lBLEU & 1lQBLEU & 1lOverall ( ) Original & 100 & 100 & 64.4 Relevant Words & 82.6 & 78.8 & 60.21 Question Type & 63.7 & 66.36 & 59.81 Stop Words & 10.8 & 42.46 & 57.37 tabularAccuracy reported across different types of questions for VQA Datasetmachine_vqaminipagetable0 subtable0.5table - format=-1.2subtabletableQuestion Generation for Question Answering SystemsAs argued earlier , one important use case of automatically generating questions from text / images is to eventually use these generated questions to train a QA system ."
PI,provider independent,TR-32380,"On the other hand , if the fee is set to the largest cost per address currently used by the RIRs ( e.g. , to the cost per address used in /48 PI allocations ) , this would render the cost of a larger block impractically high ( the cost of a /32 would be tens of millions of US if the cost per address of a /48 is used).It is challenging for the InBlock to have different cost per address depending on the size of the allocation , because this may incentivize applications for larger blocks even when not needed , resulting in address waste ( note that we do not have a complementary mechanism such as a need assessment procedure , to modulate user requests ) ."
MV,mitral valve,TR-32472,"( a ) The MV leaflets were segmented from a stack of MR images of a volunteer at early - diastole , ( b ) positions of the papillary muscle heads and the annulus ring , ( c ) reconstructed MV geometry with chordae , ( d ) a MR image showing the LV and location of the outflow tract ( AV ) and inflow tract ( MV ) , ( e ) the LV wall delineation from short and long axis MR images , ( f ) the reconstructed LV model , in which the LV model is divided into four part : the LV region bellow the LV base , the valvular region , and the inflow and outflow tracts , ( g ) the rule - based fibre orientations in the LV and the MV , and ( h ) the coupled MV - LV model ."
NN,neural network,TR-32475,"tikzpicture axis[legend style = at=(1.05,0.3),anchor = south west , legend cell align = left , xlabel = Bandwidth ( Mbps ) , ylabel = Gain in online throughput , x dir = reverse ] plot coordinates ( 40 , 74.73 ) ( 30 , 99.64 ) ( 20,149.46 ) ( 10 , 298.93 ) ; Linear Regression plot coordinates ( 40 , 12.60 ) ( 30 , 16.80 ) ( 20,25.20 ) ( 10 , 50.39 ) ; Logistic Regression plot coordinates ( 40 , 335.44 ) ( 30 , 335.44 ) ( 20,335.44 ) ( 10 , 335.44 ) ; NN plot coordinates ( 40 , 598.44 ) ( 30 , 598.44 ) ( 20,598.44 ) ( 10 , 598.44 ) ; CNN axis [ align = center , font= , xshift=2.5em , yshift=-2em ] ( title ) at ( current bounding box.north ) ; tikzpicture Throughput Gain in Low - end Networks.tab : WANGainfigurecommenttable[htb ! ]"
SL,strictly local,TR-32504,table*[t]Accuracy on Target SL Stringsets after 100 Epochstab : resultsSL4.5pttabularcccccccccc2c2Training & 2Test & 3cLSTM & 3cs - RNN & 2RPNI & & & 10 & 30 & 100 & 10 & 30 & 100 & 6SL2 & 21k & 1 & 0.772 ( 0.09 ) & 0.717 ( 0.08 ) & 0.711 ( 0.02 ) & 0.766 ( 0.11 ) & 0.761 ( 0.11 ) & 0.762 ( 0.10 ) & 0.855 & & 2 & 0.758 ( 0.09 ) & 0.696 ( 0.10 ) & 0.685 ( 0.02 ) & 0.757 ( 0.15 ) & 0.784 ( 0.17 ) & 0.768 ( 0.15 ) & 0.844 3 - 10 & 210k & 1 & 0.773 ( 0.17 ) & 0.616 ( 0.01 ) & 0.666 ( 0.01 ) & 0.682 ( 0.15 ) & 0.660 ( 0.11 ) & 0.649 ( 0.11 ) & 1.000 & & 2 & 0.772 ( 0.19 ) & 0.602 ( 0.01 ) & 0.650 ( 0.01 ) & 0.675 ( 0.16 ) & 0.650 ( 0.12 ) & 0.639 ( 0.12 ) & 1.000 3 - 10 & 2100k & 1 & 0.684 ( 0.15 ) & 0.615 ( 0.03 ) & 0.644 ( 0.01 ) & 0.700 ( 0.14 ) & 0.723 ( 0.16 ) & 0.620 ( 0.01 ) & 1.000 & & 2 & 0.669 ( 0.16 ) & 0.596 ( 0.02 ) & 0.624 ( 0.01 ) & 0.689 ( 0.16 ) & 0.718 ( 0.18 ) & 0.601 ( 0.01 ) & 1.000 6SL4 & 21k & 1 & 0.902 ( 0.01 ) & 0.907 ( 0.07 ) & 0.884 ( 0.06 ) & 0.913 ( 0.01 ) & 0.956 ( 0.01 ) & 0.968 ( 0.01 ) & 0.918 & & 2 & 0.836 ( 0.01 ) & 0.890 ( 0.04 ) & 0.901 ( 0.02 ) & 0.844 ( 0.01 ) & 0.896 ( 0.01 ) & 0.911 ( 0.01 ) & 0.813 3 - 10 & 210k & 1 & 0.840 ( 0.15 ) & 0.856 ( 0.12 ) & 0.942 ( 0.08 ) & 0.934 ( 0.12 ) & 0.982 ( 0.00 ) & 0.977 ( 0.01 ) & 0.995 & & 2 & 0.836 ( 0.16 ) & 0.852 ( 0.13 ) & 0.938 ( 0.08 ) & 0.938 ( 0.12 ) & 0.993 ( 0.00 ) & 0.991 ( 0.00 ) & 0.978 3 - 10 & 2100k & 1 & 0.975 ( 0.05 ) & 0.917 ( 0.12 ) & 0.898 ( 0.10 ) & 0.905 ( 0.16 ) & 0.989 ( 0.00 ) & 0.986 ( 0.00 ) & 1.000 & & 2 & 0.981 ( 0.04 ) & 0.923 ( 0.12 ) & 0.903 ( 0.10 ) & 0.916 ( 0.16 ) & 0.995 ( 0.00 ) & 0.994 ( 0.00 ) & 1.000 6SL8 & 21k & 1 & 0.981 ( 0.02 ) & 0.976 ( 0.04 ) & 0.995 ( 0.00 ) & 0.989 ( 0.01 ) & 0.999 ( 0.00 ) & 0.999 ( 0.00 ) & 0.991 & & 2 & 0.976 ( 0.02 ) & 0.965 ( 0.03 ) & 0.983 ( 0.01 ) & 0.991 ( 0.00 ) & 0.992 ( 0.00 ) & 0.996 ( 0.00 ) & 0.966 3 - 10 & 210k & 1 & 0.931 ( 0.09 ) & 0.979 ( 0.02 ) & 0.964 ( 0.03 ) & 0.995 ( 0.01 ) & 0.998 ( 0.00 ) & 0.997 ( 0.01 ) & 0.998 & & 2 & 0.980 ( 0.04 ) & 0.998 ( 0.00 ) & 0.999 ( 0.00 ) & 0.998 ( 0.00 ) & 0.998 ( 0.00 ) & 0.997 ( 0.01 ) & 0.994 3 - 10 & 2100k & 1 & 0.909 ( 0.11 ) & 0.864 ( 0.12 ) & 0.849 ( 0.11 ) & 0.995 ( 0.01 ) & 0.997 ( 0.00 ) & 0.997 ( 0.00 ) & 1.000 & & 2 & 0.976 ( 0.05 ) & 0.986 ( 0.02 ) & 0.980 ( 0.03 ) & 0.999 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 tabulartable *
WF,white females,TR-32538,"[ cloud , below of = gc , node distance = 2 cm ] ( m ) Race Classification ; [ cloud , below of = init , node distance = 2 cm ] ( f ) Race Classification ; [ block3 , below left of = m , node distance = 1.5 cm ] ( of ) BM Age Estimator ; [ block3 , below right of = m , node distance = 1.5 cm ] ( hf ) WM Age Estimator ; [ block3 , below left of = f , node distance = 1.5 cm ] ( om ) BF Age Estimator ; [ block3 , below right of = f , node distance = 1.5 cm ] ( hm ) WF Age Estimator ; [ - > ] ( init ) - ( pp ) ; [ - > ] ( pp ) - ( gc ) ; [ - > ] ( gc ) - node Male ( m ) ; [ - > ] ( gc.south ) - + + ( 0,-.3 cm ) - ( f ) node[near end , above left , yshift=-4pt ] Female ; [ - > ] ( m ) - ( of ) ; [ - > ] ( m ) - ( hf ) ; [ - > ] ( f ) - ( om ) ; [ - > ] ( f ) - ( hm ) ; tikzpicture adjustboxOverview of race - composite age prediction framework ."
SVM,support vector machine,TR-32693,"table1.370mm13mmtabular@llllll@ Model & ACC & PRC & RCL n - gram Language Model & 40.4 & 40.2 & 41.3 Naive Bayes & 37.9 & 37.5 & 50.2 Max Ent & 40 & 40 & 40.6 SVM & 45.2 & 44.8 & 45.4 tabularPerformance of different classifiers using lexical features , with lexicon size of 55K. ACC , PRC and RCL correspond to accuracy , precision and recall on the test set.tab:classifierstableFeature Selection Studyfab table * 1.5 170mm19 mm tabular@rrrrcrrrcrrrcrrr@ & 3c & abc & 3c & abc & 3c & abc & 3c 2 - 4 6 - 8 10 - 12 14 - 16 & & & & & & & & & & & & & & & & 38.3 & 41.9 & 39.4 & & 41.7 & 44.1 & 42.8 & & 42.9 & 45.6 & 44 & & 42.9 & 45 & 43.8 & 43.3 & 42.7 & 43.5 & & 44.6 & 44 & 44.9 & & 45.5 & 45.1 & 45.8 & & 21.9 & 20.9 & 21.9 & 45.2 & 44.8 & 45.9 & & 45.8 & 45.1 & 46.5 & & 45.2 & 44.7 & 45.8 & 44 & 43.9 & 44.7 & & 44 & 44.2 & 44.6 & & 43.9 & 44 & 44.3 Feature Combination & 44.8 & 44.2 & 45.6 & & 44.1 & 43.4 & 44.8 & & 44.8 & 44.1 & 45.4 tabular Accuracy , Precision and Recall for different senone and lexical feature based Vector Spaces ."
SO,smart object,TR-32762,"Let be a stream , initialized as empty each Let and be boolean variables , initialized as true Let and be boolean variables , initialized as false = true = true each Let = .AC Let = .RC Let be a boolean variable , initialized as true each Let be a boolean variable , initialized as false = true = each Let be a boolean variable , initialized as true = false = = jacDataFlagjacIpFlag = true = true = true = Return Let us suppose that a processor SO performs the equi - join and projection given in Example ."
ACL,agent communication language,TR-32796,"On the top communication flow showed in Figure , we have an agent sending a message to an external autonomous entity : ( i ) the agent send an ACL message addressed to a dummy agent , which is created by camel - jason component , referring to Service A ; ( ii ) the message is consumed by camel - jason component consumer ; ( iii ) the message is exchanged to the other side of the route , possibly being transformed ; and ( iv ) the message is processed by Service A component producer which prepares a service A compliance message , which will be sent to some network address to be effectively consumed by the Service A.In the other way around , on the bottom of Figure , we have : ( i ) Service B sends some data through the network reaching Service B component consumer by its network address ; ( ii ) the message is exchanged through Camel route , possibly being transformed ; ( iii ) the message is processed by camel - jason component producer which generates an ACL message ; and ( iv ) the receiver agent effectively consumes the ACL message ."
NN,nearest neighbor,TR-32808,"In fact , the interval [ 0 5 [ is divided into two groups : [ 0 2 [ and [ 2 5 [ , and two distinct sub - conditions are thus considered : Sub - condition 1 : The embedding is performed by setting to 1 for [ 0 2 [ and 0.5 for [ 2 5[. Sub - condition 2 : The embedding is performed by setting to 0.5 for [ 0 2 [ and 0.1 for [ 2 5[.The accuracy is not increased by more than 1 in all conditions when SVM is used , while the opposite for NN classifier ."
SVM,support vector machine,TR-33029,"fig : feature_selection_contribution1figurefigure[!h]tikzpicture [ thick , scale=1 , every node/.style = scale=1]box = [ rectangle , draw , thick , align = center , minimum height=10mm];arrow = [ ->,thick ] ; [ ] ( d ) ; [ box , right=5 mm of d.east,anchor=west,fill=yellow ] ( dico ) Sparse Representation;[above=5 mm of dico.north,anchor=south ] ( dicolearn ) ; [ box , right=20 mm of dico.east,anchor=west,fill=orange ] ( svm ) SVM Learning;[above=5 mm of svm.north,anchor=south ] ( target ) ; [ right=5 mm of svm.east,anchor=west ] ( dddd ) ; [ arrow ] ( d)-(dico);[arrow ] ( dicolearn)-(dico);[arrow ] ( dico)-(svm ) node[above , pos=0.5 ] ; [ arrow ] ( svm)-(dddd);[arrow ] ( target)-(svm);tikzpictureProcessing flow of SVM training over the learned dictionary and training set ."
CNN,convolutional neural network,TR-33061,"5.725 in p7 cm c c c c c c c Input of Model & B@1 & B@2 & B@3 & B@4 & M & R & C none(only seqs input ) & & & & & & & image embedding & & & & & & & detected objects and directly related terms & & & & & & & indirectly related terms & & & & & & & detected objects and directly related terms + image embedding & & & & & & & indirectly related terms + image embedding & & & & & & & detected objects and directly related terms + indirectly related terms + image embedding & & & & & & & detected objects and directly related terms + indirectly related terms + image embedding + fine tune CNN & & & & & & & ExperimentsDataWe used the Microsoft COCO captioning data set(COCO ) , the most widely used image captioning benchmark data set in our evaluations ."
SPA,simple power analysis,TR-33087,"t]Passive physical attacks and countermeasures & Parameters selection & 2001 [ HTML]FFFFFF & Bleichenbacher and restart & Parameters validation & 2003 [ HTML]FFFFFF & SPA / DPA / doubling & & [ HTML]FFFFFF & & & [ HTML]FFFFFF & SPA / timing & constant runtime & 2012 [ HTML]FFFFFF & & & 2012 [ HTML]FFFFFF & Template with lattice & Large prime finite & 2014 [ HTML]FFFFFF & CPA & & 2015 [ HTML]FFFFFF & SPA & & 2015 [ HTML]FFFFFF & SPA / DPA / ZPA & & 2017 Active attacksThere is another type of SCA attack , which uses errors to reveals some bits , called fault attacks ."
CNN,convolutional neural network,TR-33093,"parameterstabletable*[h]tabularp1.1cmp2.6cmp6.9cmp3.8 cm & Method & Description & Features 31.1cmPrevious Methods & FCM yu2014factor & A factor - based compositional embedding model by deriving sentence - level and substructure representations & word embedding , dependency parse , WordNet , name tagging 2 - 4 & CR - CNN santos2015classifying & Applying a pairwise ranking loss function over CNNs & word embedding , word position embedding 2 - 4 & Context - CNN adel2016comparing & Splitting each sentence into three parts based on query and filler positions , and apply a CNNs to each part & word embedding 61.1cmOur Methods & DepCNN & Applying CNNs to the shortest dependency path between query and filler & word embedding , dependency parse 2 - 4 & GraphCNN & DepCNN + applying CNNs to both query and filler related contextual graphs & word embedding , dependency parse 2 - 4 & GraphCNN+L & incorporating query and filler information as local attention into the GraphCNN & word embedding , dependency parse 2 - 4 & GraphCNN+G & incorporating slot type representations learned from type names as global attention into the GraphCNN & word embedding , dependency parse 2 - 4 & GraphCNN+G & incorporating slot type representations learned from external KB as global attention into the GraphCNN & word embedding , dependency parse , knowledge base 2 - 4 & GraphCNN+L+G & incorporating both local and KB based global attentions into the GraphCNN & word embedding , dependency parse , knowledge base tabularApproach Descriptions for Multi - Class Relation Classificationbaselinetable *"
FA,fractional anisotropy,TR-33102,table [ ] tabularlll 3cAmyloid Load ( PiB Positivity ) Set 1 & PiB Angular L / R & PiB Cingulum Ant L / R & PiB Cingulum Post L / R & PiB Frontal Med Orb L / R & PiB Precuneus L / R & PiB Temporal Sup L / R & PiB Temporal Mid L / R & PiB SupraMarginal L Set 2 & FA Cerebral peduncle R & FA Cerebral peduncle L & MD Corticospinal tract R & MD Corticospinal tract L & Trail - Making Test Part A Score & MD Cerebral peduncle R & PET Cingulum Post R & tabular Group difference across Amyloid Load ( PiB Positivity ) tab : wrapPIBtableC ) Graph Scan Statistics on slope differences across amyloid load positivity .
GA,global arrays,TR-33117,"MPI - parallel Multi - frame RMSD using Global Arrays alg : GA Input : size : Total number of frames assigned to each rank ga : Initialized Global Arrays xref0 : mobile group in the initial frame which will be considered as reference start stop : that tell which block of trajectory ( frames ) is assigned to each rank topology trajectory : files to read the data structure from Include : BlockRMSD ( ) from Algorithm alg : RMSD algorithmic[1 ] bsize ceil(trajectory.numberframes / size ) ga ga.create(ga.CDBL , [ bsize*size,2 ] , "" RMSD "" ) buf np.zeros([bsize*size,2 ] , dtype = float ) out BlockRMSD(topology , trajectory , xref0 , start = start , stop = stop ) ga.put(ga , out , ( start,0 ) , ( stop,2 ) ) rank = = 0 buf ga.get(ga , lo = None , hi = None ) algorithmicalgorithmMPI and Parallel HDF5sec : methods - hdf5HDF5 is a structured self - describing hierarchical data format which is the standard mechanism for storing large quantities of numerical data in Python ( http://www.hdfgroup.org/HDF5,pythonhdf5 ) ."
SMC,statistical model checking,TR-33123,"Examples of spatio - temporal model checking to compute the approximated probabilistic satisfaction can be found in bartocci2015 , CLMPV16figure[h]backgroundforegroundbackground , main , foregroundtrace=[draw , fill = blue!20 , text width=5em , text centered , minimum height=2.5em , drop shadow ] monitor=[draw , fill = green!20 , text width=5em , text centered , minimum height=2.5em , drop shadow]ann = [ above , text width=5em , text centered]wa = [ trace , text width=7em , fill = red!20 , minimum height=3em , rounded corners , drop shadow]sc = [ trace , text width=13em , fill = red!20 , minimum height=10em , rounded corners , drop shadow ] sc = [ monitor , text width=13em , fill = red!20 , minimum height=10em , rounded corners , drop shadow]2.32.5tikzpicture ( wa ) [ wa ] SIMULATOR ; ( wa.east)+(,1.5 ) node ( asr1 ) [ trace ] Trace ; ( asr1.east)+(,0 ) node ( mon1 ) [ monitor ] SSTL monitor ; ( mon1.north)+(0,1 ) node ( phi ) 0.5cm0.5 cm ; ( wa.east)+(,0.5 ) node ( asr2)[trace ] Trace ; ( asr2.east)+(,0 ) node ( mon2 ) [ monitor ] SSTL monitor ; ( wa.east)+(,-1.0 ) node ( dots)[ann ] ; ( dots.east)+(,-0.3 ) node ( dots2)[ann ] ; ( wa.east)+(,-2.0 ) node ( asr3)[trace ] Trace ; ( asr3.east)+(,0 ) node ( mon3 ) [ monitor ] SSTL monitor ; ( mon2.east)+(0.8,-0.8 ) node ( vote ) 4cm4 cm ; ( vote)+(,0 ) node ( result ) 0.5cm0.5 cm ; [ draw , - > ] ( asr1)- node [ above ] ( mon1 ) ; [ draw , - > ] ( asr2)- node [ above ] ( mon2 ) ; [ draw , - > ] ( asr3)- node [ above ] ( mon3 ) ; [ draw , - > ] ( wa.0 ) - node [ above ] ( asr2.west ) ; [ draw , - > ] ( wa.-20 ) - node [ above ] ( asr3.west ) ; [ draw , - > ] ( vote)- node [ above ] ( result.west ) ; [ draw , - > ] ( phi)- node [ above ] ( mon1 ) ; [ draw , - > ] ( wa.20)- node [ above ] ( asr1.west ) ; pgfonlayerbackground ( asr1.west - asr1.north)+(-5.7,1.3 ) node ( a ) ; ( vote.east - wa.east)+(2,-3 ) node ( c ) ; [ fill = yellow!20,rounded corners , draw = black!50 , dashed ] ( a ) rectangle ( c ) ; ( asr1.north west)+(-0.2,0.2 ) node ( a ) ; ( -3,0 ) node ( xx ) ; [ draw , - > ] ( xx ) - node [ above ] model ( wa ) ; pgfonlayertikzpictureScheme of Statistical Model Checking ( SMC ) ."
RW,random walk,TR-33190,table*[t!]table1 tabularl c c c 2*Model & Omniglot & 2 c Mini - Imagenet & 1-shot & 1-shot & 5-shot PN ren2018metalearning & 94.62 0.09 & 43.61 0.27 & 59.08 0.22 Our CPN : ( PN + VAT ) & 95.66 0.21 & 44.63 0.21 & 64.02 0.20 Our CPN : ( PN + VAT + ENT ) & 97.14 0.16 & 44.48 0.22 & 66.94 0.20 Our CPN ( PN + RW ) & 97.96 0.07 & 50.33 0.27 & 66.99 0.24 Our CPN ( final : PN+RW+VAT ) & 98.03 0.11 & 51.03 0.23 & 67.78 0.20 tabular Ablation Studytbl_ablationtable *
ECC,error correcting code,TR-33259,tabletabularlll cc cc ccType & Method & Distribution & 2cTesla K40c ( ECC on ) & 2cTesla K40c ( ECC off ) & 2cGeForce GTX 1080 6*turn90Key - onlyturn & 3*BMS & Uniform & 2.50 & & 2.48 & & 7.05 & & & 0.25-uniform & 2.64 & 1.06x & 2.61 & 1.05x & 7.36 & 1.05x & & Binomial & 2.89 & 1.15x & 2.87 & 1.16x & 7.89 & 1.11x 2 - 9 & 3*RB - sort & Uniform & 2.50 & & 3.69 & & 4.51 & & & 0.25-uniform & 2.69 & 1.08x & 3.71 & 1.00x & 4.56 & 1.01x & & Binomial & 2.80 & 1.12x & 3.72 & 1.01x & 4.95 & 1.10x 1 - 91 - 96*turn90Key - valueturn & 3*BMS & Uniform & 1.82 & & 1.81 & & 5.85 & & & 0.25-uniform & 1.99 & 1.10x & 2.00 & 1.11x & 6.71 & 1.15x & & Binomial & 2.18 & 1.20x & 2.16 & 1.20x & 7.28 & 1.24x 2 - 9 & 3*RB - sort & Uniform & 1.29 & & 1.77 & & 2.31 & & & 0.25-uniform & 1.45 & 1.13x & 1.81 & 1.02x & 2.33 & 1.01x & & Binomial & 1.48 & 1.15x & 1.93 & 1.9x & 2.52 & 1.09x tabularProcessing rate ( billion elements per second ) as well as speedup against the uniform distribution for delta - bucket multisplit with different input distributions .
ADN,activity driven networks,TR-33354,"tikzpicture customlegend[legend columns=4,legend style = at=(0.12,1.02),draw = none , column sep=2ex , line width=2 pt , legend entries = DDT , Hom GDT , Hom ADN , Het GDT , Het ADN , , , ] solid , line legend , color = blue solid , color = red solid , color = cyan color = yellow color = black only marks , mark = x only marks , mark = o only marks , mark= triangle customlegend tikzpicturesens_a.pdf sens_b.pdf1emsens_c.pdf sens_d.pdfSensitivity analysis of the contact networks with particle decay rates : A ) homogeneous GDT network , B ) homogeneous ADN network , C ) heterogeneous GDT network and D ) heterogeneous ADN networkfig : netsensfigurefigure[h ! ]"
CNN,convolutional neural network,TR-33413,"Table presents the results of our CNN - based code generation , in comparison with previous state - of - the - art models : ( 1 ) Latent Predictor Network , an enhanced sequence - to - sequence model with multiple token - level predictors ; ( 2 ) SEQ2TREE , a sequence - to - sequence model based on AST ; ( 3 ) Syntactic Neural Model , an LSTM decoder based on AST structures ; and ( 4 ) Abstract Syntax Networks , another AST - based sequence - to - sequence model , which builds two LSTMs predicting rules in the horizontal and vertical directions , respectively ."
RGB,red giant branch,TR-33463,"Due to the strong line broadening , they are among the most poorly - fitted spectra in the survey;Group 4 : Has two classes covering almost the same range of and as group 1 , RGB stars , but with higher metallicities;Group 5 : Contains three classes formed by stars from the RC and the warm end of RGB , with stellar populations from both the thin and thick disk;Group 6 : Formed of five classes composed of dwarf stars over a wide range of temperatures;Group 7 : Including five classes with peculiar stars;Group 8 : Collects 18 classes with all the outliers of the classification , less than 1 per cent of the spectra in SDSS DR12 ."
CNL,controlled natural language,TR-33477,"tabular@l@ l@ p10.9cm@class & properties & languages 1551 & c t w i & IBM 's EasyEnglish & c w s g & Special English & c w a & E - Prime & c w g & Plain Language 2132 & c s d g & CAA Phraseology , FAA Phraseology , ICAO Phraseology , PoliceSpeak , SEASPEAK 2133 & c w d i & Airbus Warning Language 2541 & f w a & AIDA 2551 & c t w d a i & ALCOGRAM , COGRAM & c t w d a & CLCM & c t w d i & ASD - STE , Avaya CE , Bull GE , CTE , CASL , CE at Douglas , DCE , General Motors GE , PACE , Sun Proof & c t w d & Wycliffe Associates ' EasyEnglish & c t w i & iCE , SMART Controlled English & c w d i & AECMA - SE , CFE , CASE , CE at Clark , CE at IBM , CE at Rockwell , EE , HELP , ILSAM , KISL , NCR FE & c w d g & Massachusetts Legislative Drafting Language & c w i & Boeing Technical English , NSE , SMART Plain English & c w & Basic English & t w d i & MCE , Oce Controlled English & t w a & KCE & t w i & CLOUT 3142 & c f w d i & SLANG & f s d i & Voice Actions 3243 & f w d a & RNLS 3333 & f w a & ClearTalk & f w i & ITA CE 3342 & f w i & CPL 3442 & c f w i & RuleSpeak , SBVR - SE 4143 & f w d a & Drafter Language , MILE Query Language 4144 & f w a & Quelo Controlled English 4153 & t f d a & PILLS Language 4243 & f w d a & Atomate Language & f w a i & Gellish English & f w a & GINO 's Guided English & f w i & CELT 4343 & f w d a & PROSPER CE & f w a & ACE 4353 & f w d a & ICONOCLAST Language 5143 & f w d a & CLEF Query Language & f w a & Ginseng 's Guided English 5144 & f w d a & Coral 's Controlled English & f w a & PathOnt CNL 5145 & f w a & Sowa 's syllogisms 5234 & f w d a i & TBNLS & f w a & OWLPath 's Guided English , SQUALL 5243 & f w a & CPE , CLIP , OWL ACE , SOS 5244 & f w d a & BioQuery - CNL , PERMIS CNL , ucsCNL & f w a & CLOnE , DL - English , E2V , Lite Natural Language , OSE & f w g & Rabbit 5333 & f w d a & CLM , ForTheL , Naproche CNL & f w a & CLCE , PNL 5343 & f w d a & Gherkin & f w a g & RECON & f w a & First Order English , PENG , PENG - D , PENG Light & f w i & iLastic Controlled English 5433 & f w a & FEtabular"
ML,machine learning,TR-33530,"Rather , they should demand to the ML - DSS designers ( and their advocates ) evidence - based validations of their systems , and adopt them only once some further information has been given about , e.g. , the size of the diagnostic improvement detected , the trade - off between specificity ( avoiding false positive , i.e. , overtesting and overtreatment ) and sensitivity ( avoiding false negatives , i.e. , failing to treat and cure ) ; between the internal ( i.e. , bias ) and external ( i.e. , variance ) validity of the model ( regarding also the extent the ML - DSS could fit multimorbid cases , instead of being excessively specialized for one disease ) ; and between its prediction power and its interpretability , that is its scrutability by doctors and lay users to understand why the ML - DSS has suggested them a certain decision over possible others and make the "" hybrid "" agency of man - and - machine more accountable towards the colleagues , the patients and their dears ."
TA,threshold algorithm,TR-33630,"iteration : continue ; Delete entries from that are inserted in for each [ 2 ] for each attribute [ 3 ] if is missing : ST - S ( , , ) Synthetic tuple with values of until IS - DOMINATED ( , , 1 , ) algorithmicalgorithmPerformance Analysissec : TASKY - performanceExpected Cost Analysis : theoremthm : expectedCostTA - SKYGiven a subspace skyline query , the expected number of sorted accesses performed by TA - SKY on an tuple boolean relation with probability of having value on attribute being is , alignm^_i=1^n iP_stop(i)alignwhere is computed using Equations eq : stopi-1 , eq : stopi-2 , and eq : stopi-3 ."
NB,naive bayes,TR-33634,"tab : suprestabularllllllllll2 * & 3cRBWH & 3cRCH & 3cGCH & P & R & F1 & P & R & F1 & P & R & F1 SVM & 0.8539 & 0.8122 & 0.8325 & 0.9366 & 0.8811 & 0.9080 & 0.9347 & 0.8810 & 0.9071 SGD & 0.8575 & 0.7329 & 0.7903 & 0.9104 & 0.8276 & 0.8670 & 0.8713 & 0.7951 & 0.8315 NB & 0.9353 & 0.7102 & 0.8074 & 0.8409 & 0.9048 & 0.8717 & 0.8049 & 0.9281 & 0.8621 RF & 0.8508 & 0.7524 & 0.7986 & 0.9182 & 0.7552 & 0.8288 & 0.8654 & 0.8210 & 0.8426 LR & 0.8872 & 0.6912 & 0.7770 & 0.7003 & 0.0725 & 0.1314 & 0.9751 & 0.5043 & 0.6648 CNN & 0.9159 & 0.9028 & 0.9085 * & 0.9370 & 0.9408 & 0.9367 * & 0.9359 & 0.9342 & 0.9335 * tabulartableSemi - supervised Learning Performancesec : semsupresTable tab : sslres presents the performance of the self - trained CNN across RBWH , RCH , and GCH ."
CNN,convolutional neural network,TR-33691,"table[H ] tabularcccccccccc & 3c80 & 3c160 & 3c320 & SRP & GMBF & CNN & SRP & GMBF & CNN & SRP & GMBF & CNN 2 * 01 & & & & & & & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & & & & & & & 2*Average & & & & & & & & & & & & & & & & & & tabular Baseline results for the SRP - PHAT strategy ( columns SRP ) ; the one invelasco2012-F ( columns GMBF ) , and the CNN trained with synthetic data without applying the fine - tuning procedure ( columns CNN ) for sequences 01 , 02 and 03 for different window sizes ."
RPE,relative pose error,TR-33835,We show the strongest results in boldtabularcY1cmY1cmY1cmY1cmY1cmY1cmY1cmY1cmY1 cm Sequence & 3cfr1-xyz & 3cfr2 - 360-hs & 3cfr3-walk - xyz 2*Method & ATE & RPE & RPE & ATE & RPE & RPE & ATE & RPE & RPE & ( m ) & ( m ) & ( ) & ( m ) & ( m ) & ( ) & ( m ) & ( m ) & ( ) LSD - SLAMengel2014lsd & 0.090 & - & - & - & - & - & 0.124 & - & - ORB - SLAMmur2015orb & 0.009 & 0.007 & 0.645 & - & - & - & 0.012 & 0.013 & 0.694 DeMoN(ftf)UZUMIDB17 & 0.183 & 0.037 & 3.612 & 0.669 & 0.032 & 3.233 & 0.279 & 0.040 & 3.174 DeMoN(1 - 10)UZUMIDB17 & 0.178 & 0.021 & 1.193 & 0.601 & 0.035 & 2.243 & 0.265 & 0.049 & 1.447 Ours(iterative ) & 0.071 & 0.024 & 1.237 & 0.461 & 0.020 & 0.736 & 0.240 & 0.026 & 0.811tabulartab : rgbd_posetablecomment
BP,backdoor poisoning,TR-33846,"a ) Target : ' Dog ' , Base : ' Cat ' ( b ) Target : ' Frog ' , Base : ' Ship ' ( c ) Target : ' Cat ' , Base : ' Car ' ( d ) Target : ' Bird ' , Base : ' Airplane ' * [ p ] Continued from Figure ; ( e ) Target : ' Deer ' , Base : ' Horse ' ( f ) Target : ' Bird ' , Base : ' Truck ' ( g ) Target : ' Horse ' , Base : ' Cat ' ( h ) Target : ' Cat ' , Base : ' Dog ' ( i ) Target : ' Dog ' , Base : ' Car ' * [ p ] Wasserstein distance between GMM clusters of input gradient first principal components with under overlay image BP attacks ."
STL,single task learning,TR-33847,tab : senttabularcccccccData & 2 & 3 & 4 & 5 & 6 & 7 Tasks & 28 & 56 & 84 & 42 & 86 & 126 tabular[c]@c@Thresholds ( Splits)tabular & 2 ( 2 ) & 2 ( 4 ) & 2 ( 6 ) & 3 ( 3 ) & 3 ( 6 ) & 3 ( 9 ) Train Size & 120 & 60 & 40 & 80 & 40 & 26 STL & 0.429 ( 0.002 ) & 0.432 ( 0.001 ) & 0.429 ( 0.002 ) & 0.400 ( 0.002 ) & 0.399 ( 0.003 ) & 0.397 ( 0.001 ) ITL & 0.433 ( 0.001 ) & 0.440 ( 0.002 ) & 0.431 ( 0.001 ) & 0.499 ( 0.001 ) & 0.486 ( 0.002 ) & 0.479 ( 0.001 ) SHAMO & 0.423 ( 0.002 ) & 0.437 ( 0.006 ) & 0.429 ( 0.002 ) & 0.498 ( 0.006 ) & 0.460 ( 0.002 ) & 0.496 ( 0.013 ) CMTL & 0.557 ( 0.016 ) & 0.436 ( 0.007 ) & 0.429 ( 0.004 ) & 0.508 ( 0.002 ) & 0.486 ( 0.002 ) & 0.476 ( 0.002 ) MTFL & 0.482 ( 0.004 ) & 0.473 ( 0.002 ) & 0.432 ( 0.007 ) & 0.522 ( 0.002 ) & 0.487 ( 0.003 ) & 0.481 ( 0.002 ) GO - MTL & 0.582 ( 0.012 ) & 0.526 ( 0.013 ) & 0.516 ( 0.007 ) & 0.587 ( 0.004 ) & 0.540 ( 0.005 ) & 0.539 ( 0.008 ) BiFactor & 0.611 ( 0.018 ) & 0.561 ( 0.013 ) & 0.598 ( 0.002 ) & 0.643 ( 0.013 ) & 0.578 ( 0.020 ) & 0.574 ( 0.052 ) TriFactor & 0.627 ( 0.008 ) & 0.588 ( 0.006 ) & 0.603 ( 0.012 ) & 0.655 ( 0.013 ) & 0.606 ( 0.020 ) & 0.632 ( 0.029 ) tabulartable*table*[!ht]Performance results ( F - measure ) on 20Newsgroups dataset .
SM,scalar multiplication,TR-34214,"ht]Improvements on different SM methods & [ HTML]FFFFFF & & Roughly 50 SM & [ HTML]FFFFFF & mbNAF & 50 SM & and [ HTML]FFFFFF & Traditional DA & PA is 1/2 PD & [ HTML]FFFFFF & & & ( 163,233 ) [ HTML]FFFFFF & Frobenius with GLV & 28.3 SM & [ HTML]FFFFFF & & 3n/4 PA and 1/2 PD & [ HTML]FFFFFF & Traditional window & Reduce PA and PD compared with NAF & ( 192,256,512 ) [ HTML]FFFFFF & & 10 better than NAF & [ HTML]FFFFFF & & 27.4 better than wNAF & [ HTML]FFFFFF & Traditional cm & & [ HTML]FFFFFF & & & [ HTML]FFFFFF & Traditional Montgomery & & ( 160,256 ) [ HTML]FFFFFF & Improved Montgomery & & and [ HTML]FFFFFF & Improved Montgomery & & [ HTML]FFFFFF & & roughly 51 SM with time=0.056ms & [ HTML]FFFFFF & & & ( 159,191,223,255 ) [ HTML]FFFFFF & SM with side information & 40 is better than traditional verification & [ HTML]FFFFFF & SM with Fibonacci & & [ HTML]FFFFFF & & Improve signature verification ( 50 ) & [ HTML]FFFFFF & & & ( 192,256 ) [ HTML]FFFFFF & SM with binary tree & Reduce SM time and complex computations & [ HTML]FFFFFF & SM with random n - cover & & Efficiency Improvement of Coordinate SystemsECDSA has to perform complex operations ; these operations consume resources in constrained devices ."
EO,eyes open,TR-34217,"R1IR = [ 90.8 ] & & & & & & EO & & & & R1IR = [ 85.6 ] 2cTask & 9lEC : resting state with eyes closed , EO : resting state with eyes open , MI : motor imagery , ERP : event related potential 2c2*Classifier & 9lANN : artificial neural networks , FDA : Fisher discriminant analysis , KNN : k - nearest neighbours , LDA : linear discriminant analysis & & 9lMAP : maximum a posteriori , CC : cross correlation , L1 ( Manhattan ) distance , L2 ( Euclidean ) distance , cosine distance tabular table*Previous protocols sec : previous_protocolTable table : comparison summarises the state - of - the - art of the existing EEG biometrics applications based on multiple data acquisition days ."
LDA,linear discriminant analysis,TR-34224,"Dist & 233 & 37 & 37 & 3743 & 1.0 & 13.7&7.3 & 98.2 LDA & 200 & 70 & 87 & 3693 & 2.3&25.9&14.1&96.1 SVM & 241 & 29 & 11 & 3769 & 0.3&10.7 & 5.5 & 99.0 tabulartableValidation including non - registered imposters sec : results4Table table : Result_which_dataset summarises the confusion matrices of both Setup - R and Setup - B with segment sizes [ ] s , classified by the minimum cosine distance , LDA , and SVM ; these correspond to Table table : Result_segment_size_SVM , panels Setup - R and Setup - B. The confusion matrices were categorised into : itemize Client matrix from dataset , Imposter matrix from dataset Imposter matrix from dataset ."
PBS,public broadcasting service,TR-34289,"This variance is clear looking at the following examples : Example 1:ORIGINAL : ( Breitbart ) EPA Chief Scott Pruitt Calls for Exit of Paris Climate AgreementCOPY : ( Truthfeed ) BREAKING Trumps EPA Chief Makes Dramatic Announcement Liberals CryingExample 2:ORIGINAL : ( AP ) Absences fitness atmosphere - new ways to track schoolsCOPY : ( PBS ) Beyond test scores here are new ways states are tracking school successExample 3:ORIGINAL : ( USA Today ) Trey Gowdy new Oversight Committee chair plans to deemphasize Russia investigationCOPY : ( Freedom Daily ) BREAKING Trey Gowdy Exposes Massive Scam By Hillary And ObamaExample 4:ORIGINAL : ( NPR ) Republicans Now Control Obamacare - Will Your Coverage ChangeCOPY : ( Salon ) Death by 1000 cuts How Republicans can still alter your health coverageNote , not all title changes are bad ."
NP,new persian,TR-34432,"NP varzidan ' sow a field ' , with v-)Elsewhere , PIr * u- MP w- NP b-:[noitemsep]PIr * uahana - ka- Phl < wh'n(k ) > , MMP < wh'n(g ) > NP bahana ' reason , pretext'PIr * uahara- MP wahar NP bahar ' spring'PIr * uata- Phl wt , MMP wd /wad/ NP bad ' bad'PIr * uat - caka- MP waccag NP baccah ' child'PIr * uana- Phl wn /wan/ NP bun ' tree'PIr * uata- Phl w't , MMP w'd NP bad ' wind'PIr * uicati- Phl / MMP wyst /wi : st/ NP bistPIr * uahia- MP wah NP bah - PIr * uacia- MP wys /we : S/ NP bes ' more'PIr * uahista- ' best ' Phl whst , MMP whyst /wahiSt/ NP bihist ; cf ."
MAPE,mean absolute percentage error,TR-34553,"llrrrr & & 4cForecasting Window ( days ) Model&Signal & 1 & 2 & 3 & mean LSTM & & 6.70 & 9.88 & 12.06 & 9.55 LSTM & & 6.64 & 9.99 & 12.40 & 9.68 LSTM & & 6.78 & 9.98 & 12.48 & 9.75 LSTM & & 6.60 & 10.46 & 12.32 & 9.79 ARIMA & & 7.30 & 10.56 & 13.10 & 10.32 * [ t ] MAPE , RMSPE , and MaxAPE results for baseline ARIMA , neural network models that rely solely on historical price , and the top performing social signal enhanced neural network models , as identified during ablation experiments ."
MR,magnetic resonance,TR-34558,"Our evaluation shows : ( I ) A sub - pixel cardiac MR image segmentation approach that , in contrast to previous CNN approaches , is robust against slice misalignment and coverage problems ; ( II ) An implicit statistical parametrisation of the left ventricular shape via NNs for pathology classification ; ( III ) An image SR technique that extends previous work and that is robust against slice misalignments ; our approach is computationally more efficient than the state - of - the - art SR - CNN model as the feature extraction is performed in the low - dimensional image space . ("
FEC,forward error correction,TR-34603,"equationI_s = 1nF_i=0^nF-1 I_s(i ) eq : MINT : avgFrameIequationequationI_s = I_sI_s + P_s + B_seq : MINT : normFrameIequationtable[!hbt ] MINT - FEC Adopted Notation small center tabularcl Notation & Meaning & Frame size average & Normalised frame size average & Frame size of the frame & Number of frames in the video sequence & Euclidean distance of a motion vector & Euclidean distance of the motion vector & Macroblock height & Macroblock width & Macroblock area & Area of the macroblock & Number of macroblock in the frame & Temporal intensity tabular tab : MINT : notation center small tableOnce all the frame sizes are normalised , it is possible to perform an exploratory analysis to cluster all frames of all video sequences together according to their sizes ."
MI,mutual information,TR-34743,"fig : attn_viz_incorrect centeringfigure*table*[]tabularclccccWord & Eng & MI & Attn - BiGRU & MTL - C & MTL - S < يسالونني > & they ask me & 1.00E-06 & 0.038055 & 0.119423 & 0.169779 < ياغالين > & you dear ( + plural ) & 9.40E-05 & 0.052512 & 0.078695 & 0.164816 < منيحاا > & beautiful ( + fem ) & 5.00E-06 & 0.031567 & 0.117903 & 0.164619 < مسساء > & evening & 4.00E-06 & 0.027612 & 0.142467 & 0.161141 < سيصادفك > & you 'll meet by chance & 0.00E+00 & 0.040002 & 0.103273 & 0.153956 < بونسوار > & good evening ( French ) & 3.00E-05 & 0.044578 & 0.10257 & 0.1479 < سيصيبك > & it 'll befall you & 2.00E-06 & 0.031588 & 0.097372 & 0.147765 < شلوونكم > & how are you ( + plural ) & 5.00E-06 & 0.0352 & 0.125699 & 0.146776 < امبيه > & I want it & 1.50E-05 & 0.041253 & 0.095131 & 0.14668 < ياعيون > & you , darling & 0.00E+00 & 0.032135 & 0.123004 & 0.142256 < ياصدفه > & what a surprise & 1.00E-06 & 0.025398 & 0.124698 & 0.142011 < انزيين > & we beautify & 1.00E-06 & 0.025684 & 0.096841 & 0.141639 < صبحهم > & good morning & 0.00E+00 & 0.03443 & 0.118689 & 0.139647 < ياسااتر > & God protect & 0.00E+00 & 0.036266 & 0.104663 & 0.138652 < سابتسم > & I 'll smile & 1.00E-06 & 0.029951 & 0.097428 & 0.138609 < تقييمكم > & what 's your evaluation & 2.00E-06 & 0.034339 & 0.091574 & 0.137949 < مسلخير > & good evening & 7.00E-06 & 0.036409 & 0.114726 & 0.137911 < االخيرر > & good ( morning / evening ) & 0.000141 & 0.058663 & 0.094799 & 0.137106 < ليلهه > & night & 9.00E-06 & 0.026944 & 0.104899 & 0.136817 < نبئ > & we want & 0.00E+00 & 0.036886 & 0.098703 & 0.136785 tabularTop 20 most highly weighted words based on average attention weights from our MTL - spec - attn ( MTL - S ) network for gender ."
AR,auto - regression,TR-34796,"tabularL1.2 cm L2 cm L2 cm L1 cm L2 cm L2 cm Item & Issue failure & Characteristic & Common measures & Common features & Used models Bearing & Outer - race , inner - race , roller , and cage failures & Raw data does not contain insightful information ; low amplitude ; high noise & Vibration , oil debris , acoustic emission & Vibration characteristic frequency , time domain statistical characteristics , metallic debris shape , size , quantity , sharp pulses and rate of development of stress - waves propagatoin & FTmcfadden2000application , mechefske1992fault , STFTrandall2005applications , WTqiu2006wavelet , EMDlei2007fault , Bispectrumyang2002third , AR Frequency Spectrawang2008fault , NNwang2001fault , yam2001intelligent , huang2007residual , HMMocak2007online , zhang2010hidden , Fuzzy logicsatish2005fuzzy , GAfeng2009ga , ARMAgalati2006application , Stochastic Modelli2000stochastic , wang2002model , PCAzhang2005integratedGear & Manufacturing error , tooth missing , tooth pitting / spall , gear crack , gear fatigue / wear & High noise ; high dynamics ; signal modulated with other factors ; gear specs need to be known & Vibration , oil debris , acoustic emission & Time domain statistical features , vibration signature frequencies , oil debris quantity and chemical analysis & FTchoy1996analysis , STFTkar2006technical , bartelmus2009vibration , WTpeng2004application , suh1999machinery , EMDloutridis2004damage , wang2007gearbox , liu2006gearbox , HHTliu2006gearbox , li2006wear , brie1997gear , NNdellomo1999helicopter , staszewski1997classification , byington2004data , khawaja2005reasoning , Fuzzy Logicdempsey2004integrating , Neuro - Fuzzy Hybrid Modelwang2004prognosis , Energy Index Analysiswang2004prognosis , KalmanFilterwu2004application , wu2004application , houser2002comparison , SVMsamanta2004gear , Autoregressive Modelwang2002autoregressive , chen2006detecting , Particle Filterorchard2009particle Shaft & Unbalance , bend , crack , misalignment , rub & Vibration signal is relatively clean and harmonic frequency components of rotating speed can indicate the defects & Vibration & Vibration characteristic frequency , time domain statistical characteristics , system modal characteristics & FTsan2007virtual , WTlingli2005research , Wigner – Ville Transforms ( WVT)kim2007comparative , EMDyang2009empirical , wu2009diagnosis , Analytical or Numerical Modelsstringer2008gear , stoisser2008comprehensive , NNmccormick1997neural , sahraoui2004friction , vijayakumar2006artificial , Fuzzy Logicjarrah2004web , Support Vector Regression ( SVR)omitaomu2006line , GAcho2010multivariate , he2001detection , ARMAwang2009autoregressive , sinha2002trend Pump & Valve impact , score , fracture , piston slap , defective bearing and revolving crank , hydraulic problem & Pump 's dynamic responses , generated by a wide range of possible impulsive sources , are very complex ; nonlinear , time - varying behavior & Vibration , pressure , acoustic emission & Vibration characteristic frequency , pressure time domain statistical characteristics , sharp pulses and rate of development of stress - waves propagation & FTha2002leakage , STFThodkiewicz2004identification , du2007condition , terao2004time , WTli2009fault , Envelop Analysisjiang2007wavelet , NNliang1988prognostics , gibiec2005prediction , engin2007prediction , Fuzzy Logicsozen2004performance , perovic2001fuzzy , Neuro - Fuzzy Hybrid Modelesen2008modelling , Rough Setli2006rmine , PCAchen2009fault tabulartabletable[htbp ! ]"
SO,smart object,TR-34813,"Let be a stream , initialized as empty each Let and be boolean variables , initialized as true Let and be boolean variables , initialized as false = true = true each Let = .AC Let = .RC Let be a boolean variable , initialized as true each Let be a boolean variable , initialized as false = true = each Let be a boolean variable , initialized as true = false = = jacDataFlagjacIpFlag = true = true = true = Return Let us suppose that a processor SO performs the equi - join and projection given in Example ."
PSO,particle swarm optimization,TR-34824,"Basically , one needs to create a main file to call PSO procedure as follows:#include "" common.h""#include "" function.h""#include "" pso.h""int main ( ) SearchSpace * s = NULL ; s = ReadSearchSpaceFromFile(""pso_model.txt "" , _ PSO _ ) ; InitializeSearchSpace(s , _ PSO _ ) ; if(CheckSearchSpace(s , _ PSO _ ) ) runPSO(s , MyFunction ) ; DestroySearchSpace(&s , _ PSO _ ) ; return 0;As one can observe , it is quite simple to execute PSO , since we need to call five main functions only : ReadSearchSpaceFromFile : it reads the model file and creates a search space ; InitializeSearchSpace : it initializes the search space ; CheckSearchSpace : it checks wether the search space is valid or not ; runPSO : it minimizes function MyFunction ; and DestroySearchSpace : it deallocates the search space ."
HPC,high performance computing,TR-34879,"Comparing LXC , Docker and Singularitytable[htp]Features of LXC , Docker and Singularitytab : comparison1.5tabularp3cmcccFeatureCOS & [ 3em]LXC & [ 3em]Docker & [ 3em]Singularity Support namespaces & Yes & Yes & Yes Support cgroups & Yes & Yes & No Support port mapping & Yes & Yes & No User escalation & Yes & Yes & No Unprivileged hardware access & No & No & Yes API for applications and developers & Yes & Yes & No Image Layering & No & Yes & Yes Support snapshots & Yes & Yes & No ( [ 1 ] ) Network interface & Host or Bridge & Bridge & Host Default filesystem & Host ( [ 2 ] ) & AuFS & ext3 Access to host filesystem & Yes & Yes & Yes Root daemon & Yes & Yes & No Registry / Repository for the images & Yes & Yes & Yes Build a container from a file & No & Yes & Yes HPC accommodations & No & No & Yes Keep modifications after restart & Yes & No & Yes tabulartableThree COS technologies have been discussed ."
ILP,inductive logic programming,TR-34886,subsec : Benchmarks0 table[h ] dNL - ILP vs dILP and Metagol in benchmark tasks tbl : BENCHMARK_ILP tabular l c c c Domain / Task & dILP & Metagol & dNL - ILP Arithmetic / Predecessor & 100 & 100 & 100 Arithmetic / Even & 100 & 100 & 100 Arithmetic / Even - Odd & 49 & 100 & 100 Arithmetic / Less than & 100 & 100 & 100 Arithmetic / Fizz & 10 & 100 & 100 Arithmetic / Buzz & 35 & 100 & 100 List / Member & 100 & 100 & 100 List / Length & 93 & 100 & 100 Family Tree / Son & 100 & 100 & 100 Family Tree / GrandParent & 97 & 100 & 100 Family Tree / Husband & 100 & 100 & 100 Family Tree / Uncle & 70 & 100 & 100 Family Tree / Relatedness & 100 & 0 & 100 Family Tree / Father & 100 & 100 & 100 Graph / Undirected Edge & 100 & 100 & 100 Graph / Adjacent to Red & 51 & 100 & 100 Graph / Two Children & 95 & 100 & 100 Graph / Graph Colouring & 95 & 100 & 100 Graph / Connectedness & 100 & 0 & 100 Graph / Cyclic & 100 & 0 & 100 tabulartableLearning decimal multiplicationWe use dNL - ILP solver for learning the predicates for decimal multiplication using only the positive and negative examples .
MRE,median recovery error,TR-34896,"0 ToDo look at distribution of attributes in the latent space If we have time : Do a figure with auto - encoder ( if we manage to sample the latent space , for instance by fitting a Gaussian to the encoded images in the latent space ) Show a picture of a few examples of Eiffel tower or london bridge in LSUN that seem to be memorize but are just samples learn from template show ( at least on some images ) that VGG , perceptual loss , L1 or L1 on Laplacian pyramid gives the same results for recovery because GLO reported some mitigated results on this ( sometimes L2 is better , sometimes pyramid : in the end , they mixed the two metrics ) give statistics about the recovery precision with LBFGS : for a given * generated * target image , the distribution of errors using a lot of random initializations ( to demonstrate that the problem is almost convex , or at least ' easy ' to optimize ) show convergence speed vs SGD or other optimization method ( because it is was is generaly used in the literature : this finding makes it possible to experiment more easily ) , for a given distorted target generated image , the average / median error vs the distorsion : : you did that on training image but it would be nice to show it first on generated images , to show robustness discuss difference LBFGS vs SGD : LBFGS is much faster to converge , not prone to gradient step setting , but can suffer from instabilities add experiments on Auto Encoder : visual recovery results , histograms , MRE values , and maybe FID if we have time ( sampling from the latent space ) add a small paragraph to explain experimental settings , i.e. that we reproduce several architectures from the literature and trained again on splitting , ... tell somewhere the difference ( I think in the intro ) between over - fitting and verbatim memorization , for which we provide a solid definition : add more experiments on LSUN : show histograms with PG - GAN without GAP , show recovery failure results add experiments on MNIST , CIFAR ?"
CNN,convolutional neural network,TR-35151,table*[ht]Accuracy of 's modules with different classifierstab : classifiertabularccccc2*Classifier Type & 4cAccuracy 2 - 5 & BF Multi - classifier & AF Multi - classifier & Profile Image CNN & Overall Decision Tree & 0.721 & 0.618 & 5 * 0.790 & 0.721 1 - 3 5 - 5 SVM & 0.739 & 0.352 & & 0.800 1 - 3 5 - 5 AdaBoost & 0.790 & 0.704 & & 0.850 1 - 3 5 - 5 GradientBoosting & 0.816 & 0.738 & & 0.842 1 - 3 5 - 5 Random Forest & 0.796 & 0.708 & & 0.899 tabulartable*table*[ht]performance with different feature setstab : featuretabularl c c c c c c c c c c2*Feature Set Description & 3cMale & 3cFemale & 3cBrand & 2*Acc 2 - 10 & R & P & F1 & R & P & F1 & R & P & F1 0 .
CNN,convolutional neural network,TR-35168,table[H ] tabularccccccc & 2c80 & 2c160 & 2c320 & GMBF & CNNf15 + 11 & GMBF & CNNf15 + 11 & GMBF & CNNf15 + 11 2 * 01 & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & 2*Average & & & & & & & & & & & & tabular Relative improvements over SRP - PHAT for the strategy invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with sequences 15 and 11 ( columns CNNf15 + 11 ) tab : baselineResults+ft15 + 11table
ECC,error correcting code,TR-35171,tabletabularl c ccc ccc ccc & & 3cK40c ( ECC on ) & 3cK40c ( ECC off ) & 3cGeFroce GTX 1080 ( r)3 - 5 6 - 8 ( l)9 - 11Method & & time & throughput & speedup & time & throughput & speedup & time & throughput & speedup 5*Our sort ( key - only ) & 4 & 25.84 & 1.299 & 1.01x & 26.00 & 1.290 & 0.75x & 14.11 & 2.368 & 0.70x & 5 & 24.82 & 1.35 & 1.05x & 24.81 & 1.352 & 0.78x & 12.65 & 2.654 & 0.78x & 6 & 26.18 & 1.282 & 0.99x & 26.41 & 1.271 & 0.74x & 11.44 & 2.933 & 0.86x & 7 & 34.59 & 0.970 & 0.75x & 34.93 & 0.961 & 0.56x & 11.21 & 2.994 & 0.88x & 8 & 50.17 & 0.669 & 0.52x & 50.58 & 0.663 & 0.38x & 14.59 & 2.300 & 0.68x 2cCUB ( key - only ) & 25.99 & 1.291 & - & 19.42 & 1.728 & - & 9.88 & 3.397 & - 5*Our sort ( key - value ) & 4 & 36.86 & 0.910 & 1.18x & 35.17 & 0.954 & 0.81x & 14.11 & 1.441 & 0.75x & 5 & 34.90 & 0.962 & 1.25x & 34.32 & 0.978 & 0.83x & 12.65 & 1.619 & 0.85x & 6 & 34.79 & 0.97 & 1.26x & 34.70 & 0.967 & 0.82x & 18.01 & 1.852 & 0.97x & 7 & 43.90 & 0.764 & 1.00x & 44.59 & 0.753 & 0.64x & 15.97 & 2.101 & 1.10x & 8 & 67.47 & 0.497 & 0.65x & 67.94 & 0.494 & 0.42x & 18.01 & 1.863 & 0.97x 2cCUB ( key - value ) & 43.73 & 0.767 & - & 28.58 & 1.174 & - & 17.56 & 1.911 & - tabularOur Multisplit - based radix sort is compared to CUB .
AI,artificial intelligence,TR-35173,"author = Ramilli , Marco and Prandini , Marco , doi = 10.1109/CCNC.2009.4784952,file = : home / niu/.local / share / data / Mendeley Ltd./Mendeley Desktop / Downloaded / Ramilli , Prandini - 2009 - Comment spam injection made easy.pdf:pdf,isbn = 9781424423095,journal = 2009 6th IEEE Consumer Communications and Networking Conference , CCNC 2009,keywords = Comment spam , Social TV , Social networks , pages = 1 - 5,title = Comment spam injection made easy , year = 2009@articleRao12 , Author = Rao , Justin M. and Reiley , David H. , Title = The Economics of Spam , Journal = Journal of Economic Perspectives , Volume = 26 , Number = 3 , Year = 2012 , Pages = 87 - 110 , DOI = 10.1257/jep.26.3.87 , URL = http://www.aeaweb.org/articles.php?doi=10.1257/jep.26.3.87@miscRivest92 , author=""R. Rivest "" , title=""The MD5 Message - Digest Algorithm "" , series=""Request for Comments "" , number=""1321 "" , howpublished=""RFC 1321 ( Informational ) "" , publisher=""IETF "" , organization=""Internet Engineering Task Force "" , year=1992 , month = apr , note=""Updated by RFC 6151 "" , url=""http://www.ietf.org / rfc / rfc1321.txt"",@articleRobinson01 , title = Can Hard AI Problems Foil Internet Interlopers ? ,"
RDF,resource description framework,TR-35181,"C1 & CGP & & & & & 2 & 2C2 & CGP & & & & & 1 & 1C3 & CGP & & & & & 1 & 1 F1 & CONDITION & ( 1 ) & & & & 3 & 3F2 & CONDITION & ( 2 ) & & & & 3 & 3F3 & CONDITION & ( 1 ) & & & & 2 & 1L1 & RESTRICTION & ( 1 ) & & & & 4 & 2L2 & RESTRICTION & & & & & 2 & 2L3 & RESTRICTION & & & & & 2 & 2G1 & GROUP BY & & & & & 2 & 2G2 & GROUP BY & ( 1 ) & & & & 6 & 2G3 & GROUP BY & & & & & 1 & 2Gc1 & GROUP COUNT & & & & & 3 & 2 Gc2 & GROUP COUNT & & & & & 2 & 2Gc3 & GROUP COUNT & & & & & 1 & 2O1 & ORDER BY & & & & & 1 & 1O2 & ORDER BY & ( 1 ) & & & & 4 & 3O3 & ORDER BY & & & & & 1 & 1U1 & UNION & ( 2 ) & & & & 8 & 1U2 & UNION & ( 2 ) & & & & 6 & 2 U3 & UNION & ( 2 ) & & & & 4 & 1Op1 & OPTIONAL & ( 1 ) & & & & 3 & 3Op2 & OPTIONAL & & & & & 6 & 2 Op3 & OPTIONAL & ( 2 ) & & & & 8 & 3 M1 & MIXED & & & & & 3 & 2 M2 & MIXED & & & & & 2 & 2 M3 & MIXED & & & & & 4 & 2S1 & STAR & ( 1 ) & & & & 12 & 11S2 & STAR & ( 1 ) & & & & 5 & 4 S3 & STAR & ( 1 ) & & & & 10 & 9 TOTAL & 30 Q. & - & - & - & - & - & - tabulartable Experimental Setupsec : Exp_SetupWe selected the following DMSs for the experiments : RDF DMS : Openlink Virtuoso virtuoso [ v7.2.4 ] , JenaTDB(Apache Jena TDB ( https://jena.apache.org/documentation/tdb/index.html ) ) [ v3.2.0 ] , 4Store harris20094store [ v1.1.5];Graph DMS : TinkerGraph home2016apache [ v3.2.3 ] , Neo4J(Neo4J ( https://neo4j.com/ ) ) [ v1.9.6 ] , Sparksee(Sparksee - formerly DEX ( http://sparsity-technologies.com/sparksee ) ) [ v5.1]. All the experiments were performed on a machine with the following configuration : CPU : Intel Xeon CPU E5 - 2660 v3 ( 20 cores @2.60GHz ) , RAM : 128 GB DDR3 , HDD : 512 GB SSD , OS : Linux 4.2-generic ."
CI,constructive interference,TR-35364,"By exploiting the S - procedure in Lemma 1 , ( 56 ) can be expanded and converted into a LMI as shown below 0 , j , We note that by using the fact that , ( 57 ) can always be guaranteed by the following constraintwhose robust formulation is given by Futhermore , we define and , therefore , the constraint ( 60 ) can be written in terms of real valued numbers as Therefore , the robust optimisation problem based on CI isNote that problem is a convex problem and thus can be optimally solved using standard convex softwares like CVX ."
ECC,error correcting code,TR-35434,tabularlll cccccccc & & & 8cNumber of buckets ( bins ) 4 - 11Example & GPU & Method & 2 & 4 & 8 & 16 & 32 & 64 & 128 & 256 9*turn90Even Histogramturn & 3*Tesla K40c ( ECC on ) & Our Histogram & 45.3 & 42.5 & 45.4 & 37.8 & 32.1 & 26.5 & 24.2 & 18.8 & & CUB & 13.7 & 14.9 & 16.9 & 19.1 & 21.4 & 21.8 & 20.8 & 19.5 & & Speedup vs. CUB & 3.30x & 2.86x & 2.69x & 1.98x & 1.50x & 1.21x & 1.16x & 0.96x 2 - 11 & 3*Tesla K40c ( ECC off ) & Our Histogram & 53.0 & 47.2 & 48.1 & 38.3 & 32.3 & 26.5 & 24.0 & 18.7 & & CUB & 13.6 & 14.7 & 16.7 & 18.9 & 21.3 & 21.8 & 20.7 & 19.5 & & Speedup vs. CUB & 3.90x & 3.20x & 2.88x & 2.03x & 1.52x & 1.21x & 1.16x & 0.96x 2 - 11 & 3*GeForce GTX 1080 & Our Histogram & 61.0 & 61.1 & 60.9 & 60.7 & 60.2 & 45.2 & 59.6 & 52.7 & & CUB & 60.5 & 60.7 & 60.5 & 60.7 & 61.1 & 60.6 & 60.3 & 60.9 & & Speedup vs. CUB & 1.01x & 1.01x & 1.01x & 1.00x & 0.98x & 0.75x & 0.99x & 0.87x 9*turn90Range Histogramturn & 3*Tesla K40c ( ECC on ) & Our Histogram & 28.0 & 22.1 & 18.4 & 14.6 & 11.9 & 9.0 & 7.7 & 7.3 & & CUB & 8.7 & 6.8 & 6.2 & 5.8 & 5.7 & 5.5 & 5.2 & 4.8 & & Speedup vs. CUB & 3.21x & 3.26x & 2.96x & 2.51x & 2.09x & 1.63x & 1.50x & 1.51x 2 - 11 & 3*Tesla K40c ( ECC off ) & Our Histogram & 27.6 & 22.2 & 17.8 & 14.5 & 11.7 & 8.7 & 7.6 & 7.1 & & CUB & 8.4 & 6.8 & 6.2 & 5.8 & 5.6 & 5.4 & 5.1 & 4.8 & & Speedup vs. CUB & 3.29x & 3.28x & 2.89x & 2.50x & 2.10x & 1.61x & 1.50x & 1.50x 2 - 11 & 3*GeForce GTX 1080 & Our Histogram & 56.7 & 51.4 & 45.4 & 39.8 & 33.9 & 28.4 & 24.8 & 20.0 & & CUB & 42.4 & 35.2 & 30.9 & 27.1 & 24.4 & 22.3 & 19.3 & 14.8 & & Speedup vs. CUB & 1.34x & 1.46x & 1.47x & 1.47x & 1.39x & 1.28x & 1.29x & 1.35x tabularHistogram computation over two examples of even bins ( Even Histogram ) and customized bins ( Range Histogram ) .
PAP,process arrival pattern,TR-35462,"The future works cover the following topics : evaluation of the method for a wider range of interconnecting network speeds and larger number of nodes using a simulation tool e.g. , expansion of the method for other collective communication algorithms , e.g. all - gather , a framework for automatic PAP detection and proper algorithm selection , e.g. providing a regular ring for balanced PAPs and PRR for imbalanced ones , introduction of the presented PAT estimation method for other purposes e.g. asynchronous SDG training or deadlock and race detection in distributed programs , deployment of the solution in a production environment ."
NN,neural network,TR-35463,"& & & & & & Prot1 & NN & 10000000 & [ 0.00171600 0.0033500 ] & 0 & -6 & 0000000000 & 0000111111 & [ 0.03141600 0.9035500 ] & [ 0.1117600 0.0223500 ] regulator & NN & 00000000 & [ 0.99121600 0.0233500 ] & 1 & -5 & 0000000001 & 0000011111 & [ 0.77171600 0.4858500 ] & [ 0.83191600 between & IN & 00100000 & [ 0.25191600 0.1739500 ] & 2 & -4 & 0000000011 & 0000001111 & [ 0.33171600 & [ 0.58961600 0.7189200 ] Interaction & NN & 10000000 & [ 0.17171219 0.7583350 ] & 3 & -3 & 0000000111 & 0000000111 & [ 0.75171600 0.5533500 ] & [ 0.99171600 0.7633500 ] and & CC & 00100000 & [ 0.17001600 0.3030350 ] & 4 & -2 & 0000001111 & 0000000011 & [ 0.78117600 & [ 0.72171600 0.1233500 ] repression & NN & 10000000 & [ 0.17858500 0.8835300 ] & 5 & -1 & 0000011111 & 0000000001 & [ 0.45897600 & [ 0.7800100 0.3311500 ] Prot2 & NN & 10000000 & [ 0.98581600 0.0263500 ] & 6 & 0 & 0000111111 & 0000000000 & [ 0.77451600 0.8985500 ] & [ 0.1745100 0.3323500 ] Feature Encoding for sentence "" Interaction between cell cycle regulator , Prot1 , and Prot2 mediates repression of HIV-1 gene transcription . "" ."
SVM,support vector machine,TR-35529,Results overviewapp : results_overviewtable[h ] tabularl c c l Approach & Acc & & Dataset Transfer Learning RNN & - & 67.8 & mohammad16 Bidirectional LSTM & - & 58.3 & mohammad16 Branch LSTM & - & 43.4 & derczynski17 CNN & - & 53.6 & derczynski17 SVM & 53.0 & - & derczynski17 J48 & 79.02 & - & derczynski17 MLP & - & 60.4 & fakenewschallenge MLP & - & 58.3 & fakenewschallenge CNN and Tree ensemble & - & 58.2 & fakenewschallenge Bayes & 94.1 & 92.5 & qazvinian11 Hidden Markov Models & - & 80.4 & zubiaga16 TriFN & - & 87.0 & [ BuzzFeed]TriFN TriFN & - & 88.0 & [ Politifact]TriFN tabular Overview of performance results for the different approaches for stance classification(top ) and fake news detection(bottom ) tab : performance_overviewtableThesis planapp : thesis_plantable[h ! ]
NB,naive bayes,TR-35567,"In the bag of words method , we extract the features by using presence / absence of words ( binary ) term frequency of the words ( tf ) inverse document frequency of words ( idf ) product of term frequency and inverse document frequency of words ( tf - idf ) We also evaluate some of the recent state of the art methods for text classification on the above datasets naive bayes features in bag of words followed by Logistic Regression ( NB - LR ) inversion of distributed language representation ( W2V inversion ) ( We use the code available at https://github.com/TaddyLab/deepir which builds on top of gensim toolkit ) Convolutional Neural Networks for text categorization ( CNN ) Paragraph Vectors - Distributed Bag of Words Model ( PV - DBOW)Class Vector method based scoring and feature extraction ."
MAC,medium access control,TR-35581,"User registration is performed once , namely , the user does not need to complete the registration protocol the next times ( only the login protocol ) , the registration information has been kept in the servers until the revocation protocol and deletion the user security parameters such as pseudonyms , MAC address and , this protocol accomplishes the following steps ( Figure shows registration , and login protocol and Figure shows login protocol ) : Registration and login protocol Login protocol side : The user enters ( such as his / her name ) , ( such as medical centre name ) , and for registration and login while only entering , , and for the login protocol the next times to the client ( ) application ."
RPE,relative pose error,TR-35587,tabularcY1.35cmY1.35cmY1.35cmY1.35cmY1.35cmY1.35 cm Sequence & 3c09 & 3c10 1*Method & ATE(m ) & RPE(m ) & RPE ( ) & ATE(m ) & RPE(m ) & RPE ( ) ORB - SLAM(no - loop)mur2015orb & 57.57 & 0.040 & 0.103 & 8.090 & 0.033 & 0.105 ORB - SLAM(full)mur2015orb & 9.104 & 0.056 & 0.084 & 7.349 & 0.031 & 0.100 SfM - learner(5)zhou2017unsupervised & 58.31 & 0.077 & 0.803 & 31.75 & 0.069 & 1.242 SfM - learner(1)zhou2017unsupervised & 81.09 & 0.050 & 0.976 & 75.89 & 0.045 & 1.599 Ours(fully connected ) & 41.50 & 0.087 & 0.387 & 29.29 & 0.081 & 0.486 Ours(full ) & 16.55 & 0.047 & 0.128 & 9.846 & 0.039 & 0.138 tabulartab : kitti_posetabletable[h ! ]
CT,class table,TR-35599,"figure*[t]flalign * & addMs(C , M , CT ) = C.m : C C ' CT & removeM ( C , C ' m(C x ) return e , CR ) = CR'_S & where CR ' = ( T.m : TT',cond ( T C ) ) ( T.m : TT ' , ) CR & ( CR(T.m : TT ' , ) ) & S = ( T ' = C ' if T= C ) ( T = C if T= C ) ( T.m : TT ' , ) CR & removeMs(C , M , CR ) = CR'_S & where CR ' = CR_m ( C ' m(C x ) return e ) M & removeM(C , C ' m(C e ) return e , CR ) = CR_m_S_m & S = S_m ( C ' m(C x ) return e ) M & removeM(C , C ' m(C x ) return e , CR ) = CR_m_S_m flalign*flalign*&addExt(class C extends D , CT)= ( C extends D)CT & removeExt(class C extends D , CR ) = CR'_S & where CR'= aligned[t]&(T.extends : T',cond ( T C ) ) ( T.extends : T ' , cond ) CR & ( T.m : TT ' , cond(TC ) ) & ( D.m : T T ' , cond ( T = C))(T.m : TT ' , cond ) CR & ( T.m : TT ' , cond ( TC))_opt & ( D.m : T T ' , cond ( T = C))_opt & ( T.m : TT ' , cond)_opt CR & ( T.f : T ' , cond ( TC ) ) ( D.f : T ' , cond ( T = C ) ) & ( T.f : T ' , cond ) CR & S = ( T ' = D if T= C ) ( T.extends : T ' , ) CR alignedflalign*Add and remove operations of method and extends clauses ."
FA,fractional anisotropy,TR-35640,multicols2enumerateMiddle cerebellar pedunclePontine crossing tract ( a part of MCP)Genu of corpus callosumBody of corpus callosumSplenium of corpus callosumFornix ( column and body of fornix)Corticospinal tract R / LMedial lemniscus R / LInferior cerebellar peduncle R / LSuperior cerebellar peduncle R / LCerebral peduncle R / LAnterior limb of internal capsule R / LPosterior limb of internal capsule R / LRetrolenticular part of internal capsule R / LAnterior corona radiata R / LSuperior corona radiata R / LPosterior corona radiata R / LPosterior thalamic radiation ( include optic radiation ) R / LSagittal stratum ( include inferior longitidinal fasciculus and inferior fronto - occipital fasciculus ) R / LExternal capsule R / LCingulum ( cingulate gyrus ) R / LCingulum ( hippocampus ) R / LFornix ( cres ) / Stria terminalis ( can not be resolved with current resolution ) R / LSuperior longitudinal fasciculus R / LSuperior fronto - occipital fasciculus ( could be a part of anterior internal capsule ) R / LUncinate fasciculus R / LTapetum R / Lenumeratemulticolsfigure * majorDTI_FA / screenshot0001.png majorDTI_FA / screenshot0002.png majorDTI_FA / screenshot0003.png fig : majorDTI17 major DTI fiber bundles measured using Fractional Anisotropy ( FA ) .
AIDA,"atomic , independent , declarative , and absolute",TR-35751,"[ max=35,step=10,width=10cm , scale=0.8][label = about as useful ( 48.4 ) 2015,color = green!50]29[label = helpful ( 70.3 ) 2016,color = green!50]31[label=2017,color = green!50]30[label=2015,color = blue!30]8[label = maybe helpful ( 28.1 ) 2016,color = blue!30]17[label=2017,color = blue!30]11[label=2015,color = red!50]0[label = not helpful 0(1.6 ) 2016,color = red!50]1[label=2017,color = red!50]1[max=35,step=10,width=10cm , scale=0.8][label=2015,color = green!50]19[label = more useful ( 45.3 ) 2016,color = green!50]16[label=2017,color = green!50]23[label=2015,color = blue!30]15[label = about as useful ( 48.4 ) 2016,color = blue!30]28[label=2017,color = blue!30]19[label=2015,color = red!50]3[label = less useful 0(6.3 ) 2016,color = red!50]5[label=2017,color = red!50]0number of participantsResponses from the participants on whether AIDA sentences were helpful ( top ) and on how AIDA sentences compared to classical text summaries ( bottom)We see that 70.3 of all participating students thought the AIDA sentences were helpful to understand and remember the content of the papers ."
SP,strictly piecewise,TR-35753,table*[t]Accuracy on Target SP Stringsets after 100 Epochstab : resultsSP4.5pttabularcccccccccc2c2Training & 2Test & 3cLSTM & 3cs - RNN & 2RPNI & & & 10 & 30 & 100 & 10 & 30 & 100 & 6SP2 & 21k & 1 & 0.847 ( 0.06 ) & 0.935 ( 0.07 ) & 0.952 ( 0.07 ) & 0.910 ( 0.05 ) & 0.999 ( 0.00 ) & 0.999 ( 0.00 ) & 1.000 & & 2 & 0.873 ( 0.10 ) & 0.951 ( 0.08 ) & 0.947 ( 0.08 ) & 0.976 ( 0.01 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 3 - 10 & 210k & 1 & 0.734 ( 0.12 ) & 0.673 ( 0.04 ) & 0.720 ( 0.03 ) & 0.937 ( 0.13 ) & 0.960 ( 0.08 ) & 0.972 ( 0.06 ) & 1.000 & & 2 & 0.723 ( 0.12 ) & 0.656 ( 0.04 ) & 0.701 ( 0.03 ) & 0.934 ( 0.13 ) & 0.960 ( 0.08 ) & 0.972 ( 0.06 ) & 1.000 3 - 10 & 2100k & 1 & 0.680 ( 0.08 ) & 0.707 ( 0.10 ) & 0.732 ( 0.07 ) & 0.974 ( 0.07 ) & 0.974 ( 0.07 ) & 0.982 ( 0.04 ) & 1.000 & & 2 & 0.665 ( 0.09 ) & 0.697 ( 0.12 ) & 0.716 ( 0.08 ) & 0.977 ( 0.07 ) & 0.974 ( 0.08 ) & 0.986 ( 0.04 ) & 1.000 6SP4 & 21k & 1 & 0.883 ( 0.06 ) & 0.885 ( 0.08 ) & 0.775 ( 0.05 ) & 0.890 ( 0.05 ) & 0.969 ( 0.02 ) & 0.988 ( 0.01 ) & 1.000 & & 2 & 0.943 ( 0.04 ) & 0.840 ( 0.09 ) & 0.749 ( 0.06 ) & 0.885 ( 0.06 ) & 0.975 ( 0.01 ) & 0.985 ( 0.01 ) & 1.000 3 - 10 & 210k & 1 & 0.862 ( 0.13 ) & 0.880 ( 0.14 ) & 0.853 ( 0.08 ) & 0.696 ( 0.05 ) & 0.840 ( 0.15 ) & 0.903 ( 0.12 ) & 1.000 & & 2 & 0.862 ( 0.14 ) & 0.877 ( 0.15 ) & 0.843 ( 0.08 ) & 0.686 ( 0.05 ) & 0.841 ( 0.16 ) & 0.900 ( 0.12 ) & 1.000 3 - 10 & 2100k & 1 & 0.842 ( 0.13 ) & 0.791 ( 0.14 ) & 0.720 ( 0.09 ) & 0.884 ( 0.14 ) & 0.828 ( 0.17 ) & 0.895 ( 0.12 ) & 1.000 & & 2 & 0.831 ( 0.13 ) & 0.785 ( 0.13 ) & 0.716 ( 0.08 ) & 0.900 ( 0.15 ) & 0.827 ( 0.17 ) & 0.902 ( 0.13 ) & 1.000 6SP8 & 21k & 1 & 0.844 ( 0.04 ) & 0.863 ( 0.05 ) & 0.901 ( 0.01 ) & 0.871 ( 0.01 ) & 0.885 ( 0.02 ) & 0.878 ( 0.01 ) & 0.817 & & 2 & 0.699 ( 0.08 ) & 0.627 ( 0.05 ) & 0.692 ( 0.03 ) & 0.719 ( 0.02 ) & 0.663 ( 0.06 ) & 0.668 ( 0.03 ) & 0.587 3 - 10 & 210k & 1 & 0.827 ( 0.15 ) & 0.798 ( 0.11 ) & 0.804 ( 0.04 ) & 0.818 ( 0.12 ) & 0.856 ( 0.10 ) & 0.979 ( 0.02 ) & 0.873 & & 2 & 0.654 ( 0.11 ) & 0.672 ( 0.10 ) & 0.638 ( 0.05 ) & 0.566 ( 0.05 ) & 0.646 ( 0.05 ) & 0.811 ( 0.08 ) & 0.634 3 - 10 & 2100k & 1 & 0.880 ( 0.10 ) & 0.927 ( 0.08 ) & 0.904 ( 0.08 ) & 0.893 ( 0.14 ) & 0.978 ( 0.04 ) & 0.988 ( 0.01 ) & 1.000 & & 2 & 0.760 ( 0.12 ) & 0.802 ( 0.13 ) & 0.739 ( 0.09 ) & 0.825 ( 0.15 ) & 0.909 ( 0.11 ) & 0.907 ( 0.09 ) & 1.000 tabulartable *
CC,connected caveman,TR-35960,"[ ] Mean and Std Dev of Under the Null [ ER , 35 runs , 12 clust ] [ ER , 100 runs , 12 clust ] [ ER , 1000 runs , 12 clust ] [ ER , 35 runs , 24 clust ] [ ER , 100 runs , 24 clust ] [ ER , 1000 runs , 24 clust ] [ CC , 35 runs , 12 clust ] [ CC , 100 runs , 12 clust ] [ CC , 1000 runs , 12 clust ] [ CC , 35 runs , 24 clust ] [ CC , 100 runs , 24 clust ] [ CC , 1000 runs , 24 clust ] Null Distributions of , Number of Runs and Number of Clusters [ ER , 35 runs , 12 clust ] [ ER , 100 runs , 12 clust ] [ ER , 1000 runs , 12 clust ] [ ER , 35 runs , 24 clust ] [ ER , 100 runs , 24 clust ] [ ER , 1000 runs , 24 clust ] [ CC , 35 runs , 12 clust ] [ CC , 100 runs , 12 clust ] [ CC , 1000 runs , 12 clust ] [ CC , 35 runs , 24 clust ] [ CC , 100 runs , 24 clust ] [ CC , 1000 runs , 24 clust ] Null Distributions of in Red , Number of Runs and Number of Clusters , Gaussian with Same Mean and Std Dev in BlueThe histograms in Figure reveal that is roughly symmetrically distributed about its mean of zero , even with a small number of runs ."
PRR,pre - reduced ring,TR-36068,"The future works cover the following topics : evaluation of the method for a wider range of interconnecting network speeds and larger number of nodes using a simulation tool e.g. , expansion of the method for other collective communication algorithms , e.g. all - gather , a framework for automatic PAP detection and proper algorithm selection , e.g. providing a regular ring for balanced PAPs and PRR for imbalanced ones , introduction of the presented PAT estimation method for other purposes e.g. asynchronous SDG training or deadlock and race detection in distributed programs , deployment of the solution in a production environment ."
CNN,convolutional neural network,TR-36075,Experiments and Results * [ t ] Xcccccc 7cSubtask A - Technical Domain 2*Experiment & 3cValidation & 3cTest & 1cPrecision & 1cRecall & 1c & 1cPrecision & 1cRecall & 1c Organizer Baseline&58.72 & 93.24 & 72.06 & 15.69 & 91.95 & 26.80 DAN + glove&68.512.43 & 87.305.00 & 76.691.06 & 25.403.56 & 84.609.87 & 38.843.10 DAN + bert&76.061.31 & 90.271.71 & 82.550.50 & 45.804.49 & 90.801.75 & 60.823.99 DAN + bert w/o upsampling&79.042.67 & 83.382.73 & 81.110.68 & 55.066.36 & 83.682.75 & 66.284.28 CNN + bert&80.344.21 & 89.934.23 & 84.760.52 & 50.346.70 & 91.722.55 & 64.814.86 CNN + bert w/o upsampling&83.223.01 & 84.733.86 & 83.900.70 & 58.985.41 & 88.051.63 & 70.584.24 CNN + bert + tritrain*&83.061.96 & 89.191.88 & 86.000.35 & 52.892.69 & 90.802.02 & 66.811.90 7cSubtask B - Hotel Reviews Domain 2*Experiment & 3cValidation & 3cTest & 1cPrecision & 1cRecall & 1c & 1cPrecision & 1cRecall & 1c Organizer Baseline&72.84 & 81.68 & 77.01 & 68.86 & 78.16 & 73.21 DAN + glove&82.004.25 & 52.979.25 & 64.015.75 & 73.323.50 & 46.097.21 & 56.354.71 DAN + bert&89.752.79 & 65.748.71 & 75.655.10 & 78.904.03 & 64.208.77 & 70.494.09 DAN + bert w/o upsampling&94.261.87 & 31.735.73 & 47.316.27 & 87.983.41 & 31.097.17 & 45.627.47 CNN + bert&93.771.34 & 51.886.88 & 66.655.68 & 90.172.45 & 50.348.71 & 64.316.72 CNN + bert w/o upsampling&93.941.36 & 45.997.59 & 61.536.73 & 89.754.41 & 44.089.38 & 58.667.79 CNN + bert + tritrain*&91.912.06 & 88.322.05 & 90.050.76 & 81.261.63 & 83.161.40 & 82.191.03 CNN + bert + tritrain&88.090.62&87.130.38&87.610.42&78.015.42&86.673.96&81.982.05 Performance metrics of different models on validation and test sets of both subtasks .
UD,universal dependencies,TR-36110,"enumerateWe compare our results to four baselines : enumerate the most frequent baseline per word ( MFC ) , where we assign the most frequent tag for a word in the training data to that word in the test data , and unseen words get the global majority tag ; the trigram statistic based TNT tagger offers a slightly tougher baselinetnt ; the Bi - lstm baseline , running the off - the - shelf state - of - the - art POS tagger for the UD datasetplank:2016 ( using default parameters with pre - trained Polyglot embeddings ) ; we use a baseline consisting of running our own system with only a Bi - gru using word representations ( ) , with pre - trained Polyglot embeddings ."
TBS,terrestrial base station,TR-36552,"The achievable FH data rate of user served from a BS , where , over RB can be given as : where is the BS transmitted power allocated to RB , is the noise power , and is the inter - cell interference at the user caused by closest BS ( no intra - cell interference on the downlink direction between different tiers is assumed ) and expressed as follows : where is representing the exclusivity of the TBS and RB allocation : , if RB of nearby station is allocated to another user from TBS , and , otherwise ."
SL,strictly local,TR-36633,table*[t]Accuracy on Target SL Stringsets Early Stoppingtab : resultsSLES4.5pttabularcccccccccc2c2Training & 2Test & 3cLSTM & 3cs - RNN & 2RPNI & & & 10 & 30 & 100 & 10 & 30 & 100 & 6SL2 & 21k & 1 & 0.818 ( 0.03 ) & 0.843 ( 0.05 ) & 0.923 ( 0.02 ) & 0.848 ( 0.06 ) & 0.904 ( 0.03 ) & 0.930 ( 0.03 ) & 0.855 & & 2 & 0.780 ( 0.06 ) & 0.820 ( 0.07 ) & 0.905 ( 0.04 ) & 0.871 ( 0.10 ) & 0.980 ( 0.02 ) & 0.992 ( 0.01 ) & 0.844 3 - 10 & 210k & 1 & 0.925 ( 0.07 ) & 0.851 ( 0.04 ) & 0.875 ( 0.04 ) & 0.936 ( 0.05 ) & 0.884 ( 0.07 ) & 0.729 ( 0.12 ) & 1.000 & & 2 & 0.919 ( 0.09 ) & 0.836 ( 0.06 ) & 0.835 ( 0.10 ) & 0.964 ( 0.07 ) & 0.868 ( 0.11 ) & 0.753 ( 0.15 ) & 1.0003 - 10 & 2100k & 1 & 0.737 ( 0.14 ) & 0.711 ( 0.14 ) & 0.730 ( 0.03 ) & 0.869 ( 0.15 ) & 0.767 ( 0.17 ) & 0.625 ( 0.01 ) & 1.000 & & 2 & 0.727 ( 0.15 ) & 0.698 ( 0.15 ) & 0.711 ( 0.04 ) & 0.885 ( 0.17 ) & 0.766 ( 0.19 ) & 0.605 ( 0.01 ) & 1.0006SL4 & 21k & 1 & 0.898 ( 0.01 ) & 0.939 ( 0.01 ) & 0.945 ( 0.01 ) & 0.908 ( 0.01 ) & 0.945 ( 0.01 ) & 0.958 ( 0.01 ) & 0.918 & & 2 & 0.829 ( 0.01 ) & 0.888 ( 0.01 ) & 0.887 ( 0.00 ) & 0.840 ( 0.01 ) & 0.883 ( 0.01 ) & 0.898 ( 0.01 ) & 0.8133 - 10 & 210k & 1 & 0.953 ( 0.05 ) & 0.956 ( 0.04 ) & 0.997 ( 0.00 ) & 0.976 ( 0.03 ) & 0.982 ( 0.00 ) & 0.981 ( 0.00 ) & 0.995 & & 2 & 0.934 ( 0.06 ) & 0.932 ( 0.05 ) & 0.995 ( 0.01 ) & 0.973 ( 0.04 ) & 0.989 ( 0.00 ) & 0.990 ( 0.00 ) & 0.978 3 - 10 & 2100k & 1 & 0.994 ( 0.00 ) & 0.973 ( 0.07 ) & 1.000 ( 0.00 ) & 0.990 ( 0.00 ) & 0.990 ( 0.00 ) & 0.987 ( 0.00 ) & 1.000 & & 2 & 0.996 ( 0.00 ) & 0.975 ( 0.07 ) & 1.000 ( 0.00 ) & 0.996 ( 0.00 ) & 0.996 ( 0.00 ) & 0.995 ( 0.00 ) & 1.0006SL8 & 21k & 1 & 0.966 ( 0.02 ) & 0.983 ( 0.01 ) & 0.995 ( 0.00 ) & 0.962 ( 0.02 ) & 0.969 ( 0.01 ) & 0.971 ( 0.01 ) & 0.991 & & 2 & 0.971 ( 0.01 ) & 0.980 ( 0.02 ) & 0.994 ( 0.00 ) & 0.969 ( 0.02 ) & 0.974 ( 0.01 ) & 0.977 ( 0.01 ) & 0.9663 - 10 & 210k & 1 & 0.990 ( 0.01 ) & 0.996 ( 0.00 ) & 0.999 ( 0.00 ) & 0.998 ( 0.00 ) & 0.999 ( 0.00 ) & 0.999 ( 0.00 ) & 0.998 & & 2 & 0.994 ( 0.00 ) & 0.995 ( 0.00 ) & 0.998 ( 0.00 ) & 0.997 ( 0.00 ) & 0.998 ( 0.00 ) & 0.998 ( 0.00 ) & 0.9943 - 10 & 2100k & 1 & 0.993 ( 0.01 ) & 0.998 ( 0.00 ) & 1.000 ( 0.00 ) & 0.999 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 & & 2 & 0.994 ( 0.01 ) & 0.999 ( 0.00 ) & 1.000 ( 0.00 ) & 0.999 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000tabulartable *
FJ,friendly jamming,TR-36693,"Major abbreviations 8.7cmSlX mygray Abbreviation & mygray Description ACA - A & Alternative Ascending Clock Auction ACA - T & Traditional Ascending Clock Auction CA / CDF & Cryptographic Authority / Cumulative Distribution Function CIA & Confidentiality , Integrity and Availability triad CRN & Cognitive Ratio Network DoS / DDoS & Denial - of - Service / Distributed Denial - of - Service EBV&Encrypted Bit Vector FJ & Friendly Jamming HMAC&Hash Message Authentication Code KKT & Karush - Kuhn - Tucker LSA & Licensed Shared Access MANET & Mobile Ad hoc NETwork NUM & Network Utility Maximization OPE & Order Preserving Encryption PD / PU / SU&Primary Destination / Primary User / Secondary User SINR & Signal - to - Interference - plus - Noise Ratio SSDF & Spectrum Sensing Data Falsification TLC / TTP & Time Lapse Cryptography / Trusted Third Party VCG / GSP & Vickrey - Clarke - Groves / Generalized Second - Price auction Overview of wireless security issuesWireless networks play an extremely important role in many applications ."
TA,threshold algorithm,TR-36804,"Therefore , effects the total execution time of TA - SKY and ST - S.figure*[!ht ] minipage[t]0.23 figures / TimeVsNAirbnb.pdf AirBnB : Varying the number of tuples fig : Airbnbn minipage minipage[t]0.23 figures / TimeVsOutputTupleAirbnb.pdf AirBnB : Time vs the number of skylines returned fig : Airbnbp1 minipage minipage[t]0.25 figures / NumberOfTuplesVsOutputTuplesAirbnb.pdf AirBnB : Number of accessed tuples vs the number of skylines fig : Airbnbp2 minipage minipage[t]0.25 figures / TimeVsDimensionalityZillow.pdf Zillow : Varying query size fig : Zillowm minipagefigure*Progressive Behavior of TA - SKY : Figure fig : syn_TimeVsNumberOfSkylines and fig : syn_NumberOfTuplesVsSkylines demonstrates the incremental performance of TA - SKY for discovering the new skylines for a specific query of size , while M and all the attributes having cardinality ."
WS,write skew,TR-36906,"figure[t]minipage[c]0.43subfigure0.59tikzpicture [ shape = rectangle , draw = none , font= ] ( A ) at ( 0,0 ) [ ] ; [ shape = rectangle , draw = none , font= ] ( B ) at ( 2.5,0 ) [ ] ; [ shape = rectangle , draw = none , font= ] ( C ) at ( 5,0 ) [ ] ; scope [ every edge/.style = draw = red , very thick ] [ - > ] ( A ) edge [ bend right ] node [ above , font= ] ( B ) ; [ - > ] ( B ) edge [ bend right ] node [ above , font= ] ( C ) ; scopetikzpictureAnomaly trace of WS program ."
ROC,receiver operating characteristic,TR-36910,* [ t]SVM Training ResultsThe metrics used for evaluation in our multi - class scenario are defined as follows : Accuracy - the accuracy metric is a weighted average of the true positive rates across all four classes;AUC - average area under the curve ; as the ROC curve depicts the relation between the false positive rate and the true positive rate when the pairwise discrimination threshold is varied ; in our case we report the average AUC between the target class and the sum of the negative classes ; Figures 9 - 11 below show the best results for each data set in graphical form .
HDT,header dictionary triple,TR-36919,"tab : compare - load - sp2tabularlrrrdata set & Stardog & HDT & TripleID 5 M & 40.98 & 0 & 1.86 10 M & 873.98 & 0 & 4.1 20 M & 3,820.71 & 0.01 & 8.6650 M & 424.54 & 0.03 & 19.65100 M & 1171.36 & 0.05 & 42.56tabulartabletable[tbhp ] Comparison for conversion time ( HDT and TripleID ) in seconds for tab : compare - conv - btc tabularlrrrdata set & HDT & TripleID & Speedup HDT/ & ( s ) & ( s ) & TripleID01 & 19 & 3.25 & 5.850103 & 51 & 8.57 & 5.950203 & 34 & 6.15 & 5.530207 & 22 & 3.06 & 7.19012347 & 71 & 10.37 & 6.84btc-2009 & 94 & 28.86 & 3.26tabulartabletable[thbp ] Comparison for conversion time ( HDT and TripleID ) in seconds for tab : compare - conv - sp2 tabularlrrrdata set & HDT & TripleID & Speedup HDT/ & ( s ) & ( s ) & TripleID5 M & 56 & 14.37 & 3.9010 M & 62 & 31.04 & 2.0020 M & 231 & 62.08 & 3.7250 M & 360 & 148.44 & 2.43100 M & 1256 & 298.1 & 4.21tabulartableTable tab : compare - load - sp2 compares the loading time of SP in Table tab : dataset2 ."
NC,news commentary,TR-36942,"table*[t ] tabularlllcccc & chrF ( std ) & BLEU ( std ) & Avg Requests & chrF & BLEU & Avg Requests 0 & 61.86 ( 0.06 ) & 25.54 ( 0.17 ) & 15.91 ( 0.01 ) & 0 & 0 & 0 0.25 & 62.15 ( 0.17 ) & 25.84 ( 0.13 ) & 11.06 ( 0.07 ) & +0.29 & +0.3 & -5 0.5 & 61.95 ( 0.05 ) & 25.46 ( 0.09 ) & 7.26 ( 0.03 ) & +0.09 & -0.08 & -9 0.75 & 62.15 ( 0.04 ) & 25.07 ( 0.12 ) & 4.94 ( 0.02 ) & +0.29 & -0.47 & -11 tabular Impact of entropy margin on average sentence - level chrF score , corpus BLEU and average number of feedback requests per sentence on the NC validation set ."
FEC,forward error correction,TR-37022,"figure[!htb ] center ./loss_distribution_MINT.eps center MINT - FEC 's experiment PLR distribution fig : MINT : lossDistfiguretable[!ht ] MINT - FEC Simulation parameters center tabularll Parameters & Value Display sizes & 1920x1080 , 1280x720 , and 800x600 Frame rate mode & Constant Frame rate & 29.970 fps GoP & 19:2 Video format & H.264 Codec & x264 Container & MP4 Propagation model & FriisPropagationLossModel Mobility model & Gauss - Markov UAV velocity & 45 - 65 km / h ( 28 - 40 mph ) LTE Frequency band & 800MHz LTE Mode & FDD LTE Bandwidth & 5 MHz eNodeB Operating Power & 22 dBm Antenna Gain & 16 dBi tabular tab : MINT : parameters center tableFive different schemes were simulated as follows : ( 1 ) without any FEC mechanism ."
MSE,mean squared error,TR-37120,"equation*propositionproofSubstituting the tailored value of in Equation eqn : M_adaptiveUG for that in Equation eqn : nonInter_mse , we haveeqnarray & o = & 2dr^23(N)^2+d1+dk^2-d^2d(1+d ) ( ( 2+d2d)^d2 + 2d + 12(2d2+d)^2+d2 + 2d ) .eqn : augkm_mseeqnarrayTherefore , when is small , the MSE of the cluster centroids output by isproofcommentcommentfigure*[p ] tabularcc s1-performance.eps & s1-performance-closeup.eps ( a ) S1 dataset & ( b ) S1 dataset , close - up at large gowalla-2d-performance.eps & gowalla-2d-performance-closeup.eps ( c ) Gowalla 2D dataset & ( d ) Gowalla 2D dataset , close - up at large gowalla-3d-performance.eps & gowalla-3d-performance-closeup.eps ( e ) Gowalla 3D dataset & ( f ) Gowalla 3D dataset , close - up at large tabular The comparison of four algorithms proposed in this paper , by varying fig : exp - vary - epsilonfigure*figure*[p]minipage[b]0.45regression - all - in - one.epsThe linear regression for empirically finding the value.fig:regressionminipageminipage[b]0.45ds1.10-scalability.epsThe comparison of the UGkM and DPLloyd with varying dimensions.fig:exp-vary-N-dimensionminipagefigure*commentThe Hybrid Approachssec : hybridOur hybrid approach combines and ."
CNN,convolutional neural network,TR-37168,"table[H ] tabularcccccccccc & 3c80 & 3c160 & 3c320 & SRP & GMBF & CNN & SRP & GMBF & CNN & SRP & GMBF & CNN 2 * 01 & & & & & & & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & & & & & & & 2*Average & & & & & & & & & & & & & & & & & & tabular Baseline results for the SRP - PHAT strategy ( columns SRP ) ; the one invelasco2012-F ( columns GMBF ) , and the CNN trained with synthetic data without applying the fine - tuning procedure ( columns CNN ) for sequences 01 , 02 and 03 for different window sizes ."
FEC,forward error correction,TR-37232,"tabularccccccc -1.5exPacket loss & -1.5exQoE & -1.5exWithout & & -1.5exVideo - aware FEC & & -1.5exViewFEC 1.0exrate & 1.0exMetric & 1.0exFEC & 1.5exVideo - aware FEC & 1.0exImprovement & 1.5exViewFEC & 1.0exImprovement 2*Packet loss 5 & VQM & 3.05 & 1.06 & 65.14 & 1.02 & 66.48 & SSIM & 0.76 & 0.91 & 19.74 & 0.92 & 21.05 2*Packet loss 10 & VQM & 4.01 & 1.11 & 72.36 & 1.12 & 72.09 & SSIM & 0.74 & 0.91 & 22.97 & 0.91 & 22.97 2*Packet loss 15 & VQM & 6.19 & 1.60 & 74.09 & 1.49 & 75.87 & SSIM & 0.50 & 0.90 & 80.00 & 0.89 & 78.00 2*Packet loss 20 & VQM & 8.68 & 1.77 & 79.60 & 1.81 & 79.14 & SSIM & 0.33 & 0.88 & 166.67 & 0.88 & 166.67 tabular tab : vfec : allpktloss center tableTaking into consideration the results of the experiments , it is possible to say that the proposed ViewFEC mechanism showed good performance ."
MAP,mean average precision,TR-37249,adjustboxmax width=0.9tabularl c c c c c c2*Embedding type & 3cRobust04 & 3cClueWeb ( lr)2 - 4 ( lr)5 - 7 & MAP & P@20 & nDCG@20 & MAP & P@20 & nDCG@20 Pretrained ( external ) + Uniform weighting & 0.1656 & 0.2543 & 0.3017 & 0.0612 & 0.1300 & 0.1401 Pretrained ( external ) + IDF weighting & 0.1711 & 0.2755 & 0.3104 & 0.0712 & 0.1346 & 0.1469 Pretrained ( external ) + Weight learning & 0.1880 & 0.2890 & 0.3413 & 0.0756 & 0.1344 & 0.1583 Pretrained ( target ) + Uniform weighting & 0.1217 & 0.2009 & 0.2791 & 0.0679 & 0.1331 & 0.1587 Pretrained ( target ) + IDF weighting & 0.1402 & 0.2230 & 0.2876 & 0.0779 & 0.1674 & 0.1540 Pretrained ( target ) + Weight learning & 0.1477 & 0.2266 & 0.2804 & 0.0816 & 0.1729 & 0.1608Learned + Uniform weighting & 0.2612 & 0.3602 & 0.4180 & 0.0912 & 0.2216 & 0.1841Learned + IDF weighting & 0.2676 & 0.3619 & 0.4200 & 0.1032 & 0.2419 & 0.1922 Learned + Weight learning & 0.2837 & 0.3802 & 0.4389 & 0.1387 & 0.2967 & 0.2330 tabularadjustboxtable *
CNS,copenhagen networks study,TR-37263,"@ X rr rr@ & 2 c CNS & 2 c MDC ( lr)2 - 3(lr)4 - 5 & ( PC 0 ) & ( PC 1 ) & ( PC 0 ) & ( PC 1 ) Social circle size , & 0.41 & 0.16 & 0.37 & -0.15 Activity space size , & 0.42 & -0.24 & 0.42 & -0.08 New ties / week , & 0.33 & 0.28 & 0.27 & 0.33 New locations / week , & 0.38 & -0.05 & 0.37 & 0.19 Social circle entropy , & 0.31 & 0.30 & 0.34 & 0.09 Activity space entropy , & 0.38 & -0.16 & 0.30 & -0.07 Social circle stability , & -0.16 & -0.46 & 0.07 & -0.72 Activity space stability , & -0.10 & -0.49 & -0.12 & -0.51 Social circle rank turnover , & -0.20 & 0.28 & -0.33 & 0.10 Activity space rank turnover , & -0.30 & 0.44 & -0.38 & 0.17 Contribution of the original variables to the first principal component ."
NN,nearest neighbor,TR-37370,"figure*[!htb]tabularcccc 4cRegistered target images at resolution fig_teaser / woman_test4_query.jpg & fig_teaser/02014_target.jpg & fig_teaser/02014_query.jpg & fig_teaser / fake0_target.jpg & & ( Flipped ) & 4cNN recovery in CelebA - HQ fig_teaser / woman_test4_query_NN_MSE.jpg & fig_teaser/02014_target.jpg & fig_teaser/02014_query_NN_MSE.jpg & fig_teaser / fake0_target_NN_MSE.jpg 4cLatent recovery from the manifold fig_teaser / woman_test4_recovery_query.jpg & fig_teaser/02014_recovery_target.jpg & fig_teaser/02014_recovery_query.jpg & fig_teaser / fake0_recovery_target.jpg tabular Rather than inspecting the most similar pictures ( NN ) in the training dataset for a few sampled generated images , as usually done for evaluating GANs , we consider in this work the opposite : finding the most similar image in the manifold of generated images and statistically test the discrepancy with the reference images ."
CT,computed tomography,TR-37412,figure [ ht ] center minipage0.15 ./fig / snapshot0101new3.png minipage minipage0.15 ./fig / snapshot0121new3.png minipage minipage0.15 ./fig / snapshot0131new3.png minipage minipage0.15 ./fig / snapshot0141new3.png minipage minipage0.15 ./fig / snapshot0102new3.png minipage minipage0.15 ./fig / snapshot0122new3.png minipage minipage0.15 ./fig / snapshot0132new3.png minipage minipage0.15 ./fig / snapshot0142new3.png minipage minipage0.15 ./fig / snapshot0104new.png minipage minipage0.15 ./fig / snapshot0124new.png minipage minipage0.15 ./fig / snapshot0134new.png minipage minipage0.15 ./fig / snapshot0144new.png minipage minipage0.15 ./fig / snapshot0105new2.png minipage minipage0.15 ./fig / snapshot0125new2.png minipage minipage0.15 ./fig / snapshot0135new2.png minipage minipage0.15 ./fig / snapshot0145new2.png minipage minipage0.15 ./fig / snapshot0107new.png minipage minipage0.15 ./fig / snapshot0127new.png minipage minipage0.15 ./fig / snapshot0137new.png minipage minipage0.15 ./fig / snapshot0147new.png minipage minipage0.15 ./fig / snapshot0108new2.png minipage minipage0.15 ./fig / snapshot0128new2.png minipage minipage0.15 ./fig / snapshot0138new2.png minipage minipage0.15 ./fig / snapshot0148new2.png minipage center 5fig : vis2 Visualizations for the first four anatomy on the first four holdout CT images .
CBT,children 's book test,TR-37424,[ b ] < Bleu1-Bleu4-Meteor - RougeL>. TableNotes 1mmlongtableccccccccccc & tabular@c@SQuAD(v1.1 ) tabular & tabular@c@SQuAD(v2.0 ) tabular & tabular@c@CNNDaily Mail tabular & CBT & NewsQA & TriviaQA & WIKIHOP & & MS MARCO & NarrativeQA tabular@c@Release datetabular & 2016 & 2018 & 2015 & 2015 & 2017 & 2017 & 2018 & & 2018(v2 ) & 2017 Type & extractive & extractive & extractive & extractive & extractive & extractive & extractive & & narrative & narrative domain & Wikipedia & Wikipedia & News & books & News & Trivia & Wikipedia & & Search Engine & Scripts tabular@c@Question sourcetabular & tabular@c@crowd - sourcedtabular & tabular@c@crowd - sourcedtabular & automatic & automatic & tabular@c@crowd - sourcedtabular & natural & automatic & & tabular@c@query : natural answer : automatictabular & tabular@c@crowd - sourcedtabular tabular@c@Human performance tabular & tabular@c@EM 82.3 F1 91.2tabular & tabular@c@EM 86.8 F1 89.5tabular & - & tabular@l@NE 0.816 CN 81.6 VB 82.8 PR .
MPI,message passing interface,TR-37496,"MPI - parallel Multi - frame RMSD using Global Arrays alg : GA Input : size : Total number of frames assigned to each rank ga : Initialized Global Arrays xref0 : mobile group in the initial frame which will be considered as reference start stop : that tell which block of trajectory ( frames ) is assigned to each rank topology trajectory : files to read the data structure from Include : BlockRMSD ( ) from Algorithm alg : RMSD algorithmic[1 ] bsize ceil(trajectory.numberframes / size ) ga ga.create(ga.CDBL , [ bsize*size,2 ] , "" RMSD "" ) buf np.zeros([bsize*size,2 ] , dtype = float ) out BlockRMSD(topology , trajectory , xref0 , start = start , stop = stop ) ga.put(ga , out , ( start,0 ) , ( stop,2 ) ) rank = = 0 buf ga.get(ga , lo = None , hi = None ) algorithmicalgorithmMPI and Parallel HDF5sec : methods - hdf5HDF5 is a structured self - describing hierarchical data format which is the standard mechanism for storing large quantities of numerical data in Python ( http://www.hdfgroup.org/HDF5,pythonhdf5 ) ."
RG,real graphs,TR-37549,"figure-1.7 em 6a.pdf 6b.pdf 6c.pdf 6d.pdf sdisp.pdf sdisn.pdf -1.0 em Diffusion dynamics on the sparse graph : ( a ) disease prevalences for real graph , SPDT model , BADN graph and SPST graph , ( b ) final epidemic size , ( c ) prediction error for : * lines for cumulative and other for daily ( d ) number of Momo users ( K ) and link densities per user , ( e ) for various in RG and SG , and ( f ) new infections for various fig : sdif -1.6 emfigurefigure*-1.5 em large_com.pdf large_cum.pdf lare_er.pdf ldisp.pdf ldisn.pdf -1.0 em Diffusion dynamics on the dense graphs : ( a ) disease prevalences for real graph , SPDT model , BADN graph and SPST graph , ( b ) final epidemic sizes ( c ) prediction errors , ( d ) for various r in RG and SG , and ( e ) new infections for various r fig : sdifl -1.3 emfigure*Model ValidationIn this section , we validate the proposed model simulating SPDT process on the generated synthetic graph and real contact graph ."
TDS,taint dependency sequences,TR-37620,"Application & Name & LoC & Constraints & TDS + GA & coverage + GA & Random inputs 1 & sendmail & mimefromqp & 65 & ' = n ' & 20 & 26 & 243 2 & sendmail & buildfname & 52 & '' and not ( , ; ) & 6 & 10 & 34 3 & edbrowse & ftpls & 49 & ' -- ' ( in the beginning ) & 35 & * & * tabulartab : resulttableIn the table , columns 2 - 3 denote the path of the vulnerable program in the Verisec suite ."
RS,rate - selective,TR-37644,"Substituting Eq : MGF_end_Final and Eq : OP_FSDF_Difm to Eq : MGF_SSDF , a closed - form expression for of rate - selective RS over INID Nakagami- fading with integer 's and distinct 's is derived aswhile , substituting Eq : MGF_end_Final_Equalm and Eq : OP_FSDF_Equalm to Eq : MGF_SSDF , yields the following closed - form expression for of rate - selective RS over IID Nakagami- fading channels with integer : Performance Analysis of ODF Relaying SchemesIn this section , the performance of ODF relaying schemes with repetitive and RS - based transmission over INID Nakagami- fading channels will be analyzed ."
DPs,dropped pronouns,TR-37730,"* [ t]lccclc 2*Model & 2*Params & 2cSpeed & 2cBLEU 3 - 6 & & Training & Decoding & Test & Baseline & 86.7 M & 1.60 K & 2.61 & 31.80 & - / - Baseline ( + DPs ) & 86.7 M & 1.59 K & 2.63 & 32.67 & +0.87 / - + enc - rec & +39.7 M & 0.71 K & 2.63 & 33.67 & +1.87 / +1.00 + dec - rec & +34.1 M & 0.84 K & 2.18 & 33.48 & +1.68 / +0.81 + enc - rec + dec - rec & +73.8 M & 0.57 K & 2.16 & 35.08 & +3.28 / +2.41 Multi - Source & +20.7 M & 1.17 K & 1.27 & 32.81 & +1.01 / +0.14 Multi - Layer & +75.1 M & 0.61 K & 2.42 & 33.36 & +1.56 / +0.69 Baseline ( + DPs ) + Enlarged Hidden Layer & +86.6 M & 0.68 K & 2.51 & 32.00 & +0.20 / -0.67 Evaluation of translation performance for Chinese - English . """
PP,pairwise perturbation,TR-37808,"algorithm PP - Tucker - ALS : Pairwise perturbation procedure for Tucker - ALSalg : tucker_als_ppalgorithmic[1]Input : tensor , decomposition ranks , stopping criteria , PP tolerance Initialize using HOSVD , initialize for , initialize Compute for via dimension tree in Section subsec : cost , and leading left singular vectors of Perform regular ALS sweep as in Algorithm alg : tucker - als , taking for each algorithmicalgorithmDimension Trees for Pairwise Perturbation Operatorssubsec : cost Computation of the pairwise perturbation operators and of can benefit from amortization of common tensor contraction ( Khatri - Rao product or multilinear multiplication ) subexpressions ."
GDP,gross domestic product,TR-37812,"Sources : BOE ( ID : IUQLBEDR , XUQLBK82 , IUQLBEDR , LPQAUYN ) , ONS ( ID : D7BT , UKEA , PGDP , PRDY , MGSX ) , BIS ( US private sector debt : Q : US : P : A : M : XDC : A , UK : ERI , GBP / USD ( 1955 only ) ) , OECD ( US CPI , US M3 , US GDP , US Unemployment , US CA ) , FRED ( ID : RNUSBIS , FEDFUNDS , PRS85006163 , A229RX0 ) , ( UK private sector debt , M4 , labour productivity)[h ! ]"
MDC,mobile data challenge,TR-38107,"@ X rr rr@ & 2 c CNS & 2 c MDC ( lr)2 - 3(lr)4 - 5 & ( PC 0 ) & ( PC 1 ) & ( PC 0 ) & ( PC 1 ) Social circle size , & 0.41 & 0.16 & 0.37 & -0.15 Activity space size , & 0.42 & -0.24 & 0.42 & -0.08 New ties / week , & 0.33 & 0.28 & 0.27 & 0.33 New locations / week , & 0.38 & -0.05 & 0.37 & 0.19 Social circle entropy , & 0.31 & 0.30 & 0.34 & 0.09 Activity space entropy , & 0.38 & -0.16 & 0.30 & -0.07 Social circle stability , & -0.16 & -0.46 & 0.07 & -0.72 Activity space stability , & -0.10 & -0.49 & -0.12 & -0.51 Social circle rank turnover , & -0.20 & 0.28 & -0.33 & 0.10 Activity space rank turnover , & -0.30 & 0.44 & -0.38 & 0.17 Contribution of the original variables to the first principal component ."
CT,computed tomography,TR-38273,figure [ ht ] center minipage0.15 ./fig / snapshot0101new3.png minipage minipage0.15 ./fig / snapshot0121new3.png minipage minipage0.15 ./fig / snapshot0131new3.png minipage minipage0.15 ./fig / snapshot0141new3.png minipage minipage0.15 ./fig / snapshot0102new3.png minipage minipage0.15 ./fig / snapshot0122new3.png minipage minipage0.15 ./fig / snapshot0132new3.png minipage minipage0.15 ./fig / snapshot0142new3.png minipage minipage0.15 ./fig / snapshot0104new.png minipage minipage0.15 ./fig / snapshot0124new.png minipage minipage0.15 ./fig / snapshot0134new.png minipage minipage0.15 ./fig / snapshot0144new.png minipage minipage0.15 ./fig / snapshot0105new2.png minipage minipage0.15 ./fig / snapshot0125new2.png minipage minipage0.15 ./fig / snapshot0135new2.png minipage minipage0.15 ./fig / snapshot0145new2.png minipage minipage0.15 ./fig / snapshot0107new.png minipage minipage0.15 ./fig / snapshot0127new.png minipage minipage0.15 ./fig / snapshot0137new.png minipage minipage0.15 ./fig / snapshot0147new.png minipage minipage0.15 ./fig / snapshot0108new2.png minipage minipage0.15 ./fig / snapshot0128new2.png minipage minipage0.15 ./fig / snapshot0138new2.png minipage minipage0.15 ./fig / snapshot0148new2.png minipage center 5fig : vis2 Visualizations for the first four anatomy on the first four holdout CT images .
PD,pu - primary destination,TR-38342,"Major abbreviations 8.7cmSlX mygray Abbreviation & mygray Description ACA - A & Alternative Ascending Clock Auction ACA - T & Traditional Ascending Clock Auction CA / CDF & Cryptographic Authority / Cumulative Distribution Function CIA & Confidentiality , Integrity and Availability triad CRN & Cognitive Ratio Network DoS / DDoS & Denial - of - Service / Distributed Denial - of - Service EBV&Encrypted Bit Vector FJ & Friendly Jamming HMAC&Hash Message Authentication Code KKT & Karush - Kuhn - Tucker LSA & Licensed Shared Access MANET & Mobile Ad hoc NETwork NUM & Network Utility Maximization OPE & Order Preserving Encryption PD / PU / SU&Primary Destination / Primary User / Secondary User SINR & Signal - to - Interference - plus - Noise Ratio SSDF & Spectrum Sensing Data Falsification TLC / TTP & Time Lapse Cryptography / Trusted Third Party VCG / GSP & Vickrey - Clarke - Groves / Generalized Second - Price auction Overview of wireless security issuesWireless networks play an extremely important role in many applications ."
RF,radio frequency,TR-38534,"It is worth noting that communications through RISs is different compared with other related technologies currently employed in wireless networks , such as relaying , MIMO beamforming , passive reflect - arrays , and backscatter communications , while having the following major distinguishable features : i ) RISs are nearly passive , and , ideally , they do not need any dedicated energy source for RF signal processing ; ii ) RISs do not amplify or introduce noise when reflecting the signals and provide an inherently full - duplex transmission ; iii ) RISs can be easily deployed , e.g. , on the facades of buildings , ceilings of factories , and indoor spaces ; iv ) RISs are reconfigurable in order to adapt themselves according to the changes of the wireless environment ."
MRE,median recovery error,TR-38585,"WARNING : use overfitting instead of only ' memorization ' which is more ambiguous , ' verbatim ' seems to strongToDo list : look at distribution of attributes in the latent spaceIf we have time : Do a figure with auto - encoder ( if we manage to sample the latent space , for instance by fitting a Gaussian to the encoded images in the latent space)Show a picture of a few examples of Eiffel tower or london bridge in LSUN that seem to be memorize but are just samples learn from templateadd appendix with extra experiments ( a lot of images of recovery)show ( at least on some images ) that VGG , perceptual loss , L1 or L1 on Laplacian pyramid gives the same results for recovery because GLO reported some mitigated results on this ( sometimes L2 is better , sometimes pyramid : in the end , they mixed the two metrics)give statistics about the recovery precision with LBFGS : for a given * generated * target image , the distribution of errors using a lot of random initializations ( to demonstrate that the problem is almost convex , or at least ' easy ' to optimize ) show convergence speed vs SGD or other optimization method ( because it is was is generaly used in the literature : this finding makes it possible to experiment more easily ) , for a given distorted target generated image , the average / median error vs the distorsion : : you did that on training image but it would be nice to show it first on generated images , to show robustnessdiscuss difference LBFGS vs SGD : LBFGS is much faster to converge , not prone to gradient step setting , but can suffer from instabilitiesadd experiments on Auto Encoder : visual recovery results , histograms , MRE values , and maybe FID if we have time ( sampling from the latent space)add a small paragraph to explain experimental settings , i.e. that we reproduce several architectures from the literature and trained again on splitting , ... tell somewhere the difference ( I think in the intro ) between over - fitting and verbatim memorization , for which we provide a solid definition : add more experiments on LSUN : show histograms with PG - GAN without GAP , show recovery failure resultsadd experiments on MNIST , CIFAR ?"
RRC,rank residual constraint,TR-38587,"a ) Original image ; ( b ) Noisy image ; ( c ) NNM ( PSNR = 19.57dB , SSIM = 0.6345 ) ; ( d ) BM3D ( PSNR = 20.90dB , SSIM = 0.7482 ) ; ( e ) LSSC ( PSNR = 20.54dB , SSIM = 0.7242 ) ; ( f ) EPLL ( PSNR = 20.26dB , SSIM = 0.7163 ) ; ( g ) Plow ( PSNR = 20.43dB , SSIM = 0.6814 ) ; ( h ) NCSR ( PSNR = 20.84dB , SSIM = 0.7622 ) ; ( i ) GID ( PSNR = 19.13dB , SSIM = 0.6857 ) ; ( j ) PGPD ( PSNR = 20.95dB , SSIM = 0.7469 ) ; ( k ) LINC ( PSNR = 20.44dB , SSIM = 0.7467 ) ; ( l ) aGMM ( PSNR = 20.29dB , SSIM = 0.7106 ) ; ( m ) OGLR ( PSNR = 20.28dB , SSIM = 0.6827 ) ; ( n ) RRC ( PSNR = 21.22dB , SSIM = 0.7811 ) ."
CNN,convolutional neural network,TR-38718,table[!h ] Average accuracy over three splits on UCF-101 and HMDB51 tab : HAN tabularccc Model & UCF-101 & HMDB51 Full HAN ( spatial CNN cube+temporal CNN cube ) & 92.7 & 64.3 HAN without attention(spatial CNN cube + temporal CNN cube ) & 90.6 & 62.0 HAN without attention(spatial CNN 4096 + temporal CNN 4096 ) & 91.1 & 62.7 Spatial HAN ( spatial CNN cube ) & 75.1 & 47.7 Temporal HAN ( temporal CNN cube ) & 85.4 & 58.3 tabulartableWe first evaluate our proposed HAN on UCF-101 and HMDB51 datasets by comparing HAN with different settings to show the importance of each key component in HAN in Table tab : HAN .
MRE,median recovery error,TR-38737,"fig : LBFGS_visual_realfigure0 ToDo itemize look at distribution of attributes in the latent space If we have time : Do a figure with auto - encoder ( if we manage to sample the latent space , for instance by fitting a Gaussian to the encoded images in the latent space ) Show a picture of a few examples of Eiffel tower or london bridge in LSUN that seem to be memorize but are just samples learn from template show ( at least on some images ) that VGG , perceptual loss , L1 or L1 on Laplacian pyramid gives the same results for recovery because GLO reported some mitigated results on this ( sometimes L2 is better , sometimes pyramid : in the end , they mixed the two metrics ) give statistics about the recovery precision with LBFGS : itemize for a given * generated * target image , the distribution of errors using a lot of random initializations ( to demonstrate that the problem is almost convex , or at least ' easy ' to optimize ) show convergence speed vs SGD or other optimization method ( because it is was is generaly used in the literature : this finding makes it possible to experiment more easily ) , for a given distorted target generated image , the average / median error vs the distorsion : : you did that on training image but it would be nice to show it first on generated images , to show robustness itemize discuss difference LBFGS vs SGD : LBFGS is much faster to converge , not prone to gradient step setting , but can suffer from instabilities add experiments on Auto Encoder : visual recovery results , histograms , MRE values , and maybe FID if we have time ( sampling from the latent space ) add a small paragraph to explain experimental settings , i.e. that we reproduce several architectures from the literature and trained again on splitting , ... tell somewhere the difference ( I think in the intro ) between over - fitting and verbatim memorization , for which we provide a solid definition : add more experiments on LSUN : show histograms with PG - GAN without GAP , show recovery failure results add experiments on MNIST , CIFAR ?"
DBN,deep belief network,TR-38830,"arrows , positioning , patterns multi - objective net - works C - MAPSS classification hyper - parameters pre - process pre - processing ada - boost G - mean Cost - sensitive ECS - DBN Evolutionary Algorithm A Cost - Sensitive Deep Belief Network for Imbalanced ClassificationChong Zhang , Kay Chen Tan , Fellow , IEEE , Haizhou Li , Fellow , IEEE , and Geok Soon Hong C. Zhang and H. Li are with the Department of Electrical and Computer Engineering , G. S. Hong is with the Department of Mechanical Engineering , National University of Singapore , 4 Engineering Drive 3 , 117583 , Singapore ( e - mail : zhangchong@u.nus.edu ; haizhou.li@nus.edu.sg ; mpehgs@nus.edu.sg)K. C. Tan is with the Department of Computer Science , City University of Hong Kong , 83 Tat Chee Avenue , Kowloon , Hong Kong.(e - mail : kaytan@cityu.edu.hk)This paper has been accepted by IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS in April 2018 ."
MPI,multiple parallel instances,TR-38881,figure[!htb ] subfigure.4 figures / main - RMSD - t_total - Bridges.pdf Scaling total fig : MPIscaling - Bridges subfigure subfigure.4 figures / main - RMSD - speed_up - Bridges.pdf Speed - up fig : MPIspeedup - Bridges subfigure subfigure.4 figures / main - RMSD - time_comp_IO_comparison - Bridges.pdf format = hang Scaling for different components fig : ScalingComputeIO - Bridges subfigure subfigure .5 figures / main - RMSD - BarPlot - rank - comparison_72_4-Bridges.pdf format = hang Time comparison on different parts of the calculations per MPI rank ( example ) fig : MPIranks - Bridges subfigure PSC Bridges : Performance of the RMSD task .
NN,neural network,TR-38965,"tab : paramstablePerformance discussiontable*[h ] tabular@lllllll@ 1r & 1lTrain & 5cTest 1rLength & All & & & & & All NN & 1.14 0.37 & 0.00 0.00 & 0.00 0.00 & 0.00 0.00 & 0.70 1.22 & 0.45 0.43 LogReg & 65.51 6.06 & 37.50 41.46 & 35.41 20.73 & 43.75 44.63 & 51.29 5.12 & 51.24 5.23 HMM & 59.92 6.70 & - & 2.08 3.62 & 12.50 21.65 & 51.00 10.48 & 52.38 14.62 Linear inputs & 76.43 5.98 & 25.00 43.30 & 16.67 20.41 & 31.25 44.63 & 58.64 8.14 & 55.22 5.66 Non - linear inputs & 77.30 3.69 & 25.00 43.30 & 25.00 21.65 & 43.75 44.63 & 57.78 4.93 & 56.14 4.73 Attention , L=1 & 85.16 9.92 & 25.00 43.30 & 25.00 25.00 & 43.75 44.63 & 68.97 4.08 & 67.68 3.50 Attention , L=2 & 80.70 3.55 & 25.00 43.30 & 25.00 25.00 & 43.75 44.63 & 64.45 7.16 & 63.59 7.10 Attention , L=3 & 81.63 5.02 & 25.00 43.30 & 25.00 25.00 & 43.75 44.63 & 66.69 3.60 & 65.57 3.12 Attention , L=4 & 80.76 7.47 & 25.00 43.30 & 25.00 25.00 & 43.75 44.63 & 72.63 7.53 & 71.16 7.04 tabular Recall provided by different models , including variations of our proposal ."
MI,mutual information,TR-39209,tab : topMItable*table*[h]tabularlccccSetting & Age & 1lEpoch & Gender & 1lEpoch small - GRU & 36.13 & 1 & 53.39 & 1 Attn - BiGRU & 47.03 & 3 & 61.64 & 2 MTL - common - attn & 47.85 & 6 & 62.50 & 4 MTL - spec - attn & 47.92 & 4 & 63.09 & 4 MTL - sprvsd - spec - attn & 48.23 & 5 & 62.99 & 3 MTL - MI - spec - attn & 47.90 & 3 & 63.15 & 5 BERT & 50.95 & 4 & 64.96 & 6 tabularEpoch information for best model performance on our DEV .
PI,purchase intention,TR-39386,"L ) Magnitude of the correlation between trust and sales , ( M ) Magnitude of the correlation between trust and price , ( N ) Magnitude of the correlation between trust and history , ( O ) Magnitude of the correlation between PI and sales , ( P ) Magnitude of the correlation between price and PI , ( Q ) Magnitude of the correlation between history and PI , ( R ) Magnitude of the correlation between price and sales , ( S ) Magnitude of the correlation between history and sales , ( T ) Magnitude of the correlation between history and price ."
MPI,message passing interface,TR-39525,figure[!htb ] subfigure.49 figures / main - RMSD - t_total.pdf format = hang Scaling total ( five repeats ) fig : MPIscaling subfigure subfigure.49 figures / main - RMSD - speed_up.pdf format = hang Speed - up ( five repeats ) fig : MPIspeedup subfigure subfigure.49 figures / main - RMSD - time_comp_IO_comparison.pdf format = hang Scaling for different components ( five repeats ) fig : ScalingComputeIO subfigure subfigure .5 figures / main - RMSD - BarPlot - rank - comparison_72_5.pdf format = hang Time comparison on different parts of the calculations per MPI rank ( example ) fig : MPIranks subfigure Performance of the RMSD task parallelized with MPI on SDSC Comet .
GMM,gaussian mixture model,TR-39571,"a ) Target : ' Dog ' , Base : ' Cat ' ( b ) Target : ' Frog ' , Base : ' Ship ' ( c ) Target : ' Cat ' , Base : ' Car ' ( d ) Target : ' Bird ' , Base : ' Airplane ' * [ p ] Continued from Figure ; ( e ) Target : ' Deer ' , Base : ' Horse ' ( f ) Target : ' Bird ' , Base : ' Truck ' ( g ) Target : ' Horse ' , Base : ' Cat ' ( h ) Target : ' Cat ' , Base : ' Dog ' ( i ) Target : ' Dog ' , Base : ' Car ' * [ p ] Wasserstein distance between GMM clusters of input gradient first principal components with under overlay image BP attacks ."
AV,acquaintance vaccination,TR-39604,"tikzpicture customlegend[legend columns=4,legend style = at=(0.12,1.02),draw = none , column sep=3ex , line width=2pt , font= , legend entries = RV , AV , IMV , DV , direct , indirect ] solid , color = blue color = red color = green color = magenta color = black dashdotted , color = black customlegend tikzpicture pvan_a.pdfpvan_b.pdfpvan_c.pdfpvan_d.pdfAverage outbreak sizes at various vaccination rates ( percentage of total nodes ) of different strategies : ( A , B ) nodes are vaccinated with contacts created for direct interactions and ( C , D ) comparison of outbreak sizes for vaccinating nodes with contacts based on the direct interactions ( solid lines ) and contact based on any direct or indirect interactions ( dashed lines)-1.5emfig : avacfigurefigure[h ! ]"
GCN,graph convolution networks,TR-39637,"table*[t]Summary of the different components to applying our framework to different input domainstab : componentscentersmallsctabularlcccccrAttack Domain & Input Type & Encoder & Decoder & & & ) Image & Pixels & CNN & CNN & Pixels & & Text - Emb & Word Emb & LSTM & LSTM & Word Emb & & Text - Token & Discrete Token & LSTM & LSTM & Word Emb & & Graph - Direct & Node Emb & GCN & MLP & Node Emb & & Graph - Influencer & Node Emb & GCN & MLP & Node Emb & & tabularscsmallcenterattack_summarytable*This generic encoder - decoder framework for adversarial attacks can be easily adapted to different domains ( e.g. , text , images , or graphs ) by simply specifying domain - specific variants of the four components above , with further details provided in Section sec : domain_imp ."
GMM,gaussian mixture model,TR-39733,"Given the LTI system eq : LTI driven by system and sensor noises whose probability density functions can be expressed as the Gaussian mixture models in eq : noiseGMM_eta and eq : noiseGMM_v , the probability density function of the residual at time can be written as a Gaussian mixture model of Gaussian modeswhere represents the mixture probabilities of the Gaussian modes , are the means , and are the covariances , [ ] [ ] [ ] [ ] [ ] where captures all the possible permutations of combining terms from the system and measurement noise GMM modes , following the rules:[leftmargin=1.3 cm ] [ Init : ] for [ Add : ] [ Wrap : ] if [ ] then and ."
EM,expectation maximization,TR-39835,"In the E - Step we assume to know and calculate the posterior probability of for each datapoint calling it : ( We assume the to be independent of for ) align _ ik = P(z_ik=1x_i , _ i , ) & = P(x_iz_ik=1 , _ i ) P(z_ik=1 ) P(x_i_i , ) & = _ ik^x_i ( 1-_ik)^1-x_i _ k _ j=1^K _ ij^x_i ( 1-_ij)^1-x_i _ j alignNext we calculate the value used in EM which is defined as the expectation of the complete data log - likelihood with respect to the posterior of given the data and the old parameters : align Q ( , ^old ) & = E_Z[P(x , Z ) x , ^old ] & = _ Z P(Zx , ^old)P(x , Z ) & = _ i=1^N _ k=1^K _ ik [ x_i _ ik + ( 1-x_i ) ( 1 - _ ik ) + _ k]alignIn the M - step of EM we aim to maximize over all choices of : equation ^new = _ Q ( , ^old)equationUsing a Lagrange multiplier to enforce we find : equation _ k^new = _ i=1^N _ ikNequationBut when maximizing wrt ."
NIC,neural image caption,TR-39905,"4.225 in c c c c c c c c Model & B@1 & B@2 & B@3 & B@4 & M & R & C NIC & & & & & - & - & - LRCN & & & & & - & - & - Soft Attention & & & & & & - & - Hard Attention & & & & & & - & - ATT & & & & 30.4 & & - & - Sentence Condition & & & & & & - & LSTM - A & 73 & & & & 25.1 & 53.8 & 98.6 CNet - NIC & & 54.9 & 40.5 & & & & * [ htp]Performance of variants of CNet - NIC on MS COCO dataset , where B@ , M , R , and C are short for BLEU@ , METEOR , ROUGE - L , and CIDEr - D scores ."
SR,success rate,TR-39910,"tab : table10.95tabularcccccc1 - 6 & & 2 c All&2 c 3 - 6 & & SR & SPL & SR & SPL & Random & 1.2 & 0.7 & 0.6 & 0.3 2 - 6Unseen&TD - A3C & 20.0 & 4.0 & 12.9 & 2.6 2 - 6scenes , & TD - A3C(BC ) & 23.0 & 7.9 & 13.4 & 3.7 2 - 6Known&Ours & 45.7 & 25.8 & 41.9 & 24.8 2 - 6targets & Ours - FroView & 32.3 & 10.3 & 29.8 & 9.4 2 - 6P=17.7 & Ours - NoGen & 41.2 & 23.8 & 38.5 & 22.2 2 - 6 & Ours - VallinaGen & 37.5 & 17.7 & 34.0 & 15.9 2 - 6 & Random & 2.0 & 1.0 & 0.6 & 0.4 2 - 6Unseen&TD - A3C & 10.1 & 1.9 & 6.3 & 1.1 2 - 6scenes , & TD - A3C(BC ) & 12.3 & 2.4 & 7.5 & 1.6 2 - 6Novel&Ours & 37.7 & 20.5 & 35.4 & 19.7 2 - 6targets & Ours - FroView & 24.6 & 7.8 & 23.0 & 6.9 2 - 6P=16.0 & Ours - NoGen & 35.7 & 19.1 & 31.6 & 17.4 2 - 6 & Ours - VallinaGen & 31.4 & 13.9 & 29.4 & 12.7 2 - 6tabulartabletableComparing navigation performance ( SR and SPL in ) on different scene categories on AI2-THOR with stop action ."
HMC,hybrid monte carlo,TR-39959,XXXXXXX HMC Properties & Acceptance Rate ( ) & IACF equilibration & IACF production & Acceptance Rate ( ) & IACF equilibration & IACF production ( ps ) & 3c25 fs 1000 = 25 ps & 3c25 fs 2000 = 50 ps VV & 42.92 & 105.50 & 102.76 & 39.34 & 226.57 & FAILED BCSS & 17.64 & 259.38 & FAILED & 6.01 & 373.43 & FAILED AIA & 41.30 & 107.04 & 105.94 & 41.10 & 101.02 & 141.67 Acceptance rate and IACF for equilibration and production with HMC for the biggest simulated time step and two choices of lengths of MD trajectories .
CNN,convolutional neural network,TR-40049,"Proposed ModelWe construct an end - to - end trainable CNN architecture comprising of ( 1 ) a Siamese network , encompassing a pair of standard VGGNet CNNs truncated up and until the fourth convolutional layer ( conv ) with pretrained weights , that output a pair of tensors for each image , ( 2 ) a joint graph formulation computed as shown in equation ( ) , which is then reshaped into 2D grids of corresponding sizes and further appended with ( 3 ) a pair of ( Siamese ) convolutional layers , which are then concatenated , followed by a few dense layers to ultimately regress the homography matrix ."
SVD,singular value decomposition,TR-40137,"Archival industrial gas turbine data setsPart 2 of this paper applies these theoretically supported exact results based on rank conditions to industrial data sets in order to demonstrate several features:[label=(*)]that the methods are amenable to handling less than pristine industrial data , where linearity is only a local feature , models are admittedly approximate , and data integrity is challenged;that the rank conditions of the theory can be effectively replaced by rank - revealing SVD conditions;that the methods are applicable to MIMO systems directly ; andthat model fitting can improve with the introduction of multiple data sets , thereby unlocking the data archive ."
MC,marginal contribution,TR-40152,"6 & _ p_i^req , t_i , s_i , w_i & & _ jA_i t_i , j , & & i P & subject to : & & & & & w_i , j p_j^min p^req_i , j w_i , j p_j^max , & & j A_i & & & _ jW_i p_i , j^req g_i , j ^2 = ^th & & j A_i & & & m_i(n ) p_i , j^req + y_i(n , p_-i , j^rcv ) t_i , j & & & j A_i , & 0 n M_j & & & _ j A_i w_i , j W^max & & j A_i & & & p^req_i , j , t_i , j R , s_i , j , w_i , j 0,1 & & j A_i MRC - based centralized approach with MILPIn the previous section , we discussed decision - making with the MC and the SV schemes ."
MT,machine translation,TR-40222,"Due to the differences in vocabularies and sentence lengths of the generated translations , in order to conduct a realistic comparison of the frequencies we applied 3 post - processing steps on the collected data : ( i ) we accounted for sentence variability by normalizing the frequency of each word ( in the HT or the MT output ) by the length of sentences in which it appears , ( ii ) we normalized the frequency of each word ( in the HT or the MT output ) by the accumulated frequency , reducing each frequency to a probability , and ( iii ) to account for the missing words in the MT output we counted words with zero frequencies separately ."
GPA,graph partition algorithm,TR-40245,"table*[t]F1 scores in the task of node classification.tbl:exp-node-classificationtabularcccccc 2*Algorithm & 2*Initialization & 2cMicro - F1 score & 2cMacro - F1 score 3 - 6 & & Blog & Wiki & Blog & Wiki 3*node2vec & GPA & 0.3174 & 0.6310 & 0.2395 & 0.5830 2 - 6 & HARP & 0.3028 & 0.6192 & 0.2281 & 0.5631 2 - 6 & Random & 0.2916 & 0.6033 & 0.2195 & 0.5587 3*DeepWalk & GPA & 0.3399 & 0.6295 & 0.2563 & 0.5616 2 - 6 & HARP & 0.3191 & 0.6029 & 0.2387 & 0.5481 2 - 6 & Random & 0.3106 & 0.5967 & 0.2315 & 0.5380 3*LINE & GPA & 0.3070 & 0.4987 & 0.2082 & 0.4282 2 - 6 & HARP & 0.2823 & 0.4798 & 0.2029 & 0.4165 2 - 6 & Random & 0.2799 & 0.4687 & 0.1982 & 0.4091 tabulartable*Evaluations on Node ClassificationIn node classification , we evaluate the performance of GPA , HARP and Random on the datasets , i.e. , Blog and Wiki , whose nodes are associated with labels ."
LR,logistic regression,TR-40409,"tab : suprestabularllllllllll2 * & 3cRBWH & 3cRCH & 3cGCH & P & R & F1 & P & R & F1 & P & R & F1 SVM & 0.8539 & 0.8122 & 0.8325 & 0.9366 & 0.8811 & 0.9080 & 0.9347 & 0.8810 & 0.9071 SGD & 0.8575 & 0.7329 & 0.7903 & 0.9104 & 0.8276 & 0.8670 & 0.8713 & 0.7951 & 0.8315 NB & 0.9353 & 0.7102 & 0.8074 & 0.8409 & 0.9048 & 0.8717 & 0.8049 & 0.9281 & 0.8621 RF & 0.8508 & 0.7524 & 0.7986 & 0.9182 & 0.7552 & 0.8288 & 0.8654 & 0.8210 & 0.8426 LR & 0.8872 & 0.6912 & 0.7770 & 0.7003 & 0.0725 & 0.1314 & 0.9751 & 0.5043 & 0.6648 CNN & 0.9159 & 0.9028 & 0.9085 * & 0.9370 & 0.9408 & 0.9367 * & 0.9359 & 0.9342 & 0.9335 * tabulartableSemi - supervised Learning Performancesec : semsupresTable tab : sslres presents the performance of the self - trained CNN across RBWH , RCH , and GCH ."
STL,single task learning,TR-40595,tab : sentfulltabularccccccccData & 1 & 2 & 3 & 4 & 5 & 6 & 7 Tasks & 14 & 28 & 56 & 84 & 42 & 86 & 126 tabular[c]@c@Thresholds ( Splits)tabular & 1 ( 1 ) & 2 ( 2 ) & 2 ( 4 ) & 2 ( 6 ) & 3 ( 3 ) & 3 ( 6 ) & 3 ( 9 ) Train Size & 240 & 120 & 60 & 40 & 80 & 40 & 26 STL & 0.749 ( 0.003 ) & 0.429 ( 0.002 ) & 0.432 ( 0.001 ) & 0.429 ( 0.002 ) & 0.400 ( 0.002 ) & 0.399 ( 0.003 ) & 0.397 ( 0.001 ) ITL & 0.713 ( 0.002 ) & 0.433 ( 0.001 ) & 0.440 ( 0.002 ) & 0.431 ( 0.001 ) & 0.499 ( 0.001 ) & 0.486 ( 0.002 ) & 0.479 ( 0.001 ) SHAMO & 0.721 ( 0.005 ) & 0.423 ( 0.002 ) & 0.437 ( 0.006 ) & 0.429 ( 0.002 ) & 0.498 ( 0.006 ) & 0.460 ( 0.002 ) & 0.496 ( 0.013 ) CMTL & 0.713 ( 0.002 ) & 0.557 ( 0.016 ) & 0.436 ( 0.007 ) & 0.429 ( 0.004 ) & 0.508 ( 0.002 ) & 0.486 ( 0.002 ) & 0.476 ( 0.002 ) MTFL & 0.711 ( 0.002 ) & 0.482 ( 0.004 ) & 0.473 ( 0.002 ) & 0.432 ( 0.007 ) & 0.522 ( 0.002 ) & 0.487 ( 0.003 ) & 0.481 ( 0.002 ) GO - MTL & 0.638 ( 0.006 ) & 0.582 ( 0.012 ) & 0.526 ( 0.013 ) & 0.516 ( 0.007 ) & 0.587 ( 0.004 ) & 0.540 ( 0.005 ) & 0.539 ( 0.008 ) BiFactorMTL & 0.722 ( 0.006 ) & 0.611 ( 0.018 ) & 0.561 ( 0.013 ) & 0.598 ( 0.002 ) & 0.643 ( 0.013 ) & 0.578 ( 0.020 ) & 0.574 ( 0.052 ) TriFactorMTL & 0.733 ( 0.006 ) & 0.627 ( 0.008 ) & 0.588 ( 0.006 ) & 0.603 ( 0.012 ) & 0.655 ( 0.013 ) & 0.606 ( 0.020 ) & 0.632 ( 0.029 ) tabulartable table[h ! ]
NE,nash equilibrium,TR-40652,"When players use "" canonical co - evolutionary genetic algorithms "" as learning algorithms , the process of the game is an ergodic Markov Chain , and therefore we analyze simulation results using both the relevant methodology and more general statistical tests , to find that in the "" social "" case , states leading to NE play are highly frequent at the stationary distribution of the chain , in contrast to the "" individual learning "" case , when NE is not reached at all in our simulations ; to find that the expected Hamming distance of the states at the limiting distribution from the "" NE state "" is significantly smaller in the "" social "" than in the "" individual learning case "" ; to estimate the expected time that the "" social "" algorithms need to get to the "" NE state "" and verify their robustness and finally to show that a large fraction of the games played are indeed at the Nash Equilibrium ."
EHS,enhanced hybrid swipt protocol,TR-40720,"So the derived equation after putting the values is like as below : OP_CEU^x_3 = p_F_CEUp_F_CEU+p_N_CEU(1-e^-_CEU^R_3Ps_CEU p_F ) + ( 1-e^(-_CEU^R_3_CCU _ CCU , CEU(21-+ ) ) ) -p_F_CEUp_F_CEU+p_N_CEU ( 1-e^-_CEU^R_3Ps_CEU p_F)(1-e^(-_CEU^R_3_CCU _ CCU , CEU(21-+)))Energy EfficiencyThe harvested energy at CCU by EHS protocol can be derived as below [ 11 ] , In addition , the transmitted power from CCU to CEU can be expressed as below based on the harvested energy [ 21 ] , Moreover , the EE can be derived as below for the proposed EHS - CNOMA scheme [ 21 ] , So Eq.29 shows that EE is related to the and ."
RV,random vaccination,TR-40743,"tikzpicture customlegend[legend columns=4,legend style = at=(0.12,1.02),draw = none , column sep=3ex , line width=2pt , font= , legend entries = RV , AV , IMV , DV , direct , indirect ] solid , color = blue color = red color = green color = magenta color = black dashdotted , color = black customlegend tikzpicture pvan_a.pdfpvan_b.pdfpvan_c.pdfpvan_d.pdfAverage outbreak sizes at various vaccination rates ( percentage of total nodes ) of different strategies : ( A , B ) nodes are vaccinated with contacts created for direct interactions and ( C , D ) comparison of outbreak sizes for vaccinating nodes with contacts based on the direct interactions ( solid lines ) and contact based on any direct or indirect interactions ( dashed lines)-1.5emfig : avacfigurefigure[h ! ]"
OP,old persian,TR-40951,"[ noitemsep]*huarnah-/*farnah- MP xwarrah NP farr ' glory'*uarna - ka- ' wool ' Phl wlk ' , MMP wrg /warrag/ NP barrah ' lamb'*parna- Phl pl , MMP pr /parr/ NP par(r ) ' feather'OP krnuvaka < k - r - nu - u - v - k - a > ' stonemason ' Phl < k(y)lwk ' > , MMP < qrwg > ' artisan ' , generally transcribed as kirrog on the basis of Armenian krogpet , though late OP * uva would seem to yield MP u*d(a)r - n- Phl dl- /darr/ NP darr- ' to rend , tear up'darrah*us - prna- Phl spwl , MMP ' spwr /aspurr/ ' accomplished'It has been suggested that the changes * rn rr and * rn l are interconnected , and that l(l ) r(r ) variation in reflexes of * rn represents dialectal variation within West Iranian ( [ 292 , fn ."
PC,principal component,TR-41016,"@ X rr rr@ & 2 c CNS & 2 c MDC ( lr)2 - 3(lr)4 - 5 & ( PC 0 ) & ( PC 1 ) & ( PC 0 ) & ( PC 1 ) Social circle size , & 0.41 & 0.16 & 0.37 & -0.15 Activity space size , & 0.42 & -0.24 & 0.42 & -0.08 New ties / week , & 0.33 & 0.28 & 0.27 & 0.33 New locations / week , & 0.38 & -0.05 & 0.37 & 0.19 Social circle entropy , & 0.31 & 0.30 & 0.34 & 0.09 Activity space entropy , & 0.38 & -0.16 & 0.30 & -0.07 Social circle stability , & -0.16 & -0.46 & 0.07 & -0.72 Activity space stability , & -0.10 & -0.49 & -0.12 & -0.51 Social circle rank turnover , & -0.20 & 0.28 & -0.33 & 0.10 Activity space rank turnover , & -0.30 & 0.44 & -0.38 & 0.17 Contribution of the original variables to the first principal component ."
FEC,forward error correction,TR-41077,"tabularccccccc -1.5exPacket loss & -1.5exQoE & -1.5exWithout & & -1.5exVideo - aware FEC & & -1.5exViewFEC 1.0exrate & 1.0exMetric & 1.0exFEC & 1.5exVideo - aware FEC & 1.0exImprovement & 1.5exViewFEC & 1.0exImprovement 2*Packet loss 5 & VQM & 3.05 & 1.06 & 65.14 & 1.02 & 66.48 & SSIM & 0.76 & 0.91 & 19.74 & 0.92 & 21.05 2*Packet loss 10 & VQM & 4.01 & 1.11 & 72.36 & 1.12 & 72.09 & SSIM & 0.74 & 0.91 & 22.97 & 0.91 & 22.97 2*Packet loss 15 & VQM & 6.19 & 1.60 & 74.09 & 1.49 & 75.87 & SSIM & 0.50 & 0.90 & 80.00 & 0.89 & 78.00 2*Packet loss 20 & VQM & 8.68 & 1.77 & 79.60 & 1.81 & 79.14 & SSIM & 0.33 & 0.88 & 166.67 & 0.88 & 166.67 tabular tab : vfec : allpktloss center tableTaking into consideration the results of the experiments , it is possible to say that the proposed ViewFEC mechanism showed good performance ."
MF,matrix factorization,TR-41197,"Here , we simply let the latent representation of patient be the sum of its features ' latent vectors : equationp_i = _ uf_i e_u^iequationSimilarly , the latent representation of doctor is also given by the sum of its features ' latent vectors : equationq_j = _ uf_j e_u^jequationThen , MF learns and , such that the predicted score for unobserved entries is given by the inner product of latent patient and doctor representations : equationy_ij = g(i , j p_i , q_j ) = g(p_iq_j ) equation where denotes the function that maps model parameters to the predicted score ."
SPA,simple power analysis,TR-41283,"t]Passive physical attacks and countermeasures & Parameters selection & 2001 [ HTML]FFFFFF & Bleichenbacher and restart & Parameters validation & 2003 [ HTML]FFFFFF & SPA / DPA / doubling & & [ HTML]FFFFFF & & & [ HTML]FFFFFF & SPA / timing & constant runtime & 2012 [ HTML]FFFFFF & & & 2012 [ HTML]FFFFFF & Template with lattice & Large prime finite & 2014 [ HTML]FFFFFF & CPA & & 2015 [ HTML]FFFFFF & SPA & & 2015 [ HTML]FFFFFF & SPA / DPA / ZPA & & 2017 Active attacksThere is another type of SCA attack , which uses errors to reveals some bits , called fault attacks ."
ASA,adaptive segmentation algorithm,TR-41288,"[ ] [ Manual annotation ] [ ] [ EDF image ] [ ] [ ASA mask ] An example from our data set , where a ) is the manual annotation ( counted neurons have green dots ) , b ) is the EDF image , and c ) is the ASA mask for the EDF image shown in ( b ) [ ] [ Initial masks created using ASA followed by human verification ] [ ] [ Iterative human - in - the - loop verification of deep learning predicted masks ] Proposed method in two steps : a ) creating EDF images , and applying ASA , then human verification , and then b ) iterative process using accepted ASA masks / images for training , and ASA masks / images as an active set ."
AI,artificial intelligence,TR-41340,"Our contributions are as follows : itemizeWe enrich current understanding of public perception by presenting results of an in - depth survey focused on AI , conducted across a broad range of countries , including several developing countries ( encompassing in total the United States , Canada , Australia , France , South Korea , Brazil , Nigeria , and India)We identify four key sentiments ( exciting , useful , worrying , and futuristic ) whose prevalence distinguishes responses to AI in different countriesWe report widespread belief that AI will have significant impact on society , including positive expectations of AI in healthcare , as well as concerns about privacy and job lossitemizeIn the remainder of the paper , we review relevant background , describe our methodology , present and discuss our findings , and conclude with a discussion of future work ."
CT,computed tomography,TR-41440,"The total loss can be formulated asequation5eq : hybridlossalignedTP_p(c ) = & _ n=1^Np_n(c)g_n(c ) FN_p(c ) = & _ n=1^N(1-p_n(c))g_n(c ) FP_p(c ) = & _ n=1^Np_n(c)(1-g_n(c ) ) L = & L_Dice + L_Focal = & C - _ c=0^C-1TP_p(c)TP_p(c ) + FN_p(c ) + FP_p(c ) & - 1N_c=0^C-1_n=1^Ng_n(c)(1-p_n(c))^2(p_n(c ) ) , alignedequationwhere , and are the true positives , false negatives and false positives for class calculated by prediction probabilities respectively , is the predicted probability for voxel being class , is the ground truth for voxel being class , is the total number of anatomies plus one ( background ) , is the trade - off between dice loss and focal loss , and are the trade - offs of penalties for false negatives and false positives which are set as 0.5 here , is the total number of voxels in the CT images ."
TPR,true positive rate,TR-41459,"table[]True negative and true positive rates ( in ) for each datasettable : ExperimentalResultstabularllllllll3 - 8 & & 2c & 2c & 2c 2cMethod & TNR & TPR & TNR & TPR & 1lTNR & 1lTPR 2lModSecurity & 76,1 & 34,3 & 61,1 & 72,2 & 1l42,8 & 1l93,0 2lOne - class : & 88,9 & 34,6 & 93,3 & 86,2 & 1l0,0 & 1l98,6 2lCombined OC - MS & 97,3 & 20,1 & 99,1 & 63,0 & 1l42,8 & 1l92,1 1l5*N - grams & n=1 & 99,9 & 93,0 & 93,9 & 95,9 & 2l5 * 2 - 61l & n=2 & 99,9 & 94,8 & 94,4 & 97,6 & 2l 2 - 61l & n=3 & 99,5 & 96,1 & 92,0 & 97,5 & 2l 2 - 61l & n=4 & 96,2 & 96,8 & 90,7 & 98,8 & 2l 2 - 61l & n=5 & 90,9 & 97,5 & 89,4 & 98,9 & 2l 1 - 6tabulartablefigure * pkdd-results.png Results for the dataset ."
RTF,region templates framework,TR-41686,"The specific contributions of this work are presented below with a reference to the section in which they are described : enumerate A graphical user interface for simplifying the deployment of workflows for the RTF , which is coupled with a code generator that allows the flexible use of the RTF on distinct domains [ Section sec : improve ] ; The development and analysis of multi - level reuse algorithms : enumerate A coarse - grain merging algorithm was implemented [ Section sec : stage - merging ] ; A fine - grain Naive Merging Algorithm was proposed and implemented [ Section sec : naive - merging ] ; The fine - grain Smart Cut Merging Algorithm was proposed and implemented [ Section sec : sca ] ; The fine - grain Reuse - Tree Merging Algorithm was proposed and implemented [ Section sec : rtma ] ; enumerate Proposal and implementation of the Task - Balanced Reuse - Tree Merging Algorithm that reduces the issue of loss of parallelism due to load imbalance provoked by the Reuse - Tree Merging Algorithm [ Section sec : TRTMA ] ; The performance gains of the proposed algorithms with a real - world microscopy image analysis application were demonstrated using different SA strategies ( e.g MOAT and VBD ) at different scales ."
MAPE,mean absolute percentage error,TR-41776,"DiscussionChoosing a Control Time Series choosecontroltable*[t]tabularccccc & Treated Series ( lang ; region ) & Control Series ( lang ; region ) & Avg MAPE & tabular[c]@c@Avg MAPE ( No Control Series)tabular Model 1 & hi - wiki page views ; Madhya Pradesh & hi - wiki page views ; other states of India & 7.54 & 11.54 Model 2 & hi - wiki page views ; Madhya Pradesh & other wikis page views ; Madhya Pradesh & 9.31 & 11.54 Model 3 & hi - wiki page views ; all of India & hi - wiki page views ; other countries & 7.22 & 7.92 Model 4 & hi - wiki page views ; all of India & other wikis page views ; all of India & 9.14 & 7.92 tabularComparison of the predictive power of four sets of control time series for the Hindi Wikipedia campaign , of which the treated series and the control series either share the same geographic region or the same Wikipedia language edition ."
MPI,multiple parallel instances,TR-41789,figure[!htb ] subfigure.49 figures / main - RMSD - t_total.pdf format = hang Scaling total ( five repeats ) fig : MPIscaling subfigure subfigure.49 figures / main - RMSD - speed_up.pdf format = hang Speed - up ( five repeats ) fig : MPIspeedup subfigure subfigure.49 figures / main - RMSD - time_comp_IO_comparison.pdf format = hang Scaling for different components ( five repeats ) fig : ScalingComputeIO subfigure subfigure .5 figures / main - RMSD - BarPlot - rank - comparison_72_5.pdf format = hang Time comparison on different parts of the calculations per MPI rank ( example ) fig : MPIranks subfigure Performance of the RMSD task parallelized with MPI on SDSC Comet .
NL,natural language,TR-41849,Evaluating Effectivenesstable [ t ] 1.0 tabular l r c c c c & & N & R & P & F-1 5*minipage0.5 in Keyword Search Systemsminipage & QGA(+TransE ) & 100 & 0.59 & 0.85 & 0.70 & QGA(+Cooccur ) & 100 & 0.49 & 0.76 & 0.59 & QGA(+TFIDF ) & 100 & 0.37 & 0.64 & 0.47 & DPBF & 100 & 0.46 & 0.21 & 0.29 & SUMG & 100 & 0.31 & 0.22 & 0.23 6*minipage0.5 in NL - QA Systemsminipage & CANaLI & 100 & 0.89 & 0.89 & 0.89 & UTQA & 100 & 0.69 & 0.82 & 0.75 & QGA(+TransE ) & 100 & 0.59 & 0.85 & 0.70 & NBFramework & 63 & 0.54 & 0.55 & 0.54 & SemGraphQA & 100 & 0.25 & 0.70 & 0.37 & UIQA & 44 & 0.28 & 0.24 & 0.25 tabular tablenotes N : Number of Processed Queries ; R : Recall ; P : Precision .
VAT,virtual adversarial training,TR-42061,table*[t!]table1 tabularl c c c 2*Model & Omniglot & 2 c Mini - Imagenet & 1-shot & 1-shot & 5-shot PN ren2018metalearning & 94.62 0.09 & 43.61 0.27 & 59.08 0.22 Our CPN : ( PN + VAT ) & 95.66 0.21 & 44.63 0.21 & 64.02 0.20 Our CPN : ( PN + VAT + ENT ) & 97.14 0.16 & 44.48 0.22 & 66.94 0.20 Our CPN ( PN + RW ) & 97.96 0.07 & 50.33 0.27 & 66.99 0.24 Our CPN ( final : PN+RW+VAT ) & 98.03 0.11 & 51.03 0.23 & 67.78 0.20 tabular Ablation Studytbl_ablationtable *
PP,pairwise perturbation,TR-42311,"itemizedefn algorithmCPALSPP : Pairwise perturbation algorithm for CP - ALScpalsppalgorithmic[1]Input : Tensor , Tensor dimension , Decomposition rank , Pairwise perturbation construction tree , Map of TreeNode and Tensor Map , Tolerance , get TreeNode t such that TreeMapPP(Map , , , , t ) algorithmicalgorithmalgorithmTreeMapPP : Construction procedure for PP dimension tree mapeuclidalgorithmic[1]Input : Tensor , Map of TreeNode and Tensor Map , Pairwise perturbation construction tree , , TreeNode tt Map Map[t ] max number in size()=1 qcrIt 's the last matrix to contract Map[t ] Map[t ] Parent[t ] Map TreeMapPP(Map , , , , Parent[t ] ) Map[Parent[t ] ] Map[t ] Map[t ] algorithmicalgorithmlemmalemma : costTo get with , , tensors will be calculated on each level with ."
CT,computed tomography,TR-42471,figure [ ht ] center minipage0.15 ./fig / snapshot0101new3.png minipage minipage0.15 ./fig / snapshot0121new3.png minipage minipage0.15 ./fig / snapshot0131new3.png minipage minipage0.15 ./fig / snapshot0141new3.png minipage minipage0.15 ./fig / snapshot0102new3.png minipage minipage0.15 ./fig / snapshot0122new3.png minipage minipage0.15 ./fig / snapshot0132new3.png minipage minipage0.15 ./fig / snapshot0142new3.png minipage minipage0.15 ./fig / snapshot0104new.png minipage minipage0.15 ./fig / snapshot0124new.png minipage minipage0.15 ./fig / snapshot0134new.png minipage minipage0.15 ./fig / snapshot0144new.png minipage minipage0.15 ./fig / snapshot0105new2.png minipage minipage0.15 ./fig / snapshot0125new2.png minipage minipage0.15 ./fig / snapshot0135new2.png minipage minipage0.15 ./fig / snapshot0145new2.png minipage minipage0.15 ./fig / snapshot0107new.png minipage minipage0.15 ./fig / snapshot0127new.png minipage minipage0.15 ./fig / snapshot0137new.png minipage minipage0.15 ./fig / snapshot0147new.png minipage minipage0.15 ./fig / snapshot0108new2.png minipage minipage0.15 ./fig / snapshot0128new2.png minipage minipage0.15 ./fig / snapshot0138new2.png minipage minipage0.15 ./fig / snapshot0148new2.png minipage center 5fig : vis2 Visualizations for the first four anatomy on the first four holdout CT images .
TD,top - down,TR-42506,"To model this , the incoming presynaptic activities are transformed as following before they make up the afferent input via the respective receptive field of a unit : equationeq : forwardInhalignedp^pre_i = p^pre_i - 1 K _ j^K p^pre_j , & pre BU , LAT , TD I^Source = _ i^K w^Source_i p^pre_i , & Source BU , LAT , TD , alignedequationwhere stands for raw presynaptic activity , is the presynaptic activity transformed by forward inhibition , is the total number of incoming synapses of a certain origin , the weights constitute the receptive field and designates the final computed value of the afferent input from the respective origin ."
AC,actor - critic,TR-42510,table[!h ] adjustwidth-.5in-.5 in font = small tabularlccccccc 2*Algorithm & 2cEncoder & 4cDecoder & 2*BLEU & NN Type & Size & NN Type & Size & Attention & Input Feed & MIXER ranzato2015sequence & 1-layer CNN & 256 & 1-layer LSTM & 256 & Dot - Prod & N & 20.73 BSO wiseman2016sequence & 1-layer BiLSTM & 128 2 & 1-layer LSTM & 256 & Dot - Prod & Y & 27.9 Q(BLEU ) li2017learning & 1-layer BiLSTM & 128 2 & 1-layer LSTM & 256 & Dot - Prod & Y & 28.3 AC bahdanau2016actor & 1-layer BiGRU & 256 2 & 1-layer GRU & 256 & MLP & Y & 28.53 RAML ma2017softmax & 1-layer BiLSTM & 256 2 & 1-layer LSTM & 256 & Dot - Prod & Y & 28.77 VAML & 1-layer BiLSTM & 128 2 & 1-layer LSTM & 256 & Dot - Prod & Y & 28.94 ERAC & 1-layer BiLSTM & 128 2 & 1-layer LSTM & 256 & Dot - Prod & Y & 29.36 NPMT huang2017toward & 2-layer BiGRU & 256 2 & 2-layer LSTM & 512 & N.A. & N.A. & 29.92 NPMT+LM huang2017toward & 2-layer BiGRU & 256 2 & 2-layer LSTM & 512 & N.A. & N.A. & 30.08 ERAC & 2-layer BiLSTM & 256 2 & 2-layer LSTM & 512 & Dot - Prod & Y & 30.85 tabular Comparison of algorithms with detailed architecture information on the IWSTL 2014 dataset for MT .
MSA,modern standard arabic,TR-42605,table1.380mm16mmtabular@lllllllll@ & EGY & GLF & LAV & MSA & NOR & Total Truth & PRC EGY & 221 & 15 & 57 & 13 & 9 & 315 & 50.3 GLF & 45 & 121 & 82 & 12 & 5 & 265 & 55.8 LAV & 74 & 43 & 199 & 18 & 14 & 348 & 46.9 MSA & 19 & 17 & 20 & 218 & 5 & 279 & 77 NOR & 80 & 21 & 66 & 22 & 166 & 355 & 83.4 class & 439 & 217 & 424 & 283 & 199 RCL & 70.2 & 45.7 & 57.2 & 78.1 & 46.8 tabularConfusion Matrix for DID.tab : cmtabletable[H]1.350mm10.5mmtabular@lllllll@ Expected Dialect & EGY & GLF & LAV & NOR & MSA EGY & 65 & & & & 32 GLF & & 41 & 4 & & 53 LAV & 1 & 1 & 53 & & 39 NOR & 1 & & & 69 & 28 tabularExpected dialect of each speech segment from particular dialectal speakers.tab:expansion2table
CT,computed tomography,TR-42644,figure figure [ ht ] center minipage0.15 ./fig / snapshot0109new2.png minipage minipage0.15 ./fig / snapshot0129new2.png minipage minipage0.15 ./fig / snapshot0139new2.png minipage minipage0.15 ./fig / snapshot0149new2.png minipage minipage0.15 ./fig / snapshot0110new3.png minipage minipage0.15 ./fig / snapshot0130new3.png minipage minipage0.15 ./fig / snapshot0140new3.png minipage minipage0.15 ./fig / snapshot0150new3.png minipage minipage0.15 ./fig / snapshot0106new3.png minipage minipage0.15 ./fig / snapshot0126new3.png minipage minipage0.15 ./fig / snapshot0136new3.png minipage minipage0.15 ./fig / snapshot0146new3.png minipage minipage0.15 ./fig / snapshot0103new3.png minipage minipage0.15 ./fig / snapshot0123new3.png minipage minipage0.15 ./fig / snapshot0133new3.png minipage minipage0.15 ./fig / snapshot0143new3.png minipage center 5fig : vis2_2 Visualizations for five anatomies on the first four holdout CT images .
MPI,multiple parallel instances,TR-42827,figure[!htb ] subfigure.49 figures / RMSD - ga4py - t_total.pdf format = hang Scaling total fig : MPIscaling - ga4py subfigure subfigure.49 figures / RMSD - ga4py - speed_up.pdf format = hang Speed - up fig : MPIspeedup - ga4py subfigure subfigure.49 figures / RMSD - ga4py - time_IO_comparison.pdf format = hang Scaling for different components fig : ScalingComputeIO - ga4py subfigure subfigure .5 figures / RMSD - ga4py - BarPlot - rank - comparison_72_1.pdf format = hang Time comparison on different parts of the calculations per MPI rank fig : MPIranks - ga4py subfigure Performance of the RMSD task using Global Arrays on SDSC Comet .
DA,data augmentation,TR-42830,"Multiple Classification based NID*[t ] ML based 4-class NID with SVM(Mean Std - Dev Percent ) * 6p1.5 cm p2.0 cm c c c c 0em 1lCategory & 1lNID Model Name & 1cAccuracy & 1cPrecision & 1cRecall & 1cF1-Score 0em 0em 3*NORMAL & NID - SVM & 76.30 0.01 & 75.91 0.01 & 98.69 0.02 & 85.81 0.01 & NID - PGM - SVM & 76.16 0.08 & 75.90 0.03 & 98.41 0.11 & 85.70 0.06 & NID - DA - SVM & 82.87 0.17 & 98.39 0.20 & 77.68 0.32 & 86.82 0.16 0em 0em 3*DOS & NID - SVM & 94.00 0.01 & 93.35 1.15 & 25.34 0.16 & 39.86 0.10 & NID - PGM - SVM & 93.84 0.10 & 86.98 2.63 & 25.33 0.49 & 39.23 0.80 & NID - DA - SVM & 99.48 0.05 & 94.09 0.58 & 99.70 0.06 & 96.81 0.29 0em 0em 3*PROBE & NID - SVM & 98.82 0.02 & 68.54 0.43 & 83.27 0.53 & 75.19 0.37 & NID - PGM - SVM & 98.80 0.02 & 67.39 0.31 & 85.89 0.38 & 75.53 0.30 & NID - DA - SVM & 99.32 0.04 & 88.70 0.88 & 78.13 1.85 & 83.07 1.00 0em 0em 3*R2L & NID - SVM & 83.43 0.01 & 97.91 0.64 & 4.83 0.02 & 9.20 0.04 & NID - PGM - SVM & 83.34 0.05 & 94.15 5.15 & 4.51 0.02 & 8.60 0.04 & NID - DA - SVM & 83.74 0.19 & 51.75 0.31 & 96.57 0.66 & 67.38 0.23 0em * [ t ] DL based 4-class NID with DNN(Mean Std - Dev Percent ) * 6p1.5 cm p2.0 cm c c c c 0em 1lCategory & 1lNID Model Name & 1cAccuracy & 1cPrecision & 1cRecall & 1cF1-Score 0em 0em 3*NORMAL & NID - DNN & 77.15 3.22 & 76.5 2.81 & 99.15 0.51 & 86.33 1.64 & NID - PGM - DNN & 83.17 1.40 & 81.58 1.31 & 99.26 0.37 & 89.55 0.76 & NID - DA - DNN & 87.96 0.12 & 86.95 0.28 & 98.15 0.61 & 92.21 0.11 0em 0em 3*DOS & NID - DNN & 93.81 2.94 & 89.70 6.58 & 23.55 9.17 & 27.72 7.37 & NID - PGM - DNN & 99.04 0.22 & 97.84 3.73 & 89.97 0.98 & 93.7 1.34 & NID - DA - DNN & 99.59 0.08 & 97.14 1.36 & 97.65 0.40 & 97.39 0.52 0em 0em 3*PROBE & NID - DNN & 98.92 0.31 & 74.54 1.93 & 81.49 8.26 & 76.79 3.29 & NID - PGM - DNN & 98.40 0.07 & 64.93 1.90 & 55.84 7.72 & 59.78 3.86 & NID - DA - DNN & 99.05 0.07 & 75.27 1.24 & 83.47 4.52 & 79.11 2.17 0em 0em 3*R2L & NID - DNN & 83.90 2.72 & 58.99 3.87 & 7.55 5.81 & 11.28 3.13 & NID - PGM - DNN & 84.90 1.26 & 88.45 5.67 & 13.92 7.48 & 23.60 2.57 & NID - DA - DNN & 89.09 0.10 & 92.17 4.86 & 40.97 1.82 & 56.64 0.92 0em In order to verify the performance of the DA module on enhancing the existing learning based IDSs , we have undertaken multi - class based NID experiments ."
BU,bottom - up,TR-42971,"To model this , the incoming presynaptic activities are transformed as following before they make up the afferent input via the respective receptive field of a unit : equationeq : forwardInhalignedp^pre_i = p^pre_i - 1 K _ j^K p^pre_j , & pre BU , LAT , TD I^Source = _ i^K w^Source_i p^pre_i , & Source BU , LAT , TD , alignedequationwhere stands for raw presynaptic activity , is the presynaptic activity transformed by forward inhibition , is the total number of incoming synapses of a certain origin , the weights constitute the receptive field and designates the final computed value of the afferent input from the respective origin ."
LDA,linear discriminant analysis,TR-43075,"In particular , LDA looks for the directions that are most effective for discrimination by minimizing the ratio between the intra - category ( ) and inter - category ( ) scatters : where is the number of super - pixels in all training images , is the number of categories , is the number of super - pixels for the -th category , , , is the feature vector of one training super - pixel , is the category label of the -th super - pixel in the training images , is the mean of feature vector of training super - pixels , and is the mean of the -th category ."
ANN,artificial neural network,TR-43151,"tab : litreviewtabularl l l l l l l lReference & Year & Knowledge- & Experience- & Data- & Model- & Hybrid & Other methods & & based & based & Driven & based & & luo2003model & 2003 & & & & & & schwabacher2005survey & 2005 & & & & & & jardine2006review & 2006 & & & & & & AI lee2006intelligent & 2006 & & & & & & goh2006review & 2006 & & & & & & kothamasu2009system & 2006 & & & & & & Reliability , Stochasticcoble2008prognostic & 2008 & & & & & & Stress and effects - based heng2009rotating & 2009 & & & & & & sikorska2011prognostic & 2011 & & & & & & Life Expectancy , ANN si2011remaining & 2011 & & & & & & ahmadzadeh2014remaining & 2014 & & & & & & tsui2015prognostics & 2014 & & & & & & SA , Stochastic , ANNtsui2015prognostics & 2015 & & & & & & tabulartabletable[htbp ! ]"
RRC,rank residual constraint,TR-43168,"a ) Original image ; ( b ) JPEG compressed image ( PSNR = 28.00dB , SSIM = 0.7551 ) ; ( c ) SA - DCT ( PSNR = 28.97dB , SSIM = 0.7789 ) ; ( d ) PC - LRM ( PSNR = 28.97dB , SSIM = 0.7739 ) ; ( e ) ANCE ( PSNR = 29.05dB , SSIM = 0.7852 ) ; ( f ) WNNM ( PSNR = 28.90dB , SSIM = 0.7684 ) ; ( g ) CONCOLOR ( PSNR = 28.97dB , SSIM = 0.7833 ) ; ( h ) SSR - QC ( PSNR = 28.92dB , SSIM = 0.7810 ) ; ( i ) LERaG ( PSNR = 28.97dB , SSIM = 0.7865 ) ; ( j ) RRC ( PSNR = 29.14dB , SSIM = 0.7997 ) ."
DBP,discrete base problem,TR-43293,"htbp ] @XXXXXX Dataset & k & Asso & & NaiveCol & topFiberMChess & 1 & 0.497 & 0.447 & 0.119 & 0.610 & 2 & 0.574 & 0.506 & 0.177 & 0.625 & 5 & 0.628 & 0.621 & 0.323 & 0.667 & 10 & 0.703 & 0.710 & 0.461 & 0.724 DBLP & 1 & 0.111 & 0.131 & 0.111 & 0.187 & 2 & 0.217 & 0.238 & 0.217 & 0.293 & 5 & 0.475 & 0.468 & 0.475 & 0.495 & 10 & 0.738 & 0.692 & 0.738 & 0.738Firewall 1 & 1 & 0.726 & 0.688 & 0.651 & 0.724 & 2 & 0.818 & 0.841 & 0.804 & 0.847 & 5 & 0.908 & 0.951 & 0.932 & 0.953 & 10 & 0.917 & 0.979 & 0.976 & 0.980 Mushroom & 1 & 0.226 & 0.131 & 0.129 & 0.253 & 2 & 0.323 & 0.235 & 0.234 & 0.305 & 5 & 0.461 & 0.504 & 0.398 & 0.425 & 10 & 0.555 & 0.613 & 0.512 & 0.522 Paleo & 1 & 0.027 & 0.027 & 0.027 & 0.027 & 2 & 0.047 & 0.047 & 0.047 & 0.049 & 5 & 0.105 & 0.106 & 0.105 & 0.106 & 10 & 0.182 & 0.181 & 0.182 & 0.182Count best & & 5 & 4 & 3 & 16 Coverage of low ranks ( DBP view ) , bold values are the best in row ."
PBS,primary base station,TR-43388,"These signals are respectively given aswhere and are the channel vector between the PBS and the th PU and that between the CBS and the th PU in the th cluster , respectively ; and denote the channel vector between the PBS and the th SU and that between the CBS and the th SU , respectively ; and are the channel vector between the PBS and the th EHR and that between the CBS and the th EHR in the th cluster , respectively ; and represent the channel vector between the PBS and the th EHR and that between the CBS and the th EHR in the secondary network , respectively ."
HPC,high performance computing,TR-43525,"adjustboxmax width= tabularc c c c c c c c c Name & Nodes & Number of Nodes & CPUs & RAM & Network Topology & Scheduler and Resource Manager & parallelfile system SDSC Comet & Compute & 6400 & 2 Intel Xeon ( E5 - 2680v3 ) 12 cores / CPU , 2.5 GHz & 128 GB DDR4 DRAM & 56 Gbps IB & SLURM & Lustre PSC Bridges & RSM & 752 & 2 Intel Haswell ( E5 - 2695 v3 ) 14 cores / CPU , 2.3 GHz & 128 GB , DDR4 - 2133MHz & 12.37 Gbps OPA & SLURM & Lustre LSU SuperMIC & Standard & 360 & 2 Intel Ivy Bridge ( E5 - 2680 ) 10 cores / CPU , 2.8 GHz & 64 GB , DDR3 - 1866MHz & 56 Gbps IB & PBS & Lustre tabular adjustbox [ Configuration of HPC resources ] Configuration of the HPC resources that were benchmarked ."
RS,rate - selective,TR-43729,"OP : Using the expressions given by Eq : OP_FSDF_Difm and Eq : OP_FSDF_Equalm over INID and IID Nakagami- fading channels , respectively , with integer fading parameters , the end - to - end OP of pure RS is easily obtained asFor rate - selective RS , a closed - form expression for over INID Nakagami- fading with arbitrary 's can be easily obtained by substituting Eq : Gamma_CDF and Eq : CDF_gbest_SD_Distinct_C to , yieldingASEP : Following the MGF - based approach and using the expressions for pure RS given by Eq : MGF_end_Final and Eq : MGF_end_Final_Equalm over INID and IID Nakagami- fading channels , respectively , with integer fading parameters , the ASEP of several modulation formats for pure RS can be easily calculated ."
FTE,foveal tilt effects,TR-43737,"the edge maps at fine to medium scales with the overlayed Hough lines are shown in [ fig : B5]Figs to [ fig : B7]. We show the mean tilt angles measured in the DoG edge maps across multiple scales in [ appendix : AppxC]Appendix C.figuresectionC. Quantitative mean tiltsThe absolute mean tilts and the standard errors of detected tilt angles for the Cafe Wall variations tested are provided here in Figs [ fig:]Figs and [ fig : C2]. For the ' foveal tilt effect ' ( FTE , explained in [ sec:3.1.1]Sections and [ sec:3.1.2]Section ) , we used the near horizontal mean tilts at scale 4 , and reflected these values to [ fig:6]Fig ."
CT,computed tomography,TR-43797,figure [ ht ] center minipage0.15 ./fig / snapshot0001new3.png minipage minipage0.15 ./fig / snapshot0011new3.png minipage minipage0.15 ./fig / snapshot0041new3.png minipage minipage0.15 ./fig / snapshot0091new3.png minipage minipage0.15 ./fig / snapshot0002new3.png minipage minipage0.15 ./fig / snapshot0012new3.png minipage minipage0.15 ./fig / snapshot0042new3.png minipage minipage0.15 ./fig / snapshot0092new3.png minipage minipage0.15 ./fig / snapshot0004new.png minipage minipage0.15 ./fig / snapshot0014new.png minipage minipage0.15 ./fig / snapshot0044new.png minipage minipage0.15 ./fig / snapshot0094new.png minipage minipage0.15 ./fig / snapshot0005new2.png minipage minipage0.15 ./fig / snapshot0015new2.png minipage minipage0.15 ./fig / snapshot0045new2.png minipage minipage0.15 ./fig / snapshot0095new2.png minipage minipage0.15 ./fig / snapshot0007new.png minipage minipage0.15 ./fig / snapshot0017new.png minipage minipage0.15 ./fig / snapshot0047new.png minipage minipage0.15 ./fig / snapshot0097new.png minipage minipage0.15 ./fig / snapshot0008new2.png minipage minipage0.15 ./fig / snapshot0018new2.png minipage minipage0.15 ./fig / snapshot0048new2.png minipage minipage0.15 ./fig / snapshot0098new2.png minipage center 5fig : vis1 Visualizations on four test CT images .
MPI,multiple parallel instances,TR-43812,sidewaystable[hp]adjustboxmax width = tabularc c c c c c c c c c c c 10r ( r)5 - 12 Cluster & Gather & File Access & Time & Serial & tabularc Comet : 24 Bridges : 24 SuperMIC : 20 tabular & tabularc Comet : 48 Bridges : 48 SuperMIC : 40 tabular & tabularc Comet : 72 Bridges : 60 SuperMIC : 80 tabular & tabularc Comet : 96 Bridges : 78 tabular & tabularc Comet : 144 Bridges : 84 SuperMIC : 160tabular & Comet : 192 & tabularc Comet : 384 SuperMIC : 320tabular Comet & MPI & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - & - & - Bridges & MPI & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - & - & - SuperMIC & MPI & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & - Comet & GA & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - & - & - Comet & MPI & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - SuperMIC & MPI & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & - Comet & GA & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - SuperMIC & GA & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & - Comet & MPI & PHDF5 & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular Bridges & MPI & PHDF5 & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - SuperMIC & MPI & PHDF5 & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & tabularc tabular tabularadjustboxComparison of the compute and I / O scaling for different test cases and number of processes .
NC,node classification,TR-43829,"The reasons for these observations could be : 1 ) DMF and DeepWalk are both network methods and differ only in one aspect - they are different types of methods ( RS versus NC ) ; 2 ) DeepWalk and the non - network method both use the logistic regression to make predictions and differ only in one aspect - DeepWalk uses network features while the non - network method does not ; and 3 ) DMF differs from the non - network method in two ways : the former is a network method and it does not use logistic regression to make predictions , while the latter is a non - network method that uses logistic regression ."
MER,maximum entropy regularizer,TR-43937,"tabularcccccccccUnit ( ) & Accuracy & A10 & F10 & I10 & SDF10 & SDI10 & & Baseline & 62.640.26 & 64.400.29 & 0.210.32 & 25.010.33 & 10.170.36 & 18.800.31 & 6.490.10 & 8.420.11 w/o MER , DOS & 68.320.09 & 67.160.42 & 1.590.37 & 16.030.28 & 5.960.27 & 20.250.46 & 3.720.11 & 9.690.13 w/o DOS & 70.070.19 & 70.420.07 & 0.310.16 & 12.780.18 & 5.510.18 & 16.610.55 & 3.170.05 & 8.550.13 w/o MER & 69.650.18 & 70.410.21 & -0.140.35 & 13.290.15 & 5.380.05 & 16.710.46 & 3.560.08 & 8.190.12 ( Full Model ) & 72.510.17 & 73.350.35 & 0.130.16 & 9.680.44 & 5.050.10 & 13.140.28 & 2.960.06 & 6.550.26 tabularAblation Study ."
DI,direct inspection,TR-43969,"Finding the optimal policy corresponds to solve the following optimization problem [ v01]Resources to sense[v02]Resources to sense at time [ v03]Type I error probabilities[v04]Type II error probabilities[v05]Sensing matrix[v06]Mixing coefficients at time [ v07]Tests for resource [ v08]Cycle for a single test[v09]Set of cycles[v10]Test thresholds[v11]Decision rules[v12]Set of edges[v13]Observations ' distribution parameter[v14]Time horizon[v15]Number of designed tests[v16]Maximum number of mixed sub - bands per test[v17]Resources[v18]Average received noise power[v19]Probability of declaring given [ v20]Reward for resource [ v21]Penalty for resource [ v22]Resources ' binary state[v23]Average received signal power[v24]Observation[v25]Resources ' prior beliefDynamic Design of Sensing MatricesDirect Inspection ( DI ) caseIn the DI case , we limit to have only one non - zero entry , i.e. , ."
PSC,pittsburgh supercomputing center,TR-43992,figure[!htb ] subfigure.4 figures / main - RMSD - t_total - Bridges.pdf Scaling total fig : MPIscaling - Bridges subfigure subfigure.4 figures / main - RMSD - speed_up - Bridges.pdf Speed - up fig : MPIspeedup - Bridges subfigure subfigure.4 figures / main - RMSD - time_comp_IO_comparison - Bridges.pdf format = hang Scaling for different components fig : ScalingComputeIO - Bridges subfigure subfigure .5 figures / main - RMSD - BarPlot - rank - comparison_72_4-Bridges.pdf format = hang Time comparison on different parts of the calculations per MPI rank ( example ) fig : MPIranks - Bridges subfigure PSC Bridges : Performance of the RMSD task .
DA,dialogue acts,TR-44075,table*[t]tab : LSTM_result Baseline and DAMIC LSTM results with or without utterance collapsing -1 cm -1cmtabularlllllllllllllll2l2 * & 1c2*F1 & 12cF1 per DA 4 - 15 2l & 1c & CQ & FD & FQ & GG & IR & JK & NF & O & OQ & PA & PF & RQ 2*Baseline & -collapsed & 51.85 & 16.54 & 49.93 & 28.50 & 49.06 & 32.56 & 7.14 & 22.55 & 1.81 & 85.35 & 72.76 & 35.12 & 19.41 2 - 15 & + collapsed & 52.48 & 19.04 & 47.81 & 23.73 & 51.54 & 27.60 & 6.64 & 25.52 & 1.13 & 86.88 & 73.86 & 34.68 & 17.56 2*DAMIC & -collapsed & 64.39 & 23.83 & 55.79 & 27.87 & 75.88 & 34.50 & 8.06 & 32.31 & 3.10 & 93.60 & 85.40 & 50.07 & 34.42 2 - 15 & collapsed & 65.00 & 21.22 & 52.13 & 27.76 & 77.91 & 34.86 & 8.93 & 32.54 & 2.09 & 93.59 & 85.73 & 52.99 & 32.14 tabulartable *
BM,black males,TR-44154,"[ cloud , below of = gc , node distance = 2 cm ] ( m ) Race Classification ; [ cloud , below of = init , node distance = 2 cm ] ( f ) Race Classification ; [ block3 , below left of = m , node distance = 1.5 cm ] ( of ) BM Age Estimator ; [ block3 , below right of = m , node distance = 1.5 cm ] ( hf ) WM Age Estimator ; [ block3 , below left of = f , node distance = 1.5 cm ] ( om ) BF Age Estimator ; [ block3 , below right of = f , node distance = 1.5 cm ] ( hm ) WF Age Estimator ; [ - > ] ( init ) - ( pp ) ; [ - > ] ( pp ) - ( gc ) ; [ - > ] ( gc ) - node Male ( m ) ; [ - > ] ( gc.south ) - + + ( 0,-.3 cm ) - ( f ) node[near end , above left , yshift=-4pt ] Female ; [ - > ] ( m ) - ( of ) ; [ - > ] ( m ) - ( hf ) ; [ - > ] ( f ) - ( om ) ; [ - > ] ( f ) - ( hm ) ; tikzpicture adjustboxOverview of race - composite age prediction framework ."
MI,motor imagery,TR-44185,"& Task&Segment & Feature Extraction&Classifier&Performance 4*Biased & 2*Abdullah2010 & 2 * 10 & 2 * 5 & 2*[2]weeks & 2 * 4 & EC & 2*[5]s & 2*AR & 2*ANN&CRR = [ 97.0 ] & & & & & & EO & & & & CRR = [ 96.0]2 - 11 & Riera2008&51&4 & 34 [ 74]days & 2 & EC & [ 4]s & AR , PSD , MuI , COH , CC & FDA & EER = [ 3.4 ] 2 - 11 & Su2010&40&2 & - & 1 & EC & [ 180]s & AR , PSD & KNN , LDA & CRR = [ 97.5 ] 8*Rigorous&Marcel2007&9&3&[3]days & 8 & MI & [ 1]s & PSD & MAP model & HTER = [ 19.3 ] 2 - 11 & Lee2013&4&2 & [ 10]days - [ 5]months & 1 & EC & [ 50]s & PSD & LDA & AC = [ 100 ] 2 - 11 & 2*Rocca2013 & 2 * 9 & 2 * 2 & 2 * 1 - [ 3]weeks & 3 & 2*EC & 2*[1]s & 2*AR & 2*Linear classier & CRR = [ 100 ] & & & & & 5 & & & & & CRR = [ 100 ] 2 - 11 & 2*Armstrong2015 & 15 & 2 & 5 - [ 40]days & 2 * 1 & 2*ERP & 2*[1.1]s & 2*Time - series & 2*CC & CRR = [ 89.0 ] & & 9 & 3 & 134 - [ 188]days & & & & & & CRR = [ 93.0 ] 2 - 11 & 2*Maiorana2016a & 2 * 50 & 2 * 3 & 2*Ave . ["
OP,outage probability,TR-44213,"OP : Using the expressions given by Eq : OP_FSDF_Difm and Eq : OP_FSDF_Equalm over INID and IID Nakagami- fading channels , respectively , with integer fading parameters , the end - to - end OP of pure RS is easily obtained asFor rate - selective RS , a closed - form expression for over INID Nakagami- fading with arbitrary 's can be easily obtained by substituting Eq : Gamma_CDF and Eq : CDF_gbest_SD_Distinct_C to , yieldingASEP : Following the MGF - based approach and using the expressions for pure RS given by Eq : MGF_end_Final and Eq : MGF_end_Final_Equalm over INID and IID Nakagami- fading channels , respectively , with integer fading parameters , the ASEP of several modulation formats for pure RS can be easily calculated ."
CC,core connected,TR-44275,"0Some of our important contributions are as follows : ( i ) introducing the CC property and the analytical justification proving that the core periphery structure of a network can be used to identify top central nodes , ( ii ) an algorithm based on time series forecasting to predict the overlap of the top central nodes across time points ( iii ) an algorithm to exactly pinpoint the ids of the nodes that have high centrality when the network is available in time and ( iv ) a detailed validation scheme to show that the behavior of the high centrality nodes predicted by our algorithm indeed matches the behavior of the actual high centrality nodes ."
RS,rate - selective,TR-44415,"OP : Using the expressions given by Eq : OP_FSDF_Difm and Eq : OP_FSDF_Equalm over INID and IID Nakagami- fading channels , respectively , with integer fading parameters , the end - to - end OP of pure RS is easily obtained asFor rate - selective RS , a closed - form expression for over INID Nakagami- fading with arbitrary 's can be easily obtained by substituting Eq : Gamma_CDF and Eq : CDF_gbest_SD_Distinct_C to , yieldingASEP : Following the MGF - based approach and using the expressions for pure RS given by Eq : MGF_end_Final and Eq : MGF_end_Final_Equalm over INID and IID Nakagami- fading channels , respectively , with integer fading parameters , the ASEP of several modulation formats for pure RS can be easily calculated ."
RV,random vaccination,TR-44493,"tikzpicture customlegend[legend columns=4,legend style = at=(0.12,1.02),draw = none , column sep=3ex , line width=2pt , font= , legend entries = RV , AV , IMV , DV , direct , indirect ] solid , color = blue color = red color = green color = magenta color = black dashdotted , color = black customlegend tikzpicture pvan_a.pdfpvan_b.pdfpvan_c.pdfpvan_d.pdfAverage outbreak sizes at various vaccination rates ( percentage of total nodes ) of different strategies : ( A , B ) nodes are vaccinated with contacts created for direct interactions and ( C , D ) comparison of outbreak sizes for vaccinating nodes with contacts based on the direct interactions ( solid lines ) and contact based on any direct or indirect interactions ( dashed lines)-1.5emfig : avacfigurefigure[h ! ]"
CI,current instruction,TR-44554,"As shown in Table , a general propagation rule for any operation ( O ) should be able to decide that what the tag on the program counter in the next machine state ( ) and the tag on the instruction ’s result ( R ) should be if the current tag on the program counter is PC , the tag on the current instruction is CI , the tags on its input operands ( if any ) are OP1 and OP2 , and the tag on the memory location ( in case of load / store ) is MR ."
ASA,adaptive segmentation algorithm,TR-44662,"htp]Results of the proposed method that shows the number of accepted images from active set in every iterartion , and the error rate ( ) on a test set ( Mouse i d 17)[][ASA][][deep learning ] Example from our data set , where a ) the ASA masks contour overlaid on manual annotation image ( counted neurons have blue marks ) , b ) the iterative deep learning predicted masks contour ( accepted on the fifth iteration of our iterative deep learning based unbiased stereology ) overlaid on manual annotation imageTest mouse cells count using manual , ASA , and Unet ( deep learning ) [ !"
SMC,sliding mode control,TR-44687,"In this context , there are some alternatives already available in the literature to suppress the disturbance while tracking the commanded trajectories including MPC [ 28],[29 ] , dynamic inversion control [ 30],[31 ] , and adaptive control [ 32],[33]. Among them , SMC is one of the strong candidates to handle internal and/or external disturbances and the system uncertainties [ 34],[35]. In this context , SMC - based approaches are investigated in a simulated environment for the attitude and the position loops of the UAVs [ 36]. As a real - time experiment , a finite time stabilizing SMC is applied on a ground - based experimental setup [ 37]. However , the evaluation is restricted to the attitude channel of the system ."
OP,old persian,TR-44811,"PIr * nThere are relatively few Proto - Iranian sources of the cluster * n , but these are realized as sn across the board in West Iranian , to the exclusion of the possible Median proper name in Akkadian Pa - at - ni - e - sa- = Med * Paniiesa- * pani - aisa- ' looking for a wife ' , and possibly the OP form ( found in the Susa inscription of Darius ) krnuvaka- ' stonemason ' , if from * krt - nu - aka following , though is less certain regarding the presence of * -t- and other scholars make no mention of etymological * -t- ."
CT,computed tomography,TR-44898,"The total loss can be formulated asequation5eq : hybridlossalignedTP_p(c ) = & _ n=1^Np_n(c)g_n(c ) FN_p(c ) = & _ n=1^N(1-p_n(c))g_n(c ) FP_p(c ) = & _ n=1^Np_n(c)(1-g_n(c ) ) L = & L_Dice + L_Focal = & C - _ c=0^C-1TP_p(c)TP_p(c ) + FN_p(c ) + FP_p(c ) & - 1N_c=0^C-1_n=1^Ng_n(c)(1-p_n(c))^2(p_n(c ) ) , alignedequationwhere , and are the true positives , false negatives and false positives for class calculated by prediction probabilities respectively , is the predicted probability for voxel being class , is the ground truth for voxel being class , is the total number of anatomies plus one ( background ) , is the trade - off between dice loss and focal loss , and are the trade - offs of penalties for false negatives and false positives which are set as 0.5 here , is the total number of voxels in the CT images ."
AI,artificial intelligence,TR-45149,"tab : litreviewtabularl l l l l l l lReference & Year & Knowledge- & Experience- & Data- & Model- & Hybrid & Other methods & & based & based & Driven & based & & luo2003model & 2003 & & & & & & schwabacher2005survey & 2005 & & & & & & jardine2006review & 2006 & & & & & & AI lee2006intelligent & 2006 & & & & & & goh2006review & 2006 & & & & & & kothamasu2009system & 2006 & & & & & & Reliability , Stochasticcoble2008prognostic & 2008 & & & & & & Stress and effects - based heng2009rotating & 2009 & & & & & & sikorska2011prognostic & 2011 & & & & & & Life Expectancy , ANN si2011remaining & 2011 & & & & & & ahmadzadeh2014remaining & 2014 & & & & & & tsui2015prognostics & 2014 & & & & & & SA , Stochastic , ANNtsui2015prognostics & 2015 & & & & & & tabulartabletable[htbp ! ]"
VSI,voltage source inverter,TR-45252,"Block Diagram of control of VSI fed 3-phase Induction MotorThe speed in RPM of the 3-phase induction motor is given by where , is the speed in RPM of the motor at a given instant is the frequency of the voltage supplied to the motor is the number of poles of the motor The Torque - slip characteristics of a 3-phase induction motor is given bywhere , is the torque produced by the motor is the slip which is defined as is the synchronous speed ( maximum speed ) of the 3-phase induction motor is the voltage supplied to the motor , are the resistance and the impedance in the rotor circuit of the induction motor is a function of angular speed at the shaft of the motor which can be expressed as From equationeq : rpm_omega , equationeq : slip can be written as where , Under operating conditions , , which implies , Also from the design characteristics of the inverter , is directly proportional to the output voltage of the potentiometer inside the accelerator pedal , which is also the input command to inverter ."
PS,power splitting,TR-45257,"The received signal at and from the BS by different OAM modes are given belowAccordingly , the received SINR for symbol at can be expressed as [ 27 - 31]where is the singular value of the channel response matrix of CNOMA - SWIPT - PS - OAM system [ 22,27,30]. OAM beam has divergence in its high - intensity region which caused attenuation [ 27 - 30]. By using Fresnel - zone - plate lenses antenna at BS , this issue can be mitigated without affecting the helical phase profile of OAM beam [ 27 - 30]. So , similarly the received SINR for symbol at can be expressed as [ 29 - 31]where similarly as before is the singular value of the channel response matrix of the proposed system [ 22,27,30]. Achievable capacity analysisBy considering normalized total time duration ( ) ."
US,uncertainty sampling,TR-45358,fig : AAAI_first_pageminipageminipage.48 0.85 tabular[b]cccccc & 3cProposed & 2cClassic & EDG & ext1 & ext2 & US ( + Div ) & Rnd / Div Intepretable & Gray & Gray & 2*No & 2*No & Gray Strategy & Gray-2*Yes & Gray-2*Yes & & & Gray-2*Yes Robustness & Gray & GrayL & GrayL & 2*Low & Gray to Noise & Gray-2*High & GrayL-2*Med & GrayL-2*Med & & Gray-2*High Output & GrayL & GrayL & 2*Prob & 2*Prob & Gray Required & GrayL-2*Pred & GrayL-2*Pred & & & Gray-2*None Validation & 2*Yes & Gray & Gray & Gray & Gray Required & & Gray-2*No & Gray-2*No & Gray-2*No & Gray-2*No Label Cost & GrayL & GrayL & Gray & Gray & 2*Low Reduction & GrayL-2*Med & GrayL-2*Med & Gray-2*High & Gray-2*High & tabular tableComparison between different sampling approaches .
RB,resource blocks,TR-45431,"The achievable FH data rate of user served from a BS , where , over RB can be given as : where is the BS transmitted power allocated to RB , is the noise power , and is the inter - cell interference at the user caused by closest BS ( no intra - cell interference on the downlink direction between different tiers is assumed ) and expressed as follows : where is representing the exclusivity of the TBS and RB allocation : , if RB of nearby station is allocated to another user from TBS , and , otherwise ."
FA,fractional anisotropy,TR-45533,table tabular[t]ll 2cGender Set 1 & RAVLT Total ( 1 - 5 ) & FA Cingulum L Set 2 & FA Medial lemniscus L & FA Cingulum ( hippocampus ) L & FA Post thalamic radiation L Set 3 & FA Corticospinal tract R & FA Superior O.F. fasciculus R tabular tabular[t]ll 2cGenotype : APOE4 Digit Span Backward Raw Score & Stroop Color - word Score PiB Cingulum Post L & PiB Cingulum Post R PiB Frontal Med Orb L & PiB Frontal Med Orb R PiB Precuneus L & PiB Precuneus R PiB SupraMarginal & PiB Temporal Mid R tabular Group difference across Gender ( left ) and Genotype APOE4 expression ( right ) .
SL,strictly local,TR-45541,table*[t]Accuracy on Target SL Stringsets Early Stoppingtab : resultsSLES4.5pttabularcccccccccc2c2Training & 2Test & 3cLSTM & 3cs - RNN & 2RPNI & & & 10 & 30 & 100 & 10 & 30 & 100 & 6SL2 & 21k & 1 & 0.818 ( 0.03 ) & 0.843 ( 0.05 ) & 0.923 ( 0.02 ) & 0.848 ( 0.06 ) & 0.904 ( 0.03 ) & 0.930 ( 0.03 ) & 0.855 & & 2 & 0.780 ( 0.06 ) & 0.820 ( 0.07 ) & 0.905 ( 0.04 ) & 0.871 ( 0.10 ) & 0.980 ( 0.02 ) & 0.992 ( 0.01 ) & 0.844 3 - 10 & 210k & 1 & 0.925 ( 0.07 ) & 0.851 ( 0.04 ) & 0.875 ( 0.04 ) & 0.936 ( 0.05 ) & 0.884 ( 0.07 ) & 0.729 ( 0.12 ) & 1.000 & & 2 & 0.919 ( 0.09 ) & 0.836 ( 0.06 ) & 0.835 ( 0.10 ) & 0.964 ( 0.07 ) & 0.868 ( 0.11 ) & 0.753 ( 0.15 ) & 1.0003 - 10 & 2100k & 1 & 0.737 ( 0.14 ) & 0.711 ( 0.14 ) & 0.730 ( 0.03 ) & 0.869 ( 0.15 ) & 0.767 ( 0.17 ) & 0.625 ( 0.01 ) & 1.000 & & 2 & 0.727 ( 0.15 ) & 0.698 ( 0.15 ) & 0.711 ( 0.04 ) & 0.885 ( 0.17 ) & 0.766 ( 0.19 ) & 0.605 ( 0.01 ) & 1.0006SL4 & 21k & 1 & 0.898 ( 0.01 ) & 0.939 ( 0.01 ) & 0.945 ( 0.01 ) & 0.908 ( 0.01 ) & 0.945 ( 0.01 ) & 0.958 ( 0.01 ) & 0.918 & & 2 & 0.829 ( 0.01 ) & 0.888 ( 0.01 ) & 0.887 ( 0.00 ) & 0.840 ( 0.01 ) & 0.883 ( 0.01 ) & 0.898 ( 0.01 ) & 0.8133 - 10 & 210k & 1 & 0.953 ( 0.05 ) & 0.956 ( 0.04 ) & 0.997 ( 0.00 ) & 0.976 ( 0.03 ) & 0.982 ( 0.00 ) & 0.981 ( 0.00 ) & 0.995 & & 2 & 0.934 ( 0.06 ) & 0.932 ( 0.05 ) & 0.995 ( 0.01 ) & 0.973 ( 0.04 ) & 0.989 ( 0.00 ) & 0.990 ( 0.00 ) & 0.978 3 - 10 & 2100k & 1 & 0.994 ( 0.00 ) & 0.973 ( 0.07 ) & 1.000 ( 0.00 ) & 0.990 ( 0.00 ) & 0.990 ( 0.00 ) & 0.987 ( 0.00 ) & 1.000 & & 2 & 0.996 ( 0.00 ) & 0.975 ( 0.07 ) & 1.000 ( 0.00 ) & 0.996 ( 0.00 ) & 0.996 ( 0.00 ) & 0.995 ( 0.00 ) & 1.0006SL8 & 21k & 1 & 0.966 ( 0.02 ) & 0.983 ( 0.01 ) & 0.995 ( 0.00 ) & 0.962 ( 0.02 ) & 0.969 ( 0.01 ) & 0.971 ( 0.01 ) & 0.991 & & 2 & 0.971 ( 0.01 ) & 0.980 ( 0.02 ) & 0.994 ( 0.00 ) & 0.969 ( 0.02 ) & 0.974 ( 0.01 ) & 0.977 ( 0.01 ) & 0.9663 - 10 & 210k & 1 & 0.990 ( 0.01 ) & 0.996 ( 0.00 ) & 0.999 ( 0.00 ) & 0.998 ( 0.00 ) & 0.999 ( 0.00 ) & 0.999 ( 0.00 ) & 0.998 & & 2 & 0.994 ( 0.00 ) & 0.995 ( 0.00 ) & 0.998 ( 0.00 ) & 0.997 ( 0.00 ) & 0.998 ( 0.00 ) & 0.998 ( 0.00 ) & 0.9943 - 10 & 2100k & 1 & 0.993 ( 0.01 ) & 0.998 ( 0.00 ) & 1.000 ( 0.00 ) & 0.999 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 & & 2 & 0.994 ( 0.01 ) & 0.999 ( 0.00 ) & 1.000 ( 0.00 ) & 0.999 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000tabulartable *
EM,expectation maximization,TR-45781,"Expectation Maximization ( EM ; ) takes advantage of this and instead optimizes a lower bound given by the expected log likelihood : Iterative optimization of this bound alternates between two steps : in the E - step we compute a new estimate of the posterior probability distribution over the latent variables given from the previous iteration , yielding a new soft - assignment of the pixels to the components ( clusters ) : In the M - step we then aim to find the configuration of that would maximize the expected log - likelihood using the posteriors computed in the E - step ."
AV,acquaintance vaccination,TR-45870,"tikzpicture customlegend[legend columns=4,legend style = at=(0.12,1.02),draw = none , column sep=3ex , line width=2pt , font= , legend entries = RV , AV , IMV , DV , direct , indirect ] solid , color = blue color = red color = green color = magenta color = black dashdotted , color = black customlegend tikzpicture pvan_a.pdfpvan_b.pdfpvan_c.pdfpvan_d.pdfAverage outbreak sizes at various vaccination rates ( percentage of total nodes ) of different strategies : ( A , B ) nodes are vaccinated with contacts created for direct interactions and ( C , D ) comparison of outbreak sizes for vaccinating nodes with contacts based on the direct interactions ( solid lines ) and contact based on any direct or indirect interactions ( dashed lines)-1.5emfig : avacfigurefigure[h ! ]"
CT,computed tomography,TR-45903,figure [ ht ] center minipage0.15 ./fig / snapshot0001new3.png minipage minipage0.15 ./fig / snapshot0011new3.png minipage minipage0.15 ./fig / snapshot0041new3.png minipage minipage0.15 ./fig / snapshot0091new3.png minipage minipage0.15 ./fig / snapshot0002new3.png minipage minipage0.15 ./fig / snapshot0012new3.png minipage minipage0.15 ./fig / snapshot0042new3.png minipage minipage0.15 ./fig / snapshot0092new3.png minipage minipage0.15 ./fig / snapshot0004new.png minipage minipage0.15 ./fig / snapshot0014new.png minipage minipage0.15 ./fig / snapshot0044new.png minipage minipage0.15 ./fig / snapshot0094new.png minipage minipage0.15 ./fig / snapshot0005new2.png minipage minipage0.15 ./fig / snapshot0015new2.png minipage minipage0.15 ./fig / snapshot0045new2.png minipage minipage0.15 ./fig / snapshot0095new2.png minipage minipage0.15 ./fig / snapshot0007new.png minipage minipage0.15 ./fig / snapshot0017new.png minipage minipage0.15 ./fig / snapshot0047new.png minipage minipage0.15 ./fig / snapshot0097new.png minipage minipage0.15 ./fig / snapshot0008new2.png minipage minipage0.15 ./fig / snapshot0018new2.png minipage minipage0.15 ./fig / snapshot0048new2.png minipage minipage0.15 ./fig / snapshot0098new2.png minipage center 5fig : vis1 Visualizations on four test CT images .
NN,neural network,TR-45912,"& LinearRegression & LogisticRegression & NN & CNN 4[3]*LAN ( ) & 2 * 1 & ABY3 & & & & & & This & & & & 2 - 7 & 2 * 100 & ABY3 & & & & & & This & & & & 4[3]*WAN ( ) & 2 * 1 & ABY3 & & & & & & This & & & & 2 - 7 & 2 * 100 & ABY3 & & & & & & This & & & & tabular Online Runtime of ABY3 ( Malicious ) and This for Secure Prediction of Linear , Logistic , NN , and CNN models for . ("
BN,bayesian network,TR-45922,"The main benefits of SV over first - order and total effect sensitivity measures include : ( 1 ) the uncertainty contributions sum up to total variance of output ; ( 2 ) SV can automatically account for probabilistic dependence and structural interactions occurring in the complex production process ; and ( 3 ) combing SV with BN ( represented by BN - SV ) can facilitate the appropriate and interpretable risk and sensitivity analysis since BN is built based on underlying physical mechanics causing the interdependence of raw material quality , production process , and bio - drug properties ."
MSE,model selection eqn,TR-45982,"Given an matrix , a norm , and a subset of indices , a sufficient condition that there exists a decoder such that the instance optimality in expectation in in eqn : instance : optimality holds with constant , is that the null space property in expectation eqn : null : space : property holds with for this : A necessary condition is the null space property in expectation eqn : null : space : property with : Similar results hold between the MSE instance optimality in eqn : instance : optimality : MSE and the null space property eqn : null : space : property : MSE , with the constant in the sufficient condition ."
CS,charging station,TR-45988,"In particular , we now make the following assumptions , that are common in most PEV - CS assignment problems : We use a Poisson process to model a new PEV requiring charging ( as in and);We assume that the time between the PEV charging request and the recommendation of the optimal CS is short enough so that the optimal solution is computed upon updated information ( this implies that the assignment optimization problem is solved in a very short time , e.g. , 1 s);We assume that the time required for charging is proportional to the required energy ( as in , and similar to and with constant charge step);Finally , we assume that once a vehicle accepts to get charged to the recommended CS , it will in fact drive towards the CS ."
AI,artificial intelligence,TR-46102,"Partnership on AI , an association of tech companies , academics , and civil society groupsANSWER CHOICES : A great deal of confidence ( 3)A fair amount of confidence ( 2)Not too much confidence ( 1)No confidence ( 0)I do n’t knowAdditional Tables and FiguresSize of demographic subgroupsResults from a saturated regression predicting perceived issue importance using demographic variables , AI governance challenge , and interactions between the two types of variables ; the coefficients for the interactions variables are not shown due to space constraintsTrust in various actors to develop AI : distribution of responsesTrust in various actors to manage AI : distribution of responsesSupport for developing AI across demographic characteristics : distribution of responses"
RL,reinforcement learning,TR-46149,Initialize weights of neural net arbitrary Initialize Precompute support grid On each interaction step : select observe transition construct -step transition and add it to experience replay with priority sample batch of size from experience replay using probabilities compute weights for the batch ( where is the size of experience replay memory ) for each transition from the batch compute target ( detached from computational graph to prevent backpropagation ) : project on support update transition priorities compute loss : make a step of gradient descent using if : Policy Gradient algorithmsPolicy Gradient theoremAlternative approach to solving RL task is direct optimization of objectiveas a function of .
CNN,convolutional neural network,TR-46173,"Convolutional neural networks ( CNN ) are one of the most famous set of neural network architectures used for classifying images , CNN takes advantage of local spatial coherence of the input ( Rippel , Snoek , Adams , 2015 ) because we assume that the spatially close images used for training are correlated , but in the case of the CMB dataset , the pixels in the images are random noise following a gaussian distribution , the CNN network will not be able to find any common features in the inputs and thus the training accuracy and test error will be less than favorable ."
CNN,convolutional neural network,TR-46229,"table[H ] tabularcccccccccc & 3c80 & 3c160 & 3c320 & SRP & CNNt15 & CNNf15 & SRP & CNNt15 & CNNf15 & SRP & CNNt15 & CNNf15 2 * 01 & & & & & & & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & & & & & & & 2*Average & & & & & & & & & & & & & & & & & & tabular Results for the SRP - PHAT strategy ( columns SRP ) and the CNN , either trained from scratch with sequence 15 ( columns CNNt15 ) or fine tuned with sequence 15 ( columns CNNf15 ) ."
MT,machine translation,TR-46272,compat=1.14filecontentsworkpyear.datX MT DS Morphology Other 2000 0 1 0 0 2001 1 0 1 12002 0 1 0 02003 0 1 0 12004 0 1 0 02005 1 1 0 02006 3 1 3 12007 1 0 1 02008 0 1 1 02009 1 0 0 12010 0 0 1 02011 0 0 1 12012 0 1 1 02013 1 0 0 12014 1 2 2 12015 1 1 0 02016 2 3 3 52017 2 4 9 5filecontentsfilecontentstotalworkpyear.datX all 2000 1 2001 1 2002 1 2003 1 2004 1 2005 2 2006 2 2007 2 2008 2 2009 1 2010 1 2011 2 2012 1 2013 3 2014 4 2015 2 2016 112017 16filecontents
CT,computed tomography,TR-46278,"to 0.99@X[1.8,l , p ] X[1,c , p ] X[1,c , p ] X[1,c , p ] X[1,c , p]@parameter & 2cNSCLC & 2cHNSCC & CT 1 & CT 2 & CT 1 & CT 2number & 31 & 31 & 19 & 19scanner & GE Healthcare Lightspeed 16 & GE Healthcare Lightspeed 16 & Siemens Biograph 16 & Siemens Biograph 16 & GE Healthcare VCT & GE Healthcare VCT & & tube voltage ( kVp ) & 120 & 120 & 120 & 120exposure ( mAs ) & 8 ( 4 - 10 ) & 8 ( 4 - 13 ) & 36 ( 18 - 62 ) & 9 ( 9 - 10)reconstruction kernel & Lung & Lung & B31f & B19fvoxel spacing ( -axis ; mm ) & 0.67 ( 0.51 - 0.90 ) & 0.67 ( 0.51 - 0.91 ) & 0.98 & 1.37voxel spacing ( -axis ; mm ) & 0.67 ( 0.51 - 0.90 ) & 0.67 ( 0.51 - 0.91 ) & 0.98 & 1.37voxel spacing ( -axis ; mm ) & 1.25 & 1.25 & 3.00 ( 2.00 - 3.00 ) & 2.00image noise (; HU ) & 29.3 ( 16.6 - 76.1 ) & 28.4 ( 16.9 - 70.3 ) & 4.1 ( 3.9 - 5.4 ) & 4.2 ( 3.7 - 6.8)Image acquisition parameters and characteristics for both NSCLC and HNSCC image data sets ."
RNN,recurrent neural network,TR-46504,"l X , & Input token , input vector , & Hidden state , hidden state vector , & Output token , output vector & Parameterized and differentiable functions & Loss function & Learning rate , & Arbitrary weight , arbitrary weight matrix & Number of time steps after which one truncated BPTT operation is performed & Number of time steps over which gradients are backpropagated in truncated BPTT & Nonlinear activation function & Sigmoid function : & RNN model & Output function of RNN model & Hidden state of RNN model & Element - wise vector multiplication operator & Sequence concatenation operator , , & Dataset , train set , test set & Ordered set of tokens appearing in a dataset ( ' vocabulary ' ) & Number of recurrent layers in an RNN model & Dimensionality of the recurrent layers in an RNN modelCharacter - level Recurrent Neural NetworksAs mentioned in the introduction , this paper mainly focuses on dynamic sequences of discrete tokens ."
MF,matrix factorization,TR-46526,"We ran all six models and we report the results of the following three : 1 ) MF , a matrix factorization algorithm that does not optimize for fairness ( we include this model as the simplest MF without fairness ) , 2 ) Fair MF ( non - parity ) which optimizes for the non - parity unfairness metric ( this model performed the best in terms of RMSE and MAE in the splits of the dataset that we used for our experiments ) , and 3 ) Fair MF ( Value ) which optimizes for the value unfairness metric ( this model performed the best in the splits of the dataset that Yao and HuangsiruiNIPS2017 operated on ) ."
CNN,convolutional neural network,TR-46622,"Proposed ModelWe construct an end - to - end trainable CNN architecture comprising of ( 1 ) a Siamese network , encompassing a pair of standard VGGNet CNNs truncated up and until the fourth convolutional layer ( conv ) with pretrained weights , that output a pair of tensors for each image , ( 2 ) a joint graph formulation computed as shown in equation ( ) , which is then reshaped into 2D grids of corresponding sizes and further appended with ( 3 ) a pair of ( Siamese ) convolutional layers , which are then concatenated , followed by a few dense layers to ultimately regress the homography matrix ."
ANN,artificial neural network,TR-46782,"tabularL2.0 cm L4.0 cm L5cmModel & Advantages & Disadvantages Artificial Neural Networks & & For Forecasting with ANNs & itemize Complex multi - dimensional , non - linear systems can be modelled Physical understanding of the system behaviour not required ANN variants facilitate the use of any type of input data Computer software is available for modelling itemize & itemize Requires a significant amount of data for training data that needs to be representative of true data range and its variability Determining the most appropriate model is largely trial and error and therefore can be time consuming Most networks can not provide confidence limits on the output Pre - processing is required to limit the number of data inputs and reduce model complexity All published research is relatively recent Outputs need to mapped to a physical representation itemizeParameter Estimation with ANNs & itemize As for RUL Forecasting with ANNs Useful for incorporating with physics of failure models Confidence limits available from underlying model ( for which parameters are being estimated ) itemize & itemize Less data required for estimating parameters as models tend to be failure specific Determining the most appropriate model is largely trial and error and therefore can be time consuming itemizePhysical models & & Physical Models & itemize Provide most accurate and precise estimates of all modelling options Confidence limits provided Outputs can be easily understood itemize & itemize Detailed and complete knowledge of system behaviour required The accuracy and robustness are subject to the experimental conditions under which models were developed itemize tabulartabletable[htbp ! ]"
CRF,conditional random field,TR-46840,a ) FCN-32 ; ( b ) FCN-16s ; ( c ) ResNet - DUC ; ( d ) E - Net ; ( e ) SegNet ; ( f ) U - Net ; ( g ) FCN-8s ; ( h ) CWGAN - GP ; ( i ) FC - DenseNet ; ( j ) DSFE - CRF ; ( k ) DSFE - GCN ; ( l ) DSFE - GraphSAGE ; ( m ) DSFE - GGNN ; ( n ) DSFE - GGCN ; ( o ) Ground truth ; ( p ) Optical image .
FEC,forward error correction,TR-46982,"Fn algorithm[!htb ] RuleBlock * block = new RuleBlock ( ) ; block addRule ( new MamdaniRule ( "" if ( SpatialComplexity is HIGH and PacketLossRate is HIGH and FrameType is I ) then RedundancyAmount is HIGH "" , engine ) ) block addRule ( new MamdaniRule ( "" if ( TemporalIntensity is HIGH and PacketLossRate is HIGH and FrameType is I or P ) then RedundancyAmount is HIGH "" , engine ) ) Packet loss x video characteristics rulesalgo : MINT : lossVideoRules algorithmMINT - FEC utilises the same core structure of uavFEC , so once all the fuzzy rules and sets are defined , they are employed in real - time in the fuzzy logic controller ."
DBN,directed belief net,TR-47025,"arrows , positioning , patterns multi - objective net - works C - MAPSS classification hyper - parameters pre - process pre - processing ada - boost G - mean Cost - sensitive ECS - DBN Evolutionary Algorithm A Cost - Sensitive Deep Belief Network for Imbalanced ClassificationChong Zhang , Kay Chen Tan , Fellow , IEEE , Haizhou Li , Fellow , IEEE , and Geok Soon Hong C. Zhang and H. Li are with the Department of Electrical and Computer Engineering , G. S. Hong is with the Department of Mechanical Engineering , National University of Singapore , 4 Engineering Drive 3 , 117583 , Singapore ( e - mail : zhangchong@u.nus.edu ; haizhou.li@nus.edu.sg ; mpehgs@nus.edu.sg)K. C. Tan is with the Department of Computer Science , City University of Hong Kong , 83 Tat Chee Avenue , Kowloon , Hong Kong.(e - mail : kaytan@cityu.edu.hk)This paper has been accepted by IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS in April 2018 ."
SO,smart object,TR-47054,"Let be a stream , initialized as empty each Let and be boolean variables , initialized as true Let and be boolean variables , initialized as false = true = true each Let = .AC Let = .RC Let be a boolean variable , initialized as true each Let be a boolean variable , initialized as false = true = each Let be a boolean variable , initialized as true = false = = jacDataFlagjacIpFlag = true = true = true = Return Let us suppose that a processor SO performs the equi - join and projection given in Example ."
DL,depth loss,TR-47064,stbl : ablation_sdn 2.5pt tabularlcccccc 1c2*Depth Estimation & 3c & 3c 2 - 7 1c & Easy & Moderate & Hard & Easy & Moderate & Hard & 73.9 / - & 54.0 / - & 46.9 / - & 74.9 / 61.9 & 56.8 / 45.3 & 49.0 / 39.0 + DL & 75.8 / - & 56.2 / - & 51.9 / - & 75.7 / 60.5 & 57.1 / 44.8 & 49.2 / 38.4 & 79.7 / - & 61.1 / - & 54.5 / - & 77.0 / 63.2 & 63.7 / 46.8 & 56.0 / 39.8 tabulartable*table*[hbt ! ]
PAP,process arrival pattern,TR-47135,"The future works cover the following topics : evaluation of the method for a wider range of interconnecting network speeds and larger number of nodes using a simulation tool e.g. , expansion of the method for other collective communication algorithms , e.g. all - gather , a framework for automatic PAP detection and proper algorithm selection , e.g. providing a regular ring for balanced PAPs and PRR for imbalanced ones , introduction of the presented PAT estimation method for other purposes e.g. asynchronous SDG training or deadlock and race detection in distributed programs , deployment of the solution in a production environment ."
FTE,foveal tilt effects,TR-47137,"the edge maps at fine to medium scales with the overlayed Hough lines are shown in [ fig : B5]Figs to [ fig : B7]. We show the mean tilt angles measured in the DoG edge maps across multiple scales in [ appendix : AppxC]Appendix C.figuresectionC. Quantitative mean tiltsThe absolute mean tilts and the standard errors of detected tilt angles for the Cafe Wall variations tested are provided here in Figs [ fig:]Figs and [ fig : C2]. For the ' foveal tilt effect ' ( FTE , explained in [ sec:3.1.1]Sections and [ sec:3.1.2]Section ) , we used the near horizontal mean tilts at scale 4 , and reflected these values to [ fig:6]Fig ."
NN,neural network,TR-47159,"We make the following assumptions : a dynamic RNN with neurons , , exists that will map from a compact input space to an output space on the Lebesgue integrable functions with closed interval or open - ended interval ; the nonlinear function is exactly with vectorized coefficients and a Lipschitz - continuous vector of basis functions ; inside a ball of known , finite radius , the ideal NN approximation , is realized to a sufficient degree of accuracy , ; the process noise is estimated alongside model parameters by the dynamic RNN ; outside , the NN approximation error can be upper - bounded by a known scalar function such that ; there exists an exponentially stable reference model with a Hurwitz matrix commanded by a reference signal ."
RS,relay station,TR-47263,Detection ( ) Figure out the three candidate relay stations by its location and destination 's location Send detection message to the three relay stations Algorithm for initiator(MU ) to send detection messageroutingfig Prediction ( ) Calculate its potential energy by formula ( 7 ) Send the energy quantity back to the initiator Algorithm of computation for potential energyroutingfig Selection ( ) Compare the three energy quantities from the three relay stations Select the RS with highest energy values Hands over to the RS Send payload to the RS Algorithm of hand overroutingfigPerformance EvaluationThe performance evaluation was conducted in a simulated noiselessradio network environment using MATLAB .
IP,inductive programming,TR-47277,"& Louis Johnson , PhD & PhD & red Lou & verde PhD & Robert Mills & & red Rob & verde 3rAccuracy : & 0.72 & 1 4 * 20 & 235 - 7654 Taiwan & ( 886 ) 235 - 7654 & & & 17 - 455 - 81 - 39 Spain & ( 34 ) 17 - 455 - 81 - 39 & red ( 886 ) 17 - 455 - 81 - 39 & verde ( 34 ) 17 - 455 - 81 - 39 & 618 - 4390 Panama & ( 507 ) 618 - 4390 & red ( 886 ) 618 - 4390 & verde ( 507 ) 618 - 4390 & 25 - 613 - 24 - 50 Chile & ( 56 ) 25 - 613 - 24 - 50 & red ( 886 ) 25 - 613 - 24 - 50 & verde ( 56 ) 25 - 613 - 24 - 50 3rAccuracy : & 0 & 1 4 * 23 & 23/11/18 425 - 785 - 4210 & 425 - 785 - 4210 & & & 425 - 613 - 2450 000 - 000 & 425 - 613 - 2450 & red 2450 000 - 000 & verde 425 - 613 - 2450 & [ TS]865 - 000 - 0000 - 06 - 23 - 09 & 865 - 000 - 0000 & red 06 - 23 - 2009 & verde 865 - 000 - 0000 & 17:58 - 19:29 , 425 - 743 - 1650 & 425 - 743 - 1650 & verde 425 - 743 - 1650 & verde 425 - 743 - 1650 3rAccuracy : & 0.36 & 1 4 * 25 & 08:55 PM CET & 08:55 & & & 20:15:00 & 20:15:00 & verde 20:15:00 & verde 20:15:00 & 10:05:00 AM & 10:05:00 & verde 10:05:00 & verde 10:05:00 & UTC 21:20 & 21:20 & red UTC 21:20 & verde 21:20 3rAccuracy : & 0.91 & 1 4 * 28 & 01:34:00 5 & 06:34:00 & & & 01:55 5 & 06:55 & verde 06:55 & verde 06:55 & 16:15:12 5 & 21:15:12 & red 06:15:12 & verde 21:15:12 & 21:20 5 & 02:20 & red 06:20 & verde 02:20 3rAccuracy : & 0.10 & 1 4 * 30 & 56.77cl & cl & & & 84Kg & Kg & verde Kg & verde Kg & 39.88 A & A & verde A & verde A & 1 nm & nm & verde nm & verde nm 3rAccuracy : & 1 & 1 4 * 31 & 56.77cl & Volume & & & 84Kg & Mass & red Volume & verde Mass & 39.88 A & Electricity & red Volume & verde Electricity & 1 nm & Length & red Volume & verde Length 3rAccuracy : & 0.10 & 1 tabular Example of results obtained ( using MagicHaskeller as IP core ) compared with FlashFill ."
LDE,learnable dictionary encoding,TR-47298,table*[htb]centertabularllccccc & Front - end model & Loss & Dims & Aggregation & Training set & EER ( ) VoxCeleb1 test set Nagrani Nagrani17 & I - vectors + PLDA & - & - & - & VoxCeleb1 & 8.8Nagrani Nagrani17 & VGG - M & Softmax & 1024 & TAP & VoxCeleb1 & 10.2Cai cai2018exploring & ResNet-34 & A - Softmax + PLDA & 128 & TAP & VoxCeleb1 & 4.46 Cai cai2018exploring & ResNet-34 & A - Softmax + PLDA & 128 & SAP & VoxCeleb1 & 4.40 Cai cai2018exploring & ResNet-34 & A - Softmax + PLDA & 128 & LDE & VoxCeleb1 & 4.48 Okabe okabe2018attentive & TDNN ( x - vector ) & Softmax & 1500 & TAP & VoxCeleb1 & 4.70 Okabe okabe2018attentive & TDNN ( x - vector ) & Softmax & 1500 & SAP & VoxCeleb1 & 4.19 Okabe okabe2018attentive & TDNN ( x - vector ) & Softmax & 1500 & ASP & VoxCeleb1 & 3.85 Hajibabaei hajibabaei2018unified & ResNet-20 & A - Softmax & 128 & TAP & VoxCeleb1 & 4.40 Hajibabaei hajibabaei2018unified & ResNet-20 & AM - Softmax & 128 & TAP & VoxCeleb1 & 4.30 Chung Chung18a & ResNet-34 & Softmax + Contrastive & 512 & TAP & VoxCeleb2 & 5.04 Chung Chung18a & ResNet-50 & Softmax + Contrastive & 512 & TAP & VoxCeleb2 & 4.19 Ours & Thin ResNet-34 & Softmax & 512 & TAP & VoxCeleb2 & 10.48 Ours & Thin ResNet-34 & Softmax & 512 & NetVLAD & VoxCeleb2 & 3.57 Ours & Thin ResNet-34 & AM - Softmax & 512 & NetVLAD & VoxCeleb2 & 3.32 Ours & Thin ResNet-34 & Softmax & 512 & GhostVLAD & VoxCeleb2 & 3.22 Ours & Thin ResNet-34 & AM - Softmax & 512 & GhostVLAD & VoxCeleb2 & 3.23 Ours ( cleaned ) & Thin ResNet-34 & Softmax & 512 & GhostVLAD & VoxCeleb2 & 3.24 VoxCeleb1-E Chung Chung18a & ResNet-50 & Softmax + Contrastive & 512 & TAP & VoxCeleb2 & 4.42 Ours & Thin ResNet-34 & Softmax & 512 & GhostVLAD & VoxCeleb2 & 3.24 Ours ( cleaned ) & Thin ResNet-34 & Softmax & 512 & GhostVLAD & VoxCeleb2 & 3.13 VoxCeleb1-H Chung Chung18a & ResNet-50 & Softmax + Contrastive & 512 & TAP & VoxCeleb2 & 7.33 Ours & Thin ResNet-34 & Softmax & 512 & GhostVLAD & VoxCeleb2 & 5.17 Ours ( cleaned ) & Thin ResNet-34 & Softmax & 512 & GhostVLAD & VoxCeleb2 & 5.06 tabularcenterResults for verification on the original VoxCeleb1test set Nagrani17 and the extended and hardtest sets ( VoxCeleb - E and VoxCeleb - H ) Chung18a .
OP,old persian,TR-47358,"PIr * nThere are relatively few Proto - Iranian sources of the cluster * n , but these are realized as sn across the board in West Iranian , to the exclusion of the possible Median proper name in Akkadian Pa - at - ni - e - sa- = Med * Paniiesa- * pani - aisa- ' looking for a wife ' , and possibly the OP form ( found in the Susa inscription of Darius ) krnuvaka- ' stonemason ' , if from * krt - nu - aka following , though is less certain regarding the presence of * -t- and other scholars make no mention of etymological * -t- ."
SA,signal analysis,TR-47494,"tab : litreviewtabularl l l l l l l lReference & Year & Knowledge- & Experience- & Data- & Model- & Hybrid & Other methods & & based & based & Driven & based & & luo2003model & 2003 & & & & & & schwabacher2005survey & 2005 & & & & & & jardine2006review & 2006 & & & & & & AI lee2006intelligent & 2006 & & & & & & goh2006review & 2006 & & & & & & kothamasu2009system & 2006 & & & & & & Reliability , Stochasticcoble2008prognostic & 2008 & & & & & & Stress and effects - based heng2009rotating & 2009 & & & & & & sikorska2011prognostic & 2011 & & & & & & Life Expectancy , ANN si2011remaining & 2011 & & & & & & ahmadzadeh2014remaining & 2014 & & & & & & tsui2015prognostics & 2014 & & & & & & SA , Stochastic , ANNtsui2015prognostics & 2015 & & & & & & tabulartabletable[htbp ! ]"
SP,strictly piecewise,TR-47506,table*[t]Accuracy on Target SP Stringsets after 100 Epochstab : resultsSP4.5pttabularcccccccccc2c2Training & 2Test & 3cLSTM & 3cs - RNN & 2RPNI & & & 10 & 30 & 100 & 10 & 30 & 100 & 6SP2 & 21k & 1 & 0.847 ( 0.06 ) & 0.935 ( 0.07 ) & 0.952 ( 0.07 ) & 0.910 ( 0.05 ) & 0.999 ( 0.00 ) & 0.999 ( 0.00 ) & 1.000 & & 2 & 0.873 ( 0.10 ) & 0.951 ( 0.08 ) & 0.947 ( 0.08 ) & 0.976 ( 0.01 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 3 - 10 & 210k & 1 & 0.734 ( 0.12 ) & 0.673 ( 0.04 ) & 0.720 ( 0.03 ) & 0.937 ( 0.13 ) & 0.960 ( 0.08 ) & 0.972 ( 0.06 ) & 1.000 & & 2 & 0.723 ( 0.12 ) & 0.656 ( 0.04 ) & 0.701 ( 0.03 ) & 0.934 ( 0.13 ) & 0.960 ( 0.08 ) & 0.972 ( 0.06 ) & 1.000 3 - 10 & 2100k & 1 & 0.680 ( 0.08 ) & 0.707 ( 0.10 ) & 0.732 ( 0.07 ) & 0.974 ( 0.07 ) & 0.974 ( 0.07 ) & 0.982 ( 0.04 ) & 1.000 & & 2 & 0.665 ( 0.09 ) & 0.697 ( 0.12 ) & 0.716 ( 0.08 ) & 0.977 ( 0.07 ) & 0.974 ( 0.08 ) & 0.986 ( 0.04 ) & 1.000 6SP4 & 21k & 1 & 0.883 ( 0.06 ) & 0.885 ( 0.08 ) & 0.775 ( 0.05 ) & 0.890 ( 0.05 ) & 0.969 ( 0.02 ) & 0.988 ( 0.01 ) & 1.000 & & 2 & 0.943 ( 0.04 ) & 0.840 ( 0.09 ) & 0.749 ( 0.06 ) & 0.885 ( 0.06 ) & 0.975 ( 0.01 ) & 0.985 ( 0.01 ) & 1.000 3 - 10 & 210k & 1 & 0.862 ( 0.13 ) & 0.880 ( 0.14 ) & 0.853 ( 0.08 ) & 0.696 ( 0.05 ) & 0.840 ( 0.15 ) & 0.903 ( 0.12 ) & 1.000 & & 2 & 0.862 ( 0.14 ) & 0.877 ( 0.15 ) & 0.843 ( 0.08 ) & 0.686 ( 0.05 ) & 0.841 ( 0.16 ) & 0.900 ( 0.12 ) & 1.000 3 - 10 & 2100k & 1 & 0.842 ( 0.13 ) & 0.791 ( 0.14 ) & 0.720 ( 0.09 ) & 0.884 ( 0.14 ) & 0.828 ( 0.17 ) & 0.895 ( 0.12 ) & 1.000 & & 2 & 0.831 ( 0.13 ) & 0.785 ( 0.13 ) & 0.716 ( 0.08 ) & 0.900 ( 0.15 ) & 0.827 ( 0.17 ) & 0.902 ( 0.13 ) & 1.000 6SP8 & 21k & 1 & 0.844 ( 0.04 ) & 0.863 ( 0.05 ) & 0.901 ( 0.01 ) & 0.871 ( 0.01 ) & 0.885 ( 0.02 ) & 0.878 ( 0.01 ) & 0.817 & & 2 & 0.699 ( 0.08 ) & 0.627 ( 0.05 ) & 0.692 ( 0.03 ) & 0.719 ( 0.02 ) & 0.663 ( 0.06 ) & 0.668 ( 0.03 ) & 0.587 3 - 10 & 210k & 1 & 0.827 ( 0.15 ) & 0.798 ( 0.11 ) & 0.804 ( 0.04 ) & 0.818 ( 0.12 ) & 0.856 ( 0.10 ) & 0.979 ( 0.02 ) & 0.873 & & 2 & 0.654 ( 0.11 ) & 0.672 ( 0.10 ) & 0.638 ( 0.05 ) & 0.566 ( 0.05 ) & 0.646 ( 0.05 ) & 0.811 ( 0.08 ) & 0.634 3 - 10 & 2100k & 1 & 0.880 ( 0.10 ) & 0.927 ( 0.08 ) & 0.904 ( 0.08 ) & 0.893 ( 0.14 ) & 0.978 ( 0.04 ) & 0.988 ( 0.01 ) & 1.000 & & 2 & 0.760 ( 0.12 ) & 0.802 ( 0.13 ) & 0.739 ( 0.09 ) & 0.825 ( 0.15 ) & 0.909 ( 0.11 ) & 0.907 ( 0.09 ) & 1.000 tabulartable *
CNN,convolutional neural network,TR-47626,"table[H ] tabularccccccc & 2c80 & 2c160 & 2c320 & CNNt15 & CNNf15 & CNNt15 & CNNf15 & CNNt15 & CNNf15 2 * 01 & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & 2*Average & & & & & & & & & & & & tabular Results for the CNN proposal , either trained from scratch with sequence 15 ( columns CNNt15 ) or fine tuned with sequence 15 ( columns CNNf15 ) ."
PS,projective simulation,TR-47807,"tb ] ccccc Figure & & & & SQUAB iterations & 0.05 & 0.01 & 0.01 & 1,000,000 & 0.01 & 0.01 & 0.01 & 1,000,000 & 0.05 & 0.0006 & 0.001 & 1,000,000 trials - & 0.05 & 0.01 & 0.01 & 1,000,000 trials - & 0.05 & 0.0005 & 0.001 & 4,000,000 & 0.05 & 0.0006 & 0.001 & 1,000,000 trials - & 0.05 & 0.01 & 0.01 & 1,000,000 trials - & 0.05 & 0.0006 & 0.001 & 1,000,000 Parameters of the PS agent ( see Appendix ) and SQUAB algorithm as used for the various tasks considered in Sec ."
MD,mean diffusivity,TR-47860,tabularp0.8cmp5.5cmp6 cm 3cAmyloid Load ( PiB Positivity ) Set 1 & PiB Angular L / R & PiB Cingulum Ant L / R & PiB Cingulum Post L / R & PiB Frontal Med Orb L / R & PiB Precuneus L / R & PiB Temporal Sup L / R & PiB Temporal Mid L / R & PiB SupraMarginal L Set 2 & FA Cerebral peduncle R & FA Cerebral peduncle L & MD Corticospinal tract R & MD Corticospinal tract L & Trail - Making Test Part A Score & MD Cerebral peduncle R & PET Cingulum Post R & tabular Group difference across Amyloid Load ( PiB Positivity)tabletable [ ! ]
PAT,process arrival time,TR-47950,"The future works cover the following topics : evaluation of the method for a wider range of interconnecting network speeds and larger number of nodes using a simulation tool e.g. , expansion of the method for other collective communication algorithms , e.g. all - gather , a framework for automatic PAP detection and proper algorithm selection , e.g. providing a regular ring for balanced PAPs and PRR for imbalanced ones , introduction of the presented PAT estimation method for other purposes e.g. asynchronous SDG training or deadlock and race detection in distributed programs , deployment of the solution in a production environment ."
PS,projective simulation,TR-48018,"tb ] ccccc Figure & & & & SQUAB iterations & 0.05 & 0.01 & 0.01 & 1,000,000 & 0.01 & 0.01 & 0.01 & 1,000,000 & 0.05 & 0.0006 & 0.001 & 1,000,000 trials - & 0.05 & 0.01 & 0.01 & 1,000,000 trials - & 0.05 & 0.0005 & 0.001 & 4,000,000 & 0.05 & 0.0006 & 0.001 & 1,000,000 trials - & 0.05 & 0.01 & 0.01 & 1,000,000 trials - & 0.05 & 0.0006 & 0.001 & 1,000,000 Parameters of the PS agent ( see Appendix ) and SQUAB algorithm as used for the various tasks considered in Sec ."
CI,constructive interference,TR-48194,"By exploiting the S - procedure in Lemma 1 , ( 56 ) can be expanded and converted into a LMI as shown below 0 , j , We note that by using the fact that , ( 57 ) can always be guaranteed by the following constraintwhose robust formulation is given by Futhermore , we define and , therefore , the constraint ( 60 ) can be written in terms of real valued numbers as Therefore , the robust optimisation problem based on CI isNote that problem is a convex problem and thus can be optimally solved using standard convex softwares like CVX ."
NN,neural network,TR-48332,"& LinearRegression & LogisticRegression & NN & CNN 2*Training ( ) & ABY3 & & & & & This & & & & 2*Prediction ( ) & ABY3 & & & & & This & & & & tabular Total Online Runtime ( in seconds ) of ABY3 ( Malicious ) and This for Training and Prediction of Linear , Logistic , NN , and CNN models for ( lower = better ) over a WAN setting.tab:Total_RuntimetableComparison with the ML framework of ABY3 MR18 in the semi - honest settingapp : Comp_ABY3SemiWe compare the performance of our protocol with the semi - honest version of ABY3 , giving them an advantage in terms of the threat model ."
FTE,foveal tilt effects,TR-48360,"the edge maps at fine to medium scales with the overlayed Hough lines are shown in [ fig : B5]Figs to [ fig : B7]. We show the mean tilt angles measured in the DoG edge maps across multiple scales in [ appendix : AppxC]Appendix C.figuresectionC. Quantitative mean tiltsThe absolute mean tilts and the standard errors of detected tilt angles for the Cafe Wall variations tested are provided here in Figs [ fig:]Figs and [ fig : C2]. For the ' foveal tilt effect ' ( FTE , explained in [ sec:3.1.1]Sections and [ sec:3.1.2]Section ) , we used the near horizontal mean tilts at scale 4 , and reflected these values to [ fig:6]Fig ."
ASA,adaptive segmentation algorithm,TR-48397,"[ ] [ Manual annotation ] [ ] [ EDF image ] [ ] [ ASA mask ] An example from our data set , where a ) is the manual annotation ( counted neurons have green dots ) , b ) is the EDF image , and c ) is the ASA mask for the EDF image shown in ( b ) [ ] [ Initial masks created using ASA followed by human verification ] [ ] [ Iterative human - in - the - loop verification of deep learning predicted masks ] Proposed method in two steps : a ) creating EDF images , and applying ASA , then human verification , and then b ) iterative process using accepted ASA masks / images for training , and ASA masks / images as an active set ."
CT,computed tomography,TR-48426,figure figure [ ht ] center minipage0.15 ./fig / snapshot0109new2.png minipage minipage0.15 ./fig / snapshot0129new2.png minipage minipage0.15 ./fig / snapshot0139new2.png minipage minipage0.15 ./fig / snapshot0149new2.png minipage minipage0.15 ./fig / snapshot0110new3.png minipage minipage0.15 ./fig / snapshot0130new3.png minipage minipage0.15 ./fig / snapshot0140new3.png minipage minipage0.15 ./fig / snapshot0150new3.png minipage minipage0.15 ./fig / snapshot0106new3.png minipage minipage0.15 ./fig / snapshot0126new3.png minipage minipage0.15 ./fig / snapshot0136new3.png minipage minipage0.15 ./fig / snapshot0146new3.png minipage minipage0.15 ./fig / snapshot0103new3.png minipage minipage0.15 ./fig / snapshot0123new3.png minipage minipage0.15 ./fig / snapshot0133new3.png minipage minipage0.15 ./fig / snapshot0143new3.png minipage center 5fig : vis2_2 Visualizations for five anatomies on the first four holdout CT images .
CWE,common weakness enumeration,TR-48507,"tabularccccc CWE - ID & API - GATEWAY & CUSTOMERS - SERVICE & VETS - SERVICE & VISITS - SERVICE CWE-16 & 31 & 4 & 2 & 2 CWE-524 & 48 & 17 & 6 & 11 CWE-79 & 0 & 3 & 0 & 1 CWE-425 & 0 & 0 & 20 & 0 CWE-200 & 14 & 6 & 0 & 0 CWE-22 & 0 & 1 & 0 & 0 CWE-933 & 1 & 0 & 0 & 0 TOTAL & 94 & 31 & 28 & 14 tabular tablefigure homogenious_petclininc Vulnerability scanning results of the Homogeneous PetClinic application fig : homogenious_petclinincfiguresubsec : riskanalysiseavalIn order to perform Security Risk analysis , we leveraged the Cloud Aware Vulnerability Assessment System ( CAVAS)torkura2018cavas ."
PN,pixel - wise normalization,TR-48626,"tabularc c c Part & Input Output Shape & Layer Information 9*Down - sampling & ( h , 3w , 3 ) ( , , 64 ) & CONV-(N64 , K4x4 , S2 , P1 ) , LeReLU & ( , , 64 ) ( , , 128 ) & CONV-(N128 , K4x4 , S2 , P1 ) , PN , LeReLU & ( , , 128 ) ( , , 256 ) & CONV-(N256 , K4x4 , S2 , P1 ) , PN , LeReLU & ( , , 256 ) ( , , 512 ) & CONV-(N512 , K4x4 , S2 , P1 ) , PN , LeReLU & ( , , 512 ) ( , , 512 ) & CONV-(N512 , K4x4 , S2 , P1 ) , PN , LeReLU & ( , , 512 ) ( , , 512 ) & CONV-(N512 , K4x4 , S2 , P1 ) , PN , LeReLU & ( , , 512 ) ( , , 512 ) & CONV-(N512 , K4x4 , S2 , P1 ) , PN , LeReLU & ( , , 512 ) ( , , 512 ) & CONV-(N512 , K4x4 , S2 , P1 ) , PN , LeReLU & ( , , 512 ) ( , , 512 ) & CONV-(N512 , K4x4 , S2 , P1 ) , ReLU 18*Up - sampling & 2 * ( , , 512 ) ( , , 512 ) & DECONV-(N512 , K4x4 , S2 , P1 ) , & & CONV-(N512 , K3x3 , S1 , P1 ) , PN , ReLU & 2 * ( , , 1024 ) ( , , 512 ) & DECONV-(N512 , K4x4 , S2 , P1 ) , & & CONV-(N512 , K3x3 , S1 , P1 ) , PN , ReLU & 2 * ( , , 1024 ) ( , , 512 ) & DECONV-(N512 , K4x4 , S2 , P1 ) , & & CONV-(N512 , K3x3 , S1 , P1 ) , PN , ReLU & 2 * ( , , 1024 ) ( , , 512 ) & DECONV-(N512 , K4x4 , S2 , P1 ) , & & CONV-(N512 , K3x3 , S1 , P1 ) , PN , ReLU & 2 * ( , , 1024 ) ( , , 512 ) & DECONV-(N512 , K4x4 , S2 , P1 ) , & & CONV-(N512 , K3x3 , S1 , P1 ) , PN , ReLU & 2 * ( , , 1024 ) ( , , 256 ) & DECONV-(N256 , K4x4 , S2 , P1 ) , & & CONV-(N256 , K3x3 , S1 , P1 ) , PN , ReLU & 2 * ( , , 512 ) ( , , 128 ) & DECONV-(N128 , K4x4 , S2 , P1 ) , & & CONV-(N128 , K3x3 , S1 , P1 ) , PN , ReLU & 2 * ( , , 256 ) ( , , 64 ) & DECONV-(N64 , K4x4 , S2 , P1 ) , & & CONV-(N64 , K3x3 , S1 , P1 ) , PN , ReLU & 2 * ( , , 64 ) ( h , 3w , 3 ) & DECONV-(N3 , K4x4 , S2 , P1 ) , & & CONV-(N512 , K3x3 , S1 , P1 ) , Tanh tabularGenerator network architecture ."
PP,privacy preferences,TR-48686,"& Consumer & Smart dimensions & SO 1 & SO 2 & SO & Object & & & & network Varying & & & & complexity & & & & of PP & & & & Varying & & & & query & & & & * complexity & & & & Varying & & & & number of & & & & sensing SOs & & & & tabular Experiments : - time overhead , * time and bandwidth overhead tab : expTabletable Experimental resultsIn executing our experiments , we considered three main characteristics that may impact the performance of the proposed solution ."
NC,network centre,TR-48789,"Below are lists of sets , parameters and variables defined in the MILP model : The sets of the MILP model and their description 1.5 Model parameters and their description 1.5 Model variables and their description 1.5 The model is defined as follows : Objective : minimize the power consumption of the network coding enabled IP over WDM network:*The power consumption outlined in the objective function is made of the following components : The power consumption of conventional router ports The power consumption of NC router ports The power consumption of optical switches The total power consumption contribution from multiplexers and demultiplexers The total power consumption contribution from transponders The power consumption of all the EDFAs in the network Subject to : ."
RS,rate - selective,TR-48880,"Therefore substituting Eq : MGF_gbest to Eq : MGF_g_end_PureRS , a closed - form expression for of pure RS for INID Nakagami- fading with integer 's and distinct 's is given byFor IID Nakagami- fading channels with integer , substituting Eq : MGF_gbest_EqualC to Eq : MGF_g_end_PureRS a closed - form expression for of pure RS is given byTo obtain of pure RS for INID Nakagami- fading with integer 's and distinct 's , we substitute Eq : MGF_end_Final to Eq : Inverse_Laplaca and after some algebraic manipulations yields the following closed - form expressionwhere , with being positive real , andfor and ."
CNN,convolutional neural network,TR-48986,table[H ] tabularccccccc & 2c80 & 2c160 & 2c320 & GMBF & CNNf15 & GMBF & CNNf15 & GMBF & CNNf15 2 * 01 & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & 2 * 03 & & & & & & & & & & - & & - 2*Average & & & & & & & & & & & & tabular Results for the stratgy invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with sequence 15 ( columns CNNf15 ) .
BPE,byte pair encoding,TR-49070,"* [ ht]Ablation Performance of DLGNet Models with Static Padding lcccccccc3*Model & 4cMovie & 4cUbuntu & 2cRelevance & 2cDiversity & 2cRelevance & 2cDiversity & BLEU & ROUGE & DIST-1/2 & NASL & BLEU & ROUGE & DIST-1/2 & NASL DLGNet-117 M & & & & & & & & Single - turn Joint with BPE & 0.0 & 0.0 & 0.0400/0.1502 & 0.072 & 0.0 & 0.0004 & 0.1946/0.4636 & 0.064 Single - turn Conditional with BPE & 0.0013 & 0.0296 & 0.0134/0.0482 & 3.582 & 0.0 & 0.0083 & 0.0723/0.1470 & 0.890 Multi - turn Joint with BPE & 0.1825 & 0.1321 & 0.0346/0.0838 & 0.610 & 0.0012 & 0.1172 & 0.1719/0.3482 & 0.2937 Multi - turn Conditional with BPE & 0.0096 & 0.0628 & 0.0088/0.0394 & 3.425 & 0.0048 & 0.0766 & 0.0500/0.1454 & 2.372 Multi - turn Joint with Basic Tokenizer & 0.0518 & 0.0630 & 0.0176/0.0540 & 1.101 & 0.0030 & 0.0384 & 0.0465/0.0949 & 0.566 Multi - turn Conditional with Basic Tokenizer & 0.0149 & 0.1628 & 0.0394/0.1770 & 1.472 & 0.0 & 0.0136 & 0.2211/0.4192 & 0.281 DLGNet-345 M & & & & & & & & Single - turn Joint with BPE & 0.0 & 0.0 & 0.0/0.0 & 0.072 & 0.0 & 0.0006 & 0.4741/0.9760 & 0.061 Single - turn Conditional with BPE & 0.0006 & 0.0212 & 0.0010/0.0419 & 3.582 & 0.0004 & 0.0158 & 0.0721/0.1671 & 3.437 Multi - turn Joint with BPE & 0.0449 & 0.1931 & 0.0460/0.1273 & 0.531 & 0.0 & 0.0121 & 0.3323/0.4406 & 0.227 Multi - turn Conditional with BPE & 0.0010 & 0.0125 & 0.0091/0.0422 & 3.918 & 0.0004 & 0.0158 & 0.0721/0.1671 & 4.108 Multi - turn Joint with Basic Tokenizer & 0.0376 & 0.1389 & 0.0232/0.0654 & 0.543 & 0.0042 & 0.0341 & 0.0568/0.1299 & 0.552 Multi - turn Conditional with Basic Tokenizer & 0.0057 & 0.0970 & 0.1568/0.3785 & 0.331 & 0.0015 & 0.0345 & 0.1555/0.3990 & 0.470 Ablation Studies on DLGNet Models with Random Informative PaddingIn this section , we carry out a more detailed analysis and discussion of different configurations of DLGNet models as well as their performance across datasets , using the evaluation results in Table ."
MPI,message passing interface,TR-49164,figure[!htb ] subfigure.49 figures / RMSD - ga4py - t_total.pdf format = hang Scaling total fig : MPIscaling - ga4py subfigure subfigure.49 figures / RMSD - ga4py - speed_up.pdf format = hang Speed - up fig : MPIspeedup - ga4py subfigure subfigure.49 figures / RMSD - ga4py - time_IO_comparison.pdf format = hang Scaling for different components fig : ScalingComputeIO - ga4py subfigure subfigure .5 figures / RMSD - ga4py - BarPlot - rank - comparison_72_1.pdf format = hang Time comparison on different parts of the calculations per MPI rank fig : MPIranks - ga4py subfigure Performance of the RMSD task using Global Arrays on SDSC Comet .
RV,random vaccination,TR-49196,"List of SymbolsList of abbreviationslongtablecc Acronym & Descriptions ADN & Activity driven network modelling AV & Acquaintance vaccination APV & Absolute percentage variation CIP & co - location interaction parameters CN & Common neighbours DST network & Dense SPST network DDT network & Dense SPDT network DDT1 & Vaccinating neighbours in DDT network with direct linksDDT2 & Vaccinating neighbours in DDT network with any linksGDT & Generated SPDT network with 364 K nodes GST & Generated SPST network with 364 K nodes IMV & Individual movement based vaccination strategy IMVE & Individual movement based vaccination strategy with exact information IMVT & Individual movement based vaccination strategy with temporal information LST & SPDT network with the same number of links that of DDT network LST & SPST network with the same number of links that of DST network MLE & Maximum likelihood estimator OSN & Online social network PFU & Plaque - forming unit RV & Random vaccination DV & Degree vaccination RSE & Root squared error SPST & Same place same time transmission SPDT & Same place different time transmission SIR & Susceptible - infected - recovered SDT network & Sparse SPDT network of links having direct and indirect components SST network & Sparse SPST network of links having indirect component onlySPDT graph & graph based on SPDT diffusion longtableList of symbolslongtablecc Symbol & Descriptions A & Set of active copies of nodes in SPDT graph b & active particle decay rates from an area of interaction & activity potential of node & active periods of a node C & Particle concentration in interaction area C , C , C & scenario 1 , 2 and 3 d & Activation degree - number of SPDT links created during an activation E & Intake dose or exposure of infectious particles & Average volume fraction of room air introduced by exhaled breath f & distribution function F & Disease spreading force in the network at the current day of simulation & Average disease spreading force in the network & Graph & Dynamic graph g & Particle generation rate by an infected individual h & Activation frequency I & Number of infected individuals in the system & Number of infected individuals at a simulation day & Number of infected individuals in the system at the current time that disease prevalence & Number of infected individuals up to a simulation day L & links set N & Total number of individuals , nodes , users p & Pulmonary rate of susceptible individual & Infection probability for an intake dose & Probability of creating a link during an activation & Probability of breaking a created link Q & air exchange rate from an area q & Transition probability for changing inactive to active state R & disease reproduction ability of an infected individual r & Particle removal rate from interaction area r & Median of particles removal rates S & Number of susceptible individual T & Simulation period or disease observation period & Activation period or period host user or node stays at the interacted location & Link creation delay or delay neighbour user or node arrives at the interacted location & Stay duration of user or node stays at the interacted location V & Air volume of interaction area , , & waiting periods of a node Y & Labelling sets in graph X & updates z & Number of time step Z & set of nodes in the SPDT graph & power law exponent & Infection rate at the current day of simulation in the network for an infected individual & Indirect transmission period & central tendency & Scaling parameter of activation degree distribution & neighbour proportion & average volume fraction of room air that is exhaled by an susceptible individual & links presence function & state probability & nodes presence function & switching probability form active to inactive states & Infectiousness of infection particles & Fraction of dose or exposure reaches to the target infection site & Duration that virus is generated or infectious period of infected individual & inter - event time for node in activity driven networks & lower limit of active degree distribution & activation potential in ADN networks & particle accumulation rate & Transition probability for changing active to inactive state longtable"
SOP,secrecy outage probability,TR-49305,"Next , by substituting ITS3:den_gammaBE into ITS3:cdf_gammaBE_1 , and after some simplifications , we obtainBy evaluating the integral in ITS3:cdf_gammaBE_2 , we obtainNext , the CDFs of can be derived as , or equivalentlyAdditionally , the PDF of can be derived aswhich , after some algebraic manipulations , can be rewritten asSince and are independent RV , the SOP can be obtained asBy substituting ITS3:FX and ITS3:fY into ITS3:SecOutProb_2 , and after some mathematical manipulations , we getwhere and can be respectively expressed as and By setting into Eq : I_1 and Eq : I_2 and after some basic algebraic manipulations and the use of , Eq : I_1 can be rewritten asI_2 & = e^K L_Bi Ei ( -(1+L_Bi ) K ) L_Ej - L_Bi - e^K L_Ej Ei(-(1+L_Ej ) K)L_Ej - L_Bi ."
SO,smart object,TR-49437,"table[H ] tabularccc Scenario & Simple & Complex & queries & queries Consumer SO 2 & 0.18 ms & 0.44 ms tabularVarying query complexity - Consumer SO tab : lev2tab3tabletable * tabularccccccccccc Queries & Q1 & Q2 & Q3 & Q4 & Q5 & Q6 & Q7 & Q8 & Q9 & Q10 Query selectivity & 3,5 & 1,84 & 0,85 & 0,72 & 0,55 & 0,37 & 0,28 & 0,2 & 0,15 & 0,13 Extra bits per output tuple & 98 & 140 & 210 & 245 & 294 & 336 & 392 & 455 & 483 & 546 Bandwidth overload per hour & 3440 & 2576 & 1785 & 1764 & 1617 & 1243 & 1097 & 910 & 724 & 709 tabularVarying query complexity - SO network - bandwidth overhead tab : bandwidthtable*Smart Object network : We simulate a smart object network via a Streambase query , where each single Streambase operator acts as a smart object ."
EO,eyes open,TR-49453,"& Task&Segment & Feature Extraction&Classifier&Performance 4*Biased & 2*Abdullah2010 & 2 * 10 & 2 * 5 & 2*[2]weeks & 2 * 4 & EC & 2*[5]s & 2*AR & 2*ANN&CRR = [ 97.0 ] & & & & & & EO & & & & CRR = [ 96.0]2 - 11 & Riera2008&51&4 & 34 [ 74]days & 2 & EC & [ 4]s & AR , PSD , MuI , COH , CC & FDA & EER = [ 3.4 ] 2 - 11 & Su2010&40&2 & - & 1 & EC & [ 180]s & AR , PSD & KNN , LDA & CRR = [ 97.5 ] 8*Rigorous&Marcel2007&9&3&[3]days & 8 & MI & [ 1]s & PSD & MAP model & HTER = [ 19.3 ] 2 - 11 & Lee2013&4&2 & [ 10]days - [ 5]months & 1 & EC & [ 50]s & PSD & LDA & AC = [ 100 ] 2 - 11 & 2*Rocca2013 & 2 * 9 & 2 * 2 & 2 * 1 - [ 3]weeks & 3 & 2*EC & 2*[1]s & 2*AR & 2*Linear classier & CRR = [ 100 ] & & & & & 5 & & & & & CRR = [ 100 ] 2 - 11 & 2*Armstrong2015 & 15 & 2 & 5 - [ 40]days & 2 * 1 & 2*ERP & 2*[1.1]s & 2*Time - series & 2*CC & CRR = [ 89.0 ] & & 9 & 3 & 134 - [ 188]days & & & & & & CRR = [ 93.0 ] 2 - 11 & 2*Maiorana2016a & 2 * 50 & 2 * 3 & 2*Ave . ["
SO,smart object,TR-49456,"& Consumer & Smart dimensions & SO 1 & SO 2 & SO & Object & & & & network Varying & & & & complexity & & & & of PP & & & & Varying & & & & query & & & & * complexity & & & & Varying & & & & number of & & & & sensing SOs & & & & tabular Experiments : - time overhead , * time and bandwidth overhead tab : expTabletable Experimental resultsIn executing our experiments , we considered three main characteristics that may impact the performance of the proposed solution ."
FEC,forward error correction,TR-49485,"tabularccccccc -1.5exPacket loss & -1.5exQoE & -1.5exWithout & & -1.5exVideo - aware FEC & & -1.5exViewFEC 1.0exrate & 1.0exMetric & 1.0exFEC & 1.5exVideo - aware FEC & 1.0exImprovement & 1.5exViewFEC & 1.0exImprovement 2*Packet loss 5 & VQM & 3.05 & 1.06 & 65.14 & 1.02 & 66.48 & SSIM & 0.76 & 0.91 & 19.74 & 0.92 & 21.05 2*Packet loss 10 & VQM & 4.01 & 1.11 & 72.36 & 1.12 & 72.09 & SSIM & 0.74 & 0.91 & 22.97 & 0.91 & 22.97 2*Packet loss 15 & VQM & 6.19 & 1.60 & 74.09 & 1.49 & 75.87 & SSIM & 0.50 & 0.90 & 80.00 & 0.89 & 78.00 2*Packet loss 20 & VQM & 8.68 & 1.77 & 79.60 & 1.81 & 79.14 & SSIM & 0.33 & 0.88 & 166.67 & 0.88 & 166.67 tabular tab : vfec : allpktloss center tableTaking into consideration the results of the experiments , it is possible to say that the proposed ViewFEC mechanism showed good performance ."
MI,mutual information,TR-49531,tab : topMItable*table*[h]tabularlccccSetting & Age & 1lEpoch & Gender & 1lEpoch small - GRU & 36.13 & 1 & 53.39 & 1 Attn - BiGRU & 47.03 & 3 & 61.64 & 2 MTL - common - attn & 47.85 & 6 & 62.50 & 4 MTL - spec - attn & 47.92 & 4 & 63.09 & 4 MTL - sprvsd - spec - attn & 48.23 & 5 & 62.99 & 3 MTL - MI - spec - attn & 47.90 & 3 & 63.15 & 5 BERT & 50.95 & 4 & 64.96 & 6 tabularEpoch information for best model performance on our DEV .
AFC,atomic function computation,TR-49597,"With each arc outgoing an AFC node , we associate the atomic function , which takesthe length- incoming vectors , , and produces the length- outgoing vector , i.e. : Similarly , with each arc outgoing a source node , the atomic function takes the length- incoming vectors , , as well as the generated symbols = , and produces the length- outgoing vector , i.e. : ( Note that we consider here the most general case in which a source node does not have to lie on the "" bottom - most "" level of the network , i.e. , it can also have some incoming edges . )"
AV,acquaintance vaccination,TR-49617,"tikzpicture customlegend[legend columns=4,legend style = at=(0.12,1.02),draw = none , column sep=3ex , line width=2pt , font= , legend entries = RV , AV , IMV , DV , direct , indirect ] solid , color = blue color = red color = green color = magenta color = black dashdotted , color = black customlegend tikzpicture pvan_a.pdfpvan_b.pdfpvan_c.pdfpvan_d.pdfAverage outbreak sizes at various vaccination rates ( percentage of total nodes ) of different strategies : ( A , B ) nodes are vaccinated with contacts created for direct interactions and ( C , D ) comparison of outbreak sizes for vaccinating nodes with contacts based on the direct interactions ( solid lines ) and contact based on any direct or indirect interactions ( dashed lines)-1.5emfig : avacfigurefigure[h ! ]"
ASA,adaptive segmentation algorithm,TR-49639,"In this section , we describe our iterative deep learning approach which can be described in five steps : 1 ) we train a deep learning model ( Unet ) on EDF images , and their corresponding ASA accepted masks that match manual annotation images ; 2 ) A prediction was made on EDF images of ASA rejected masks that do not match manual annotation , and we refer to this set of images as the "" active set "" ; 3 ) Another set that does not overlap with either the training nor the active set is the "" test set "" which contains EDF images of different sections of a unique mouse ( mouse i d 17 ) ; 4 ) The results of testing a trained deep learning model on the active set were verified by the user by comparing the predicted mask and manual annotation similarity ( as described in the previous paragraph ) ."
RV,random vaccination,TR-49652,"List of SymbolsList of abbreviationslongtablecc Acronym & Descriptions ADN & Activity driven network modelling AV & Acquaintance vaccination APV & Absolute percentage variation CIP & co - location interaction parameters CN & Common neighbours DST network & Dense SPST network DDT network & Dense SPDT network DDT1 & Vaccinating neighbours in DDT network with direct linksDDT2 & Vaccinating neighbours in DDT network with any linksGDT & Generated SPDT network with 364 K nodes GST & Generated SPST network with 364 K nodes IMV & Individual movement based vaccination strategy IMVE & Individual movement based vaccination strategy with exact information IMVT & Individual movement based vaccination strategy with temporal information LST & SPDT network with the same number of links that of DDT network LST & SPST network with the same number of links that of DST network MLE & Maximum likelihood estimator OSN & Online social network PFU & Plaque - forming unit RV & Random vaccination DV & Degree vaccination RSE & Root squared error SPST & Same place same time transmission SPDT & Same place different time transmission SIR & Susceptible - infected - recovered SDT network & Sparse SPDT network of links having direct and indirect components SST network & Sparse SPST network of links having indirect component onlySPDT graph & graph based on SPDT diffusion longtableList of symbolslongtablecc Symbol & Descriptions A & Set of active copies of nodes in SPDT graph b & active particle decay rates from an area of interaction & activity potential of node & active periods of a node C & Particle concentration in interaction area C , C , C & scenario 1 , 2 and 3 d & Activation degree - number of SPDT links created during an activation E & Intake dose or exposure of infectious particles & Average volume fraction of room air introduced by exhaled breath f & distribution function F & Disease spreading force in the network at the current day of simulation & Average disease spreading force in the network & Graph & Dynamic graph g & Particle generation rate by an infected individual h & Activation frequency I & Number of infected individuals in the system & Number of infected individuals at a simulation day & Number of infected individuals in the system at the current time that disease prevalence & Number of infected individuals up to a simulation day L & links set N & Total number of individuals , nodes , users p & Pulmonary rate of susceptible individual & Infection probability for an intake dose & Probability of creating a link during an activation & Probability of breaking a created link Q & air exchange rate from an area q & Transition probability for changing inactive to active state R & disease reproduction ability of an infected individual r & Particle removal rate from interaction area r & Median of particles removal rates S & Number of susceptible individual T & Simulation period or disease observation period & Activation period or period host user or node stays at the interacted location & Link creation delay or delay neighbour user or node arrives at the interacted location & Stay duration of user or node stays at the interacted location V & Air volume of interaction area , , & waiting periods of a node Y & Labelling sets in graph X & updates z & Number of time step Z & set of nodes in the SPDT graph & power law exponent & Infection rate at the current day of simulation in the network for an infected individual & Indirect transmission period & central tendency & Scaling parameter of activation degree distribution & neighbour proportion & average volume fraction of room air that is exhaled by an susceptible individual & links presence function & state probability & nodes presence function & switching probability form active to inactive states & Infectiousness of infection particles & Fraction of dose or exposure reaches to the target infection site & Duration that virus is generated or infectious period of infected individual & inter - event time for node in activity driven networks & lower limit of active degree distribution & activation potential in ADN networks & particle accumulation rate & Transition probability for changing active to inactive state longtable"
CF,collaborative filtering,TR-49654,"Similar to CF task , the CTR data likelihood is : Then the factorisation machine with logistic activation function is adopted to model the click probability over a specific ad impression : where is modelled by interactions among 3-side featuresDual - Task BridgeTo model the dependency between the two tasks , the weights of the user features and publisher features in CTR task are assumed to be generated from the counterparts in CF task ( as a prior):where is the assumed variance of the Gaussian generation process between each pair of feature weights of CF and CTR tasks and the weight generation is assumed to be independent across features ."
FEC,forward error correction,TR-49677,"The information is collected by the receiver and sent to the transmitter ; Loss Rate Prediction - Using the feedback statistics , the properties of the error probability are estimated on the server side ; Video Characteristics - This module fetches information from the video sequences that are being transmitted to identify video characteristics such as the frame type and size , as well as the motion vectors ; Ant Colony Optimization - The ACO is responsible for making a joint analysis of all the information gathered by the other modules , establishing the most suitable amount of redundancy to each FEC block ; FEC Blocks - The FEC blocks are built and a specific amount of redundancy designed by the ACO is assigned to each one ."
CT,computed tomography,TR-49734,figure [ ht ] center minipage0.15 ./fig / snapshot0001new3.png minipage minipage0.15 ./fig / snapshot0011new3.png minipage minipage0.15 ./fig / snapshot0041new3.png minipage minipage0.15 ./fig / snapshot0091new3.png minipage minipage0.15 ./fig / snapshot0002new3.png minipage minipage0.15 ./fig / snapshot0012new3.png minipage minipage0.15 ./fig / snapshot0042new3.png minipage minipage0.15 ./fig / snapshot0092new3.png minipage minipage0.15 ./fig / snapshot0004new.png minipage minipage0.15 ./fig / snapshot0014new.png minipage minipage0.15 ./fig / snapshot0044new.png minipage minipage0.15 ./fig / snapshot0094new.png minipage minipage0.15 ./fig / snapshot0005new2.png minipage minipage0.15 ./fig / snapshot0015new2.png minipage minipage0.15 ./fig / snapshot0045new2.png minipage minipage0.15 ./fig / snapshot0095new2.png minipage minipage0.15 ./fig / snapshot0007new.png minipage minipage0.15 ./fig / snapshot0017new.png minipage minipage0.15 ./fig / snapshot0047new.png minipage minipage0.15 ./fig / snapshot0097new.png minipage minipage0.15 ./fig / snapshot0008new2.png minipage minipage0.15 ./fig / snapshot0018new2.png minipage minipage0.15 ./fig / snapshot0048new2.png minipage minipage0.15 ./fig / snapshot0098new2.png minipage center 5fig : vis1 Visualizations on four test CT images .
MPI,message passing interface,TR-49791,figure[!htb ] subfigure.4 figures / main - RMSD - t_total - SuperMIC.pdf Scaling total fig : MPIscaling - SuperMIC subfigure subfigure.4 figures / main - RMSD - speed_up - SuperMIC.pdf Speed - up fig : MPIspeedup - SuperMIC subfigure subfigure.4 figures / main - RMSD - time_comp_IO_comparison - SuperMIC.pdf format = hang Scaling for different components fig : ScalingComputeIO - SuperMIC subfigure subfigure .5 figures / main - RMSD - BarPlot - rank - comparison_80_5-SuperMIC.pdf format = hang Time comparison on different parts of the calculations per MPI rank ( example ) fig : MPIranks - SuperMIC subfigure LSU SuperMIC : Performance of the RMSD task with MPI .
LSU,louisiana state university,TR-49799,"adjustboxmax width= tabularc c c c c c c c c Name & Nodes & Number of Nodes & CPUs & RAM & Network Topology & Scheduler and Resource Manager & parallelfile system SDSC Comet & Compute & 6400 & 2 Intel Xeon ( E5 - 2680v3 ) 12 cores / CPU , 2.5 GHz & 128 GB DDR4 DRAM & 56 Gbps IB & SLURM & Lustre PSC Bridges & RSM & 752 & 2 Intel Haswell ( E5 - 2695 v3 ) 14 cores / CPU , 2.3 GHz & 128 GB , DDR4 - 2133MHz & 12.37 Gbps OPA & SLURM & Lustre LSU SuperMIC & Standard & 360 & 2 Intel Ivy Bridge ( E5 - 2680 ) 10 cores / CPU , 2.8 GHz & 64 GB , DDR3 - 1866MHz & 56 Gbps IB & PBS & Lustre tabular adjustbox [ Configuration of HPC resources ] Configuration of the HPC resources that were benchmarked ."
PI,purchase intention,TR-49869,"L ) Magnitude of the correlation between trust and sales , ( M ) Magnitude of the correlation between trust and price , ( N ) Magnitude of the correlation between trust and history , ( O ) Magnitude of the correlation between PI and sales , ( P ) Magnitude of the correlation between price and PI , ( Q ) Magnitude of the correlation between history and PI , ( R ) Magnitude of the correlation between price and sales , ( S ) Magnitude of the correlation between history and sales , ( T ) Magnitude of the correlation between history and price ."
LDA,linear discriminant analysis,TR-49886,"R1IR = [ 90.8 ] & & & & & & EO & & & & R1IR = [ 85.6 ] 2cTask & 9lEC : resting state with eyes closed , EO : resting state with eyes open , MI : motor imagery , ERP : event related potential 2c2*Classifier & 9lANN : artificial neural networks , FDA : Fisher discriminant analysis , KNN : k - nearest neighbours , LDA : linear discriminant analysis & & 9lMAP : maximum a posteriori , CC : cross correlation , L1 ( Manhattan ) distance , L2 ( Euclidean ) distance , cosine distance tabular table*Previous protocols sec : previous_protocolTable table : comparison summarises the state - of - the - art of the existing EEG biometrics applications based on multiple data acquisition days ."
FEC,forward error correction,TR-49969,The parameters are defined as follows : FEC - based : accounts for mechanisms that employ FEC ; ARQ - based : mark mechanisms that use ARQ ; QoE - sensitive data : this parameter demonstrates mechanisms that identity and/or considerate the video content to define the EC policy ; Video - aware : check mark is given to mechanisms that use any video characteristics to define the amount of redundancy and/or retransmission ; High - quality video : it is marked if the mechanisms are using videos equal or higher than 720p ( HD ready ) ; Network status : this parameter defines if the mechanisms use the information about the network healthy to define the redundant data ; UEP - enabled : means that different amounts of redundancy are being added to distinct portions of the video .
SOA,service oriented architecture,TR-50020,"Still in this context , the authors refer another paper , where a comparison between HLA and SOA concluded that : HLA has good interoperability , synchronization and effective and uniform information exchange mechanism between the communicating components ( federates ) , but lacks several features of web services , such as : the integration of heterogeneous resources , web - wide accessibility across firewall boundaries;SOA benefits from loose coupling , component reuse and scalability but lacks a uniform data exchange format and time synchronization mechanisms;The combination of HLA and SOA can extend the capabilities of the two technologies and thus enable integrated simulated and real services ."

acronym_,expansion,id,text
ACI,adjacent channel interference,DEV-10,"Note that the total signal power and interference power received by VUE in RB can be computed as follows , align S_j , f , t & = _ i T_j X_i , f , t P_i , t H_i , j definitionS , I_j , f , t & = _ k T_j X_k , f , t P_k , t H_k , j & + Ff ' = 1 f ' f _ k 1^N X_k , f',t P_k , t H_k , j , definitionI alignNote that , the first term in definitionI is CCI from non - neighbouring VUEs , and the second term is ACI from all transmitting VUEs ."
MI,mutual information,DEV-18,"fig : attn_viz_incorrect centeringfigure*table*[]tabularclccccWord & Eng & MI & Attn - BiGRU & MTL - C & MTL - S < يسالونني > & they ask me & 1.00E-06 & 0.038055 & 0.119423 & 0.169779 < ياغالين > & you dear ( + plural ) & 9.40E-05 & 0.052512 & 0.078695 & 0.164816 < منيحاا > & beautiful ( + fem ) & 5.00E-06 & 0.031567 & 0.117903 & 0.164619 < مسساء > & evening & 4.00E-06 & 0.027612 & 0.142467 & 0.161141 < سيصادفك > & you 'll meet by chance & 0.00E+00 & 0.040002 & 0.103273 & 0.153956 < بونسوار > & good evening ( French ) & 3.00E-05 & 0.044578 & 0.10257 & 0.1479 < سيصيبك > & it 'll befall you & 2.00E-06 & 0.031588 & 0.097372 & 0.147765 < شلوونكم > & how are you ( + plural ) & 5.00E-06 & 0.0352 & 0.125699 & 0.146776 < امبيه > & I want it & 1.50E-05 & 0.041253 & 0.095131 & 0.14668 < ياعيون > & you , darling & 0.00E+00 & 0.032135 & 0.123004 & 0.142256 < ياصدفه > & what a surprise & 1.00E-06 & 0.025398 & 0.124698 & 0.142011 < انزيين > & we beautify & 1.00E-06 & 0.025684 & 0.096841 & 0.141639 < صبحهم > & good morning & 0.00E+00 & 0.03443 & 0.118689 & 0.139647 < ياسااتر > & God protect & 0.00E+00 & 0.036266 & 0.104663 & 0.138652 < سابتسم > & I 'll smile & 1.00E-06 & 0.029951 & 0.097428 & 0.138609 < تقييمكم > & what 's your evaluation & 2.00E-06 & 0.034339 & 0.091574 & 0.137949 < مسلخير > & good evening & 7.00E-06 & 0.036409 & 0.114726 & 0.137911 < االخيرر > & good ( morning / evening ) & 0.000141 & 0.058663 & 0.094799 & 0.137106 < ليلهه > & night & 9.00E-06 & 0.026944 & 0.104899 & 0.136817 < نبئ > & we want & 0.00E+00 & 0.036886 & 0.098703 & 0.136785 tabularTop 20 most highly weighted words based on average attention weights from our MTL - spec - attn ( MTL - S ) network for gender ."
PC,program counter,DEV-59,"As shown in Table , a general propagation rule for any operation ( O ) should be able to decide that what the tag on the program counter in the next machine state ( ) and the tag on the instruction ’s result ( R ) should be if the current tag on the program counter is PC , the tag on the current instruction is CI , the tags on its input operands ( if any ) are OP1 and OP2 , and the tag on the memory location ( in case of load / store ) is MR ."
DL,deep learning,DEV-189,"Multiple Classification based NID*[t ] ML based 4-class NID with SVM(Mean Std - Dev Percent ) * 6p1.5 cm p2.0 cm c c c c 0em 1lCategory & 1lNID Model Name & 1cAccuracy & 1cPrecision & 1cRecall & 1cF1-Score 0em 0em 3*NORMAL & NID - SVM & 76.30 0.01 & 75.91 0.01 & 98.69 0.02 & 85.81 0.01 & NID - PGM - SVM & 76.16 0.08 & 75.90 0.03 & 98.41 0.11 & 85.70 0.06 & NID - DA - SVM & 82.87 0.17 & 98.39 0.20 & 77.68 0.32 & 86.82 0.16 0em 0em 3*DOS & NID - SVM & 94.00 0.01 & 93.35 1.15 & 25.34 0.16 & 39.86 0.10 & NID - PGM - SVM & 93.84 0.10 & 86.98 2.63 & 25.33 0.49 & 39.23 0.80 & NID - DA - SVM & 99.48 0.05 & 94.09 0.58 & 99.70 0.06 & 96.81 0.29 0em 0em 3*PROBE & NID - SVM & 98.82 0.02 & 68.54 0.43 & 83.27 0.53 & 75.19 0.37 & NID - PGM - SVM & 98.80 0.02 & 67.39 0.31 & 85.89 0.38 & 75.53 0.30 & NID - DA - SVM & 99.32 0.04 & 88.70 0.88 & 78.13 1.85 & 83.07 1.00 0em 0em 3*R2L & NID - SVM & 83.43 0.01 & 97.91 0.64 & 4.83 0.02 & 9.20 0.04 & NID - PGM - SVM & 83.34 0.05 & 94.15 5.15 & 4.51 0.02 & 8.60 0.04 & NID - DA - SVM & 83.74 0.19 & 51.75 0.31 & 96.57 0.66 & 67.38 0.23 0em * [ t ] DL based 4-class NID with DNN(Mean Std - Dev Percent ) * 6p1.5 cm p2.0 cm c c c c 0em 1lCategory & 1lNID Model Name & 1cAccuracy & 1cPrecision & 1cRecall & 1cF1-Score 0em 0em 3*NORMAL & NID - DNN & 77.15 3.22 & 76.5 2.81 & 99.15 0.51 & 86.33 1.64 & NID - PGM - DNN & 83.17 1.40 & 81.58 1.31 & 99.26 0.37 & 89.55 0.76 & NID - DA - DNN & 87.96 0.12 & 86.95 0.28 & 98.15 0.61 & 92.21 0.11 0em 0em 3*DOS & NID - DNN & 93.81 2.94 & 89.70 6.58 & 23.55 9.17 & 27.72 7.37 & NID - PGM - DNN & 99.04 0.22 & 97.84 3.73 & 89.97 0.98 & 93.7 1.34 & NID - DA - DNN & 99.59 0.08 & 97.14 1.36 & 97.65 0.40 & 97.39 0.52 0em 0em 3*PROBE & NID - DNN & 98.92 0.31 & 74.54 1.93 & 81.49 8.26 & 76.79 3.29 & NID - PGM - DNN & 98.40 0.07 & 64.93 1.90 & 55.84 7.72 & 59.78 3.86 & NID - DA - DNN & 99.05 0.07 & 75.27 1.24 & 83.47 4.52 & 79.11 2.17 0em 0em 3*R2L & NID - DNN & 83.90 2.72 & 58.99 3.87 & 7.55 5.81 & 11.28 3.13 & NID - PGM - DNN & 84.90 1.26 & 88.45 5.67 & 13.92 7.48 & 23.60 2.57 & NID - DA - DNN & 89.09 0.10 & 92.17 4.86 & 40.97 1.82 & 56.64 0.92 0em In order to verify the performance of the DA module on enhancing the existing learning based IDSs , we have undertaken multi - class based NID experiments ."
AP,average precision,DEV-365,tab : state tabularlccccc Model & AP & AP1 & AP0.5 & AP0.25 & AP0.125 MLP baseline & 3.60.5 & 1.50.4 & 0.80.3 & 0.20.1 & 0.00.0 RNN baseline & 4.01.9 & 1.81.2 & 0.90.5 & 0.20.1 & 0.00.0 Ours ( 10 iters ) & 72.82.3 & 59.22.8 & 39.04.4 & 12.42.5 & 1.30.4 Ours ( 20 iters ) & 84.04.5 & 80.04.9 & 57.012.1 & 16.69.0 & 1.60.9 Ours ( 30 iters ) & 85.24.8 & 81.15.2 & 47.417.6 & 10.89.0 & 0.60.7 tabulartableResultsWe show our results in tab : state and give sample outputs in app : outputs .
SVM,support vector machine,DEV-464,"table[htb]Disambiguation accuracytabularL3.7cmR1.6cmR1.6cmR1.6cmR1.6cmCategory & Number of instances & With text features & Without text features & Most frequent ACL - ARC & 2,996 & 92.9573 & 93.7583 & 93.4246 Bessel - TypeFunctions & 1,352 & 92.8254 & 92.3077 & 86.0947 Constants & 714 & 91.1765 & 90.3361 & 83.7535 ElementaryFunctions & 6,073 & 96.1963 & 96.3774 & 89.6427 GammaBetaErf & 3,816 & 95.2830 & 94.4706 & 78.0136 HypergeometricFunctions & 72,006 & 97.5571 & 97.0697 & 88.0746 IntegerFunctions & 11,955 & 95.8009 & 95.1652 & 90.0711 Polynomials & 5,905 & 98.2388 & 95.3091 & 87.3328 All WFS Data & 320,726 & 98.9243 & 98.4398 & 92.7025 tabulartab : disrestableThe results in Table tab : disres show that disambiguation result using SVM outperformed the ' most frequent ' method ."
MF,model fair,DEV-497,"We ran all six models and we report the results of the following three : 1 ) MF , a matrix factorization algorithm that does not optimize for fairness ( we include this model as the simplest MF without fairness ) , 2 ) Fair MF ( non - parity ) which optimizes for the non - parity unfairness metric ( this model performed the best in terms of RMSE and MAE in the splits of the dataset that we used for our experiments ) , and 3 ) Fair MF ( Value ) which optimizes for the value unfairness metric ( this model performed the best in the splits of the dataset that Yao and HuangsiruiNIPS2017 operated on ) ."
SF,structure fusion,DEV-610,"For evaluating structure fusion generalization , we compare structure fusion based graph convolutional networks ( SF - GCN ) , propagation fusion based graph convolutional networks ( PF - GCN ) and structure propagation fusion based graph convolutional networks ( SPF - GCN).In Table , we observe that the performance of SPF - GCN is better than that of other method , and the least improvement of SPF - GCN respectively is for Cora , for Citeseer and for PubMed , while the performance of SP is superior to that of PF - GCN , and the improvement of SF - GCN respectively is for Cora , for Citeseer and for PubMed Therefore , PF and SF both are benefit for further mining the structure information and the role of SF is more important than that of PF ."
LDA,linear discriminant analysis,DEV-716,"Dist & 233 & 37 & 37 & 3743 & 1.0 & 13.7&7.3 & 98.2 LDA & 200 & 70 & 87 & 3693 & 2.3&25.9&14.1&96.1 SVM & 241 & 29 & 11 & 3769 & 0.3&10.7 & 5.5 & 99.0 tabulartableValidation including non - registered imposters sec : results4Table table : Result_which_dataset summarises the confusion matrices of both Setup - R and Setup - B with segment sizes [ ] s , classified by the minimum cosine distance , LDA , and SVM ; these correspond to Table table : Result_segment_size_SVM , panels Setup - R and Setup - B. The confusion matrices were categorised into : itemize Client matrix from dataset , Imposter matrix from dataset Imposter matrix from dataset ."
OCR,optical character recognition,DEV-842,"tabularlccc Architectures & & & One - Stream ( Plate ) & 93.45 & 87.30 & 90.21One - Stream ( CNN - OCR ) & 100.0 & 88.80 & 94.10One - Stream ( Shape ) & 92.57 & 96.59 & 94.45Two - Stream ( Shape + Plate ) & 93.79 & 96.38 & 95.00Two - Stream ( Shape + CNN - OCR ) & 99.58 & 99.16 & 99.37Two - Stream - Temporal , , ( Shape + CNN - OCR ) & 99.81 & 99.02 & 99.41Two - Stream - Temporal , , ( Shape + CNN - OCR ) & 99.77 & 98.89 & 99.33Three - Stream ( Shape + Plate + CNN - OCR ) & 99.80 & 99.00 & 99.40 tabular tab : architecturestableWe performed statistical paired t - tests using the -score of the five runs of all paired combinations of the architectures / algorithms described in Table tab : architectures ."
FEC,forward error correction,DEV-866,"The information is collected by the receiver and sent to the transmitter ; Loss Rate Prediction - Using the feedback statistics , the properties of the error probability are estimated on the server side ; Video Characteristics - This module fetches information from the video sequences that are being transmitted to identify video characteristics such as the frame type and size , as well as the motion vectors ; Ant Colony Optimization - The ACO is responsible for making a joint analysis of all the information gathered by the other modules , establishing the most suitable amount of redundancy to each FEC block ; FEC Blocks - The FEC blocks are built and a specific amount of redundancy designed by the ACO is assigned to each one ."
GCN,graph convolution networks,DEV-957,a ) FCN-32 ; ( b ) FCN-16s ; ( c ) ResNet - DUC ; ( d ) E - Net ; ( e ) SegNet ; ( f ) U - Net ; ( g ) FCN-8s ; ( h ) CWGAN - GP ; ( i ) FC - DenseNet ; ( j ) DSFE - CRF ; ( k ) DSFE - GCN ; ( l ) DSFE - GraphSAGE ; ( m ) DSFE - GGNN ; ( n ) DSFE - GGCN ; ( o ) Ground truth ; ( p ) Optical image .
SFC,service function chaining,DEV-959,"Computer Networks INPUT:[]OUTPUT:[]SE[DOWHILE]DodoWhile[1 ] # 1 [ algorithmbis [ Algorithm # 1 ] +0em[]thm1Remarkop - tical net - works semi - conduc - tor con - sum - ption ini - tia - lized par - ti - cu - lar va - ria - ble va - ria - bles Joint Failure Recovery , Fault Prevention , and Energy - efficient Resource Management for Real - time SFC in Fog - supported SDN[label1]Mohammad M. Tajiki[label1]Department of Electronic Engineering , University of Rome – Tor Vergata , Via del Politecnico 1 , 00133 , Rome , Italymhmtjk01@uniroma2.it[label2]Mohammad Shojafar[label2]Department of Mathematics , University of Padua , Via Trieste 63 , 35131 , Padua , Italymohammad.shojafar@math.unipd.it[label4]Behzad Akbari[label4]Deparment of Electrical and Computer Engineering , Tarbiat Modares University , Tehran , Iranb.akbari@modares.ac.ir[label1]Stefano Salsanostefano.salsano@uniroma2.it[label2]Mauro Conticonti@math.unipd.it[label5]Mukesh Singhal[label5]Department of Electrical and Computer Engineering , University of California- Merced , Merced , CA 95343 , USAmsinghal@ucmerced.eduMiddleboxes have become a vital part of modern networks by providing services such as load balancing , optimization of network traffic , and content filtering ."
MSE,mean squared error,DEV-1077,"We highlight the following a few observations : 1 ) NMSRVI ( 1 ) achieves the best performances , and outperforms ANTs in terms of both MSE and mean CC on both tasks , likely due to the representation and optimization efficiency of deep neural nets ; 2 ) NMSRV yields consistently better results than VoxelMorph , demonstrating the efficacy of self - supervised optimization during the test phase for improving velocity field estimation and reducing the estimation gap between training and testing ; 3 ) NMSR ( 1 ) achieves better performance than NMSR on all experiments , demonstrating the benefit of sequential multi - scale optimization in echocardiogram registration ."
LP,label powerset,DEV-1116,"fig_gramar4figurefigure[!htbp ] Verbatim[frame = single , samepage = true ] < ALGS - PT > : : = < ALGS - PT1 > < ALGS - PT2 > < ALGS - PT3 > < ALGS - PT4 > < ALGS - PT1 > : : = BR CC LP # BR='Binary Relevance ' , CC='Classifier Chain ' # LC='Label Powerset'<ALGS - PT2 > : : = ( BRq CCq ) < dsr > < ComplexCC_Trellis > # BRq and CCq = ' quick versions for BR and CC ' FW RT < LP_based > # FW='Four - class pairWise ' , RT='Ranking - Threshold'<ALGS - PT3 > : : = BCC < dp_complete > # BCC='Bayesian Classifier Chain'<ALGS - PT4 > : : = PMCC < B > < ts > < ii > < chi_PMCC > < ps > < pof > # PMCC='Population of Monte - Carlo Classifier Chains'<dsr > : : = RANDOM - REAL(0.2 , 0.8 ) # dsr='down - sample ratio'<ComplexCC_Trellis > : : = PCC ( MCC < chi_MCC > < CT > ) < ii > < pof > ( CDN < CDT > ) < i_cdn_cdt > < ci > # PCC='Probabilistic Classifier Chains ' # MCC='Monte - Carlo Classifier Chains ' # CT='Classication Trellis ' # CDN='Conditional Dependency Networks ' # CDT='Conditional Dependency Trellis ' < chi_MCC>::= < chi_CT > 0 # chi_MCC='nmber of chain iterations for MCC'<ii > : : = RANDOM - INT(2 , 100 ) # ii='number of inference interations'<pof > : : = Accuracy Jaccard index Hamming score Exact match Jaccard distance Rank loss Hamming loss Zero One loss Harmonic score Log Loss lim : L Micro Recall One error Log Loss lim : D Micro Precision Macro Precision Macro Recall F1 micro averaged Avg precision F1 macro averaged by example F1 macro averaged by label AUPRC macro averaged AUROC macro averaged Levenshtein distance # pof='Payoff function'<CT > : : = < chi_CT > < w > < dp><dp > : : = C I Ib Ibf H Hbf X F None # dp='dependency type'<chi_CT > : : = RANDOM - INT(2 , 1500 ) # chi_CT='number of chain iterations for CT'<w > : : = 0 1 -1 < d > # w='width of the trellis'<d > : : = RANDOM - INT(1 , SQRT(L ) +1 ) # d='neighborhood density ' # Where L is the number of labels < CDT > : : = < w > < dp > # parameters defined earlier < i_cdn_cdt > : : = RANDOM - INT(101 , 1000 ) # i_cdn_cdt='total number of iterations'<ci > : : = RANDOM - INT(1 , 100 ) # ci='collection iterations'<LP_based > : : = ( PS PSt < RAkEL - based > ) < sv > < pv > # PS='Pruned Sets ' # PSt='Pruned Sets with Threshold'<sv > : : = RANDOM - INT(0 , 5 ) # sv='subsampling value ' < pv > : : = RANDOM - INT(1 , 5 ) # pv='pruning value ' < RAkEL - based > : : = ( RAkEL < sre > RAkELd ) < les > # RAkEL='RAndom k - labEL Pruned Sets ' # RAkELd='RAndom k - labEL Disjoint Pruned Sets'<sre > : : = RANDOM - INT(2 , min(2L , 100 ) ) # sre='number of subsets to run in an ensemble'<les > : : = RANDOM - INT(1 , L/2 ) # les='number of labels in each label subset ' # Where L is the number of labels < dp_complete > : : = < dp > LEAD # dp='complete dependency type for BCC ' < B > : : = RANDOM - REAL(0.01 , 0.99 ) # B='Beta factor for deacreasing the temperature'<ts > : : = 0 1 # ts='Temperature switch'<ps > : : = RANDOM - INT(1 , 50 ) # ps='population size'<chi_PMCC > : : = RANDOM - INT(51 , 1500 ) # chi_PMCC='number of chain iterations for PMCC'Verbatim 1 mm Defined Grammar - Part 5 : MLC Problem Transformation Methods ."
RG,real graphs,DEV-1292,"figure-1.7 em 6a.pdf 6b.pdf 6c.pdf 6d.pdf sdisp.pdf sdisn.pdf -1.0 em Diffusion dynamics on the sparse graph : ( a ) disease prevalences for real graph , SPDT model , BADN graph and SPST graph , ( b ) final epidemic size , ( c ) prediction error for : * lines for cumulative and other for daily ( d ) number of Momo users ( K ) and link densities per user , ( e ) for various in RG and SG , and ( f ) new infections for various fig : sdif -1.6 emfigurefigure*-1.5 em large_com.pdf large_cum.pdf lare_er.pdf ldisp.pdf ldisn.pdf -1.0 em Diffusion dynamics on the dense graphs : ( a ) disease prevalences for real graph , SPDT model , BADN graph and SPST graph , ( b ) final epidemic sizes ( c ) prediction errors , ( d ) for various r in RG and SG , and ( e ) new infections for various r fig : sdifl -1.3 emfigure*Model ValidationIn this section , we validate the proposed model simulating SPDT process on the generated synthetic graph and real contact graph ."
ABC,artificial bee colony,DEV-1356,"In this work , the main contributions are : ( i ) the objective function in has been reformulated using additional penalty term for optimal performance , ( ii ) two other evolutionary techniques ( DE and GSA ) have been used in the second phase to efficiently deploy the RNs , ( iii ) the effectiveness of the proposed algorithms is compared and contrasted with on the basis of network lifetime enhancement and speed of convergence , and lastly ( iv ) comprehensive experiments have been carried out to show the efficacy ( faster convergence and better optimal solution ) of using DE as opposed to ABC presented in to deploy backbone devices in WSNs ."
CNN,convolutional neural network,DEV-1484,table[H ] tabularcccccccccc & 3c80 & 3c160 & 3c320 & SRP & GMBF & CNNf15 + 11+static & SRP & GMBF & CNNf15 + 11+static & SRP & GMBF & CNNf15 + 11+statu 2 * 01 & & & & & & & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & & & & & & & 2*Average & & & & & & & & & & & & & & & & & & tabular Results for the standard SRP - PHAT strategy ( columns SRP ) ; the one invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with the sequences described in Table tab : fine - tuning - material ( columns CNNf15 + 11+static ) tab : baselineResults+ft15 + 1 + 2 + 3table
MD,molecular dynamics,DEV-1616,XXXXXXX HMC Properties & Acceptance Rate ( ) & IACF equilibration & IACF production & Acceptance Rate ( ) & IACF equilibration & IACF production ( ps ) & 3c25 fs 1000 = 25 ps & 3c25 fs 2000 = 50 ps VV & 42.92 & 105.50 & 102.76 & 39.34 & 226.57 & FAILED BCSS & 17.64 & 259.38 & FAILED & 6.01 & 373.43 & FAILED AIA & 41.30 & 107.04 & 105.94 & 41.10 & 101.02 & 141.67 Acceptance rate and IACF for equilibration and production with HMC for the biggest simulated time step and two choices of lengths of MD trajectories .
SO,smart object,DEV-1643,"table[H ] tabularccc Scenario & Simple & Complex & queries & queries Consumer SO 2 & 0.18 ms & 0.44 ms tabularVarying query complexity - Consumer SO tab : lev2tab3tabletable * tabularccccccccccc Queries & Q1 & Q2 & Q3 & Q4 & Q5 & Q6 & Q7 & Q8 & Q9 & Q10 Query selectivity & 3,5 & 1,84 & 0,85 & 0,72 & 0,55 & 0,37 & 0,28 & 0,2 & 0,15 & 0,13 Extra bits per output tuple & 98 & 140 & 210 & 245 & 294 & 336 & 392 & 455 & 483 & 546 Bandwidth overload per hour & 3440 & 2576 & 1785 & 1764 & 1617 & 1243 & 1097 & 910 & 724 & 709 tabularVarying query complexity - SO network - bandwidth overhead tab : bandwidthtable*Smart Object network : We simulate a smart object network via a Streambase query , where each single Streambase operator acts as a smart object ."
CT,computed tomography,DEV-1902,figure [ ht ] center minipage0.15 ./fig / snapshot0101new3.png minipage minipage0.15 ./fig / snapshot0121new3.png minipage minipage0.15 ./fig / snapshot0131new3.png minipage minipage0.15 ./fig / snapshot0141new3.png minipage minipage0.15 ./fig / snapshot0102new3.png minipage minipage0.15 ./fig / snapshot0122new3.png minipage minipage0.15 ./fig / snapshot0132new3.png minipage minipage0.15 ./fig / snapshot0142new3.png minipage minipage0.15 ./fig / snapshot0104new.png minipage minipage0.15 ./fig / snapshot0124new.png minipage minipage0.15 ./fig / snapshot0134new.png minipage minipage0.15 ./fig / snapshot0144new.png minipage minipage0.15 ./fig / snapshot0105new2.png minipage minipage0.15 ./fig / snapshot0125new2.png minipage minipage0.15 ./fig / snapshot0135new2.png minipage minipage0.15 ./fig / snapshot0145new2.png minipage minipage0.15 ./fig / snapshot0107new.png minipage minipage0.15 ./fig / snapshot0127new.png minipage minipage0.15 ./fig / snapshot0137new.png minipage minipage0.15 ./fig / snapshot0147new.png minipage minipage0.15 ./fig / snapshot0108new2.png minipage minipage0.15 ./fig / snapshot0128new2.png minipage minipage0.15 ./fig / snapshot0138new2.png minipage minipage0.15 ./fig / snapshot0148new2.png minipage center 5fig : vis2 Visualizations for the first four anatomy on the first four holdout CT images .
RD,reciprocal degree,DEV-1920,"tableSpearman Correlations to the KORE gold standard with the text - based approach for different Wikipedia dumpstab : bag - of - word - results tabularp5cmlVersion & Correlation Wikipedia at the time when YAGO2 was created & 0.503212 Wikipedia at the time of DBpedia 2009 dump & 0.491440 Wikipedia at the time of DBpedia 2010 dump & 0.502987 tabulartabletable*tableSpearman Correlations to the KORE gold standard comparing text - based approach with graph - based approaches between models without redirects and models with redirects of data from DBpedia 2009 and DBpedia 2010tab : corr-2009 - 2010-redirectsthreeparttable tabularp40mmllll2 * & 2cDbpedia 2009 & 2cDbpedia 2010 2 - 5 & 1cWithout Redirect & 1cWith Redirects & 1cWithout Redirect & 1cWith Redirects TF - IDF & 0.491440 & - & 0.502987 & - Jaccard ( I ) & 0.568509 & 0.575758 & 0.568026 & 0.588351 Jaccard ( O ) & 0.511564 & 0.504138 & 0.559898 & 0.564088 Jaccard ( I+O ) & 0.578706 & 0.580944 & 0.585535 & 0.586593 Extended Jaccard RP ( I ) & 0.585212 & 0.591538 & 0.569857 & 0.616843 Extended Jaccard RP ( O ) & 0.578478 & 0.566556 & 0.576616 & 0.601197 Extended Jaccard RP ( I+O ) & 0.604112 & 0.613450 & 0.599284 & 0.637930 Extended Jaccard RD ( I ) & 0.623768 & 0.634961 & 0.632760 & 0.672512 Extended Jaccard RD ( O ) & 0.588592 & 0.567652 & 0.601755 & 0.609715 Extended Jaccard RD ( I+O ) & 0.639055 & 0.651450 & 0.658647 & 0.696506 tabulartablenotes[para , flushleft ] , , , and mean the Wikipedia article link network with redirects using the Extended Jaccard with Reciprocal Degree Centrality considering both in - links and out - links is significantly better than this result with p - value 0.1 , p - value 0.05 , p - value 0.01 and p - value 0.001 respectively ."
SL,strictly local,DEV-2006,table*[t]Accuracy on Target SL Stringsets after 100 Epochstab : resultsSL4.5pttabularcccccccccc2c2Training & 2Test & 3cLSTM & 3cs - RNN & 2RPNI & & & 10 & 30 & 100 & 10 & 30 & 100 & 6SL2 & 21k & 1 & 0.772 ( 0.09 ) & 0.717 ( 0.08 ) & 0.711 ( 0.02 ) & 0.766 ( 0.11 ) & 0.761 ( 0.11 ) & 0.762 ( 0.10 ) & 0.855 & & 2 & 0.758 ( 0.09 ) & 0.696 ( 0.10 ) & 0.685 ( 0.02 ) & 0.757 ( 0.15 ) & 0.784 ( 0.17 ) & 0.768 ( 0.15 ) & 0.844 3 - 10 & 210k & 1 & 0.773 ( 0.17 ) & 0.616 ( 0.01 ) & 0.666 ( 0.01 ) & 0.682 ( 0.15 ) & 0.660 ( 0.11 ) & 0.649 ( 0.11 ) & 1.000 & & 2 & 0.772 ( 0.19 ) & 0.602 ( 0.01 ) & 0.650 ( 0.01 ) & 0.675 ( 0.16 ) & 0.650 ( 0.12 ) & 0.639 ( 0.12 ) & 1.000 3 - 10 & 2100k & 1 & 0.684 ( 0.15 ) & 0.615 ( 0.03 ) & 0.644 ( 0.01 ) & 0.700 ( 0.14 ) & 0.723 ( 0.16 ) & 0.620 ( 0.01 ) & 1.000 & & 2 & 0.669 ( 0.16 ) & 0.596 ( 0.02 ) & 0.624 ( 0.01 ) & 0.689 ( 0.16 ) & 0.718 ( 0.18 ) & 0.601 ( 0.01 ) & 1.000 6SL4 & 21k & 1 & 0.902 ( 0.01 ) & 0.907 ( 0.07 ) & 0.884 ( 0.06 ) & 0.913 ( 0.01 ) & 0.956 ( 0.01 ) & 0.968 ( 0.01 ) & 0.918 & & 2 & 0.836 ( 0.01 ) & 0.890 ( 0.04 ) & 0.901 ( 0.02 ) & 0.844 ( 0.01 ) & 0.896 ( 0.01 ) & 0.911 ( 0.01 ) & 0.813 3 - 10 & 210k & 1 & 0.840 ( 0.15 ) & 0.856 ( 0.12 ) & 0.942 ( 0.08 ) & 0.934 ( 0.12 ) & 0.982 ( 0.00 ) & 0.977 ( 0.01 ) & 0.995 & & 2 & 0.836 ( 0.16 ) & 0.852 ( 0.13 ) & 0.938 ( 0.08 ) & 0.938 ( 0.12 ) & 0.993 ( 0.00 ) & 0.991 ( 0.00 ) & 0.978 3 - 10 & 2100k & 1 & 0.975 ( 0.05 ) & 0.917 ( 0.12 ) & 0.898 ( 0.10 ) & 0.905 ( 0.16 ) & 0.989 ( 0.00 ) & 0.986 ( 0.00 ) & 1.000 & & 2 & 0.981 ( 0.04 ) & 0.923 ( 0.12 ) & 0.903 ( 0.10 ) & 0.916 ( 0.16 ) & 0.995 ( 0.00 ) & 0.994 ( 0.00 ) & 1.000 6SL8 & 21k & 1 & 0.981 ( 0.02 ) & 0.976 ( 0.04 ) & 0.995 ( 0.00 ) & 0.989 ( 0.01 ) & 0.999 ( 0.00 ) & 0.999 ( 0.00 ) & 0.991 & & 2 & 0.976 ( 0.02 ) & 0.965 ( 0.03 ) & 0.983 ( 0.01 ) & 0.991 ( 0.00 ) & 0.992 ( 0.00 ) & 0.996 ( 0.00 ) & 0.966 3 - 10 & 210k & 1 & 0.931 ( 0.09 ) & 0.979 ( 0.02 ) & 0.964 ( 0.03 ) & 0.995 ( 0.01 ) & 0.998 ( 0.00 ) & 0.997 ( 0.01 ) & 0.998 & & 2 & 0.980 ( 0.04 ) & 0.998 ( 0.00 ) & 0.999 ( 0.00 ) & 0.998 ( 0.00 ) & 0.998 ( 0.00 ) & 0.997 ( 0.01 ) & 0.994 3 - 10 & 2100k & 1 & 0.909 ( 0.11 ) & 0.864 ( 0.12 ) & 0.849 ( 0.11 ) & 0.995 ( 0.01 ) & 0.997 ( 0.00 ) & 0.997 ( 0.00 ) & 1.000 & & 2 & 0.976 ( 0.05 ) & 0.986 ( 0.02 ) & 0.980 ( 0.03 ) & 0.999 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 ( 0.00 ) & 1.000 tabulartable *
CWE,common weakness enumeration,DEV-2127,table Risk Scores By CWE tab : riskscoresbycwe tabular l c r CWE - ID & OWASP T10 Risk Category & Risk Score CWE-16 & A6 - Security Misconfiguration & 6.0 CWE-524 & Not Listed & 3.0 CWE-79 & A6 - Security Misconfiguration & 6.0 CWE-425 & Not Listed & 3.0 CWE-200 & A3 - Sensitive Data Exposure & 7.0 CWE-22 & A5 - Broken Access Control & 6.0 CWE-933 & Not Listed & 3.0 tabulartablefigure polyglot_petclinic Vulnerabilities detected in the Diversified PetClinic Application fig : polyglot_petclinicfigurefigure horizontal_image_attack_surface Horizontal Attack Surface Analysisfig : horattksurfacefigureAttack Surface Analysissubsec : attacksurfaceHere we analyze the attack surfaces of the homogeneous and diversified PetClinic versions .
RV,random vaccination,DEV-2144,"tikzpicture customlegend[legend columns=4,legend style = at=(0.12,1.02),draw = none , column sep=3ex , line width=2pt , font= , legend entries = RV , AV , IMV , DV , direct , indirect ] solid , color = blue color = red color = green color = magenta color = black dashdotted , color = black customlegend tikzpicture pvan_a.pdfpvan_b.pdfpvan_c.pdfpvan_d.pdfAverage outbreak sizes at various vaccination rates ( percentage of total nodes ) of different strategies : ( A , B ) nodes are vaccinated with contacts created for direct interactions and ( C , D ) comparison of outbreak sizes for vaccinating nodes with contacts based on the direct interactions ( solid lines ) and contact based on any direct or indirect interactions ( dashed lines)-1.5emfig : avacfigurefigure[h ! ]"
MT,machine translation,DEV-2223,"The remainder of this paper is structured as follows : We first give a brief introduction of NMT , and describe the reason for the difficulty of low resource domains and languages in NMT ( Section ) ; Next , we briefly review the historical domain adaptation techniques being developed for SMT ( Section ) ; Under these background knowledge , we then present and compare the domain adaptation methods for NMT in detail ( Section ) ; After that , we introduce domain adaptation for NMT in real word scenarios , which is crucial for the practical use of MT ( Section ) ; Finally , we give our opinions of future research directions in this field ( Section ) and conclude this paper ( Section ) ."
MAT,multi - fingered adaptive tactile grasping,DEV-2365,"[ t]0.16 skip=0pt , clip , width=]images / wood/1.jpg [ t]0.16 skip=0pt , clip , width=]images / wood/2.jpg [ t]0.16 skip=0pt , clip , width=]images / wood/3.jpg [ t]0.16 skip=0pt , clip , width=]images / wood/4.jpg [ t]0.16 skip=0pt , clip , width=]images / wood/6.jpg [ t]0.16 skip=0pt , clip , width=]images / wood/7.jpg [ t]0.16 skip=0pt , clip , width=]images / icetray/1.jpg [ t]0.16 skip=0pt , clip , width=]images / icetray/3.jpg [ t]0.16 skip=0pt , clip , width=]images / icetray/4.jpg [ t]0.16 skip=0pt , clip , width=]images / icetray/5.jpg [ t]0.16 skip=0pt , clip , width=]images / icetray/6.jpg [ t]0.16 skip=0pt , clip , width=]images / icetray/7.jpg [ t]0.16 skip=0pt , clip , width=]images / icetray/8.jpg [ t]0.16 skip=0pt , clip , width=]images / icetray/9.jpg [ t]0.16 skip=0pt , clip , width=]images / icetray/10.jpg [ t]0.16 skip=0pt , clip , width=]images / icetray/11.jpg [ t]0.16 skip=0pt , clip , width=]images / icetray/12.jpg [ t]0.16 skip=0pt , clip , width=]images / icetray/14.jpg [ t]0.16 skip=0pt , clip , width=]images / black/1.jpg [ t]0.16 skip=0pt , clip , width=]images / black/2.jpg [ t]0.16 skip=0pt , clip , width=]images / black/3.jpg [ t]0.16 skip=0pt , clip , width=]images / black/4.jpg [ t]0.16 skip=0pt , clip , width=]images / black/6.jpg [ t]0.16 skip=0pt , clip , width=]images / black/7.jpg [ t]0.16 skip=0pt , clip , width=]images / tape/1.jpg [ t]0.16 skip=0pt , clip , width=]images / tape/11.jpg [ t]0.16 skip=0pt , clip , width=]images / tape/2.jpg [ t]0.16 skip=0pt , clip , width=]images / tape/3.jpg [ t]0.16 skip=0pt , clip , width=]images / tape/4.jpg [ t]0.16 skip=0pt , clip , width=]images / tape/12.jpg [ t]0.16 skip=0pt , clip , width=]images / tape/5.jpg [ t]0.16 skip=0pt , clip , width=]images / tape/6.jpg [ t]0.16 skip=0pt , clip , width=]images / tape/7.jpg [ t]0.16 skip=0pt , clip , width=]images / tape/8.jpg [ t]0.16 skip=0pt , clip , width=]images / tape/9.jpg [ t]0.16 skip=0pt , clip , width=]images / tape/10.jpg [ t]0.16 skip=0pt , clip , width=]images / bluecone/1.jpg [ t]0.16 skip=0pt , clip , width=]images / bluecone/2.jpg [ t]0.16 skip=0pt , clip , width=]images / bluecone/3.jpg [ t]0.16 skip=0pt , clip , width=]images / bluecone/5.jpg [ t]0.16 skip=0pt , clip , width=]images / bluecone/6.jpg [ t]0.16 skip=0pt , clip , width=]images / bluecone/7.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/1.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/2.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/3.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/4.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/5.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/6.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/7.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/8.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/9.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/10.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/11.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/12.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/13.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/14.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/15.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/16.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/17.jpg [ t]0.16 skip=0pt , clip , width=]images / mouse/18.jpg skip=3pt Learned Closed - Loop Behaviors of MAT ."
BR,boundary refinement,DEV-2374,"2*UNet & Step LR & 84.8603 2 - 3 & Cyclic LR & 85.9557 Dilated UNet & Cyclic LR & 84.8183 2*GCN - ResNet & Kernel & 84.0168 2 - 3 & Kernel & 81.7004 2*GCN - UNet & Kernel & 81.8659 2 - 3 & Kernel & 83.2252 3*UNet with GCN - BR & Kernel & 83.6046 2 - 3 & Kernel & 84.8266 2 - 3 & Kernel & 79.5601 UNet & Double kernel count & 84.5637 Dilated UNet & Double kernel count & 83.1449 Comparison with state - of - the - artFor comparison with the state - of - the - art results , we split our observations based on the datasets into consideration ."
CNN,convolutional neural network,DEV-2475,"max width=14.0 cm rectangle=[draw = gray , thick ] entity=[fill = gray!25,draw = gray!50,thick , text width=0.54 cm , font= , text centered ] entity2=[fill = gray!25,draw = gray!50,thick , text width=1 cm , font= , text centered ] entity4=[fill = gray!25,draw = gray!50,thick , text width=1.45 cm , font= , text centered ] entity5=[draw = gray!50,thick , text width=2.45 cm , font= , text centered ] entity3=[draw = black , fill = gray!25,semithick , text width=0.12 cm , text height=0.12 cm , font= , text centered ] cut=[circle , draw = black , fill = gray!70,semithick , inner sep=0pt , minimum size=2mm]node=[circle , draw = black , semithick , inner sep=0pt , minimum size=2 mm ] cut=[circle , draw = black , fill = gray!50,thin , inner sep=0pt , minimum size=4 mm ] node3=[circle , draw = gray!80,thick , inner sep=0pt , minimum size=3 mm ] node4=[circle , draw = black , thick , inner sep=0pt , minimum size=3 mm ] node5=[circle , draw = magenta!80!black , thick , inner sep=0pt , minimum size=3 mm ] db=[cylinder , draw = black , shape border rotate=90 , minimum height=60 , minimum width=70 , outer sep=-0.5 ] Image encoding using fine - tuned CNN : ( 1 ) We replace the last layer and transfer the parameters of the modified network to privacy data ."
TVD,total variation diminishing,DEV-2659,"After all four variants of SIMPLE - TS algorithms are tested , which are noted as follow : explicit TVD second - order scheme - approximate convective terms with explicit ( Forward Euler ) and TVD second - order schemeexplicit upwind first - order scheme - approximate convective terms with explicit ( Forward Euler ) and upwind first - order schemeimplicit TVD second - order scheme - approximate convective terms with explicit ( Backward Euler ) and TVD second - order schemeimplicit upwind first - order scheme - approximate convective terms with explicit ( Backward Euler ) and upwind first - order schemeExplicit and implicit schemes possess well - known advantages and disadvantages ."
CNN,convolutional neural network,DEV-2712,table[H ] tabularccccccc & 2c80 & 2c160 & 2c320 & GMBF & CNNf15 + 11+st & GMBF & CNNf15 + 11+st & GMBF & CNNf15 + 11+st 2 * 01 & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & 2*Average & & & & & & & & & & & & tabular Relative improvements over SRP - PHAT for the strategy invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with the sequences described in Table tab : fine - tuning - material ( columns CNNf15 + 11+st ) tab : baselineResults+ft15 + 11 + 1 + 2 + 3table
CA,contention adaptions,DEV-2927,""" @width 1pt plainFaster Concurrent Range Queries with Contention Adapting Search Trees Using Immutable DataFaster Concurrent Range Queries in CA Trees Using Immutable Data Structures [ 1]Kjell Winblad[1]Department of Information Technology , Uppsala University , Sweden kjell.winblad@it.uu.seK. WinbladKjell WinbladD.2.8 Performance measures , E.1 Trees , H.2.4 Concurrencylinearizability , concurrent data structures , treap 0 To appear in 2017 Imperial College Computing Student Workshop ( ICCSW 2017 ) 3 1 110.4230/OASIcs.xxx.yyy.p The need for scalable concurrent ordered set data structures with linearizable range query support is increasing due to the rise of multicore computers , data processing platforms and in - memory databases ."
DBN,deep belief network,DEV-2954,"arrows , positioning , patterns multi - objective net - works C - MAPSS classification hyper - parameters pre - process pre - processing ada - boost G - mean Cost - sensitive ECS - DBN Evolutionary Algorithm A Cost - Sensitive Deep Belief Network for Imbalanced ClassificationChong Zhang , Kay Chen Tan , Fellow , IEEE , Haizhou Li , Fellow , IEEE , and Geok Soon Hong C. Zhang and H. Li are with the Department of Electrical and Computer Engineering , G. S. Hong is with the Department of Mechanical Engineering , National University of Singapore , 4 Engineering Drive 3 , 117583 , Singapore ( e - mail : zhangchong@u.nus.edu ; haizhou.li@nus.edu.sg ; mpehgs@nus.edu.sg)K. C. Tan is with the Department of Computer Science , City University of Hong Kong , 83 Tat Chee Avenue , Kowloon , Hong Kong.(e - mail : kaytan@cityu.edu.hk)This paper has been accepted by IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS in April 2018 ."
PAD,presentation attack detection,DEV-2955,table[htbp ] subtable.5 tabularccccc Test & HTER ( ) & ACER ( ) & APCER ( ) & BPCER ( ) CASIA - FASD & 41.57 & 48.98 & 81.11 & 16.85 REPLAY - ATTACK & 27.61 & 34.06 & 33.96 & 34.17 3DMAD & 29.00 & 29.00 & 0.00 & 58.00 MSU - MFSD & 31.11 & 46.66 & 46.66 & 46.66 REPLAY - MOBILE & 26.89 & 28.19 & 34.37 & 22.02 HKBU & 45.00 & 45.00 & 90.0 & 0.00 OULU - NPU & 34.68 & 41.11 & 75.27 & 6.94 ROSE - YOUTU & 37.88 & 45.81 & 42.40 & 49.22 SIW & 31.97 & 48.40 & 53.07 & 43.74 CSMAD & 40.51 & 40.51 & 10.20 & 70.83 tabular Results using the Quality - Based face - PAD results : crossdataset : quality subtable subtable.5 tabularccccc Test & HTER ( ) & ACER ( ) & APCER ( ) & BPCER ( ) CASIA - FASD & 15.45 & 16.75 & 17.78 & 15.73 REPLAY - ATTACK & 25.11 & 33.35 & 31.25 & 35.44 3DMAD & 0.00 & 0.00 & 0.00 & 0.00 MSU - MFSD & 17.78 & 35.00 & 56.66 & 13.33 REPLAY - MOBILE & 18.30 & 22.99 & 23.96 & 22.02 HKBU & 0.00 & 0.00 & 0.00 & 0.00 OULU - NPU & 34.27 & 37.78 & 72.22 & 3.33 ROSE - YOUTU & 27.42 & 34.78 & 25.25 & 44.32 SIW & 9.90 & 22.06 & 30.43 & 13.69 CSMAD & 40.05 & 40.05 & 55.10 & 25.00 tabular Results using the Color - Based face - PAD results : crossdataset : color subtableResults for Cross - Dataset protocolresults : crossdatasettable
CNN,convolutional neural network,DEV-2964,"& LinearRegression & LogisticRegression & NN & CNN 4[3]*LAN ( ) & 2 * 1 & ABY3 & & & & & & This & & & & 2 - 7 & 2 * 100 & ABY3 & & & & & & This & & & & 4[3]*WAN ( ) & 2 * 1 & ABY3 & & & & & & This & & & & 2 - 7 & 2 * 100 & ABY3 & & & & & & This & & & & tabular Online Runtime of ABY3 ( Malicious ) and This for Secure Prediction of Linear , Logistic , NN , and CNN models for . ("
ECC,error correcting code,DEV-3056,tabularlcccccc & 2cTesla K40c ( ECC on ) & 2cTesla K40c ( ECC off ) & 2cGeForce GTX 1080 ( r)2 - 34 - 5 ( l)6 - 7 Method & time & rate & time & rate & time & rate Radix sort ( key - only ) & 25.99 ms & 1.29 Gkeys / s & 19.41 ms & 1.73 Gkeys / s & 9.84 ms & 3.40 Gkeys / s Radix sort ( key - value ) & 43.70 ms & 0.77 Gpairs / s & 28.60 ms & 1.17 Gpairs / s & 17.59 ms & 1.90 Gpairs / s Scan - based split ( key - only ) & 5.55 ms & 6.05 Gkeys / s & 4.91 ms & 6.84 Gkeys / s & 3.98 ms & 8.44 Gkeys / s Scan - based split ( key - value ) & 6.96 ms & 4.82 Gpairs / s & 5.97 ms & 5.62 Gpairs / s & 5.13 ms & 6.55 Gpairs / s tabular On the top : CUB 's radix sort .
LR,logistic regression,DEV-3185,"tab : suprestabularllllllllll2 * & 3cRBWH & 3cRCH & 3cGCH & P & R & F1 & P & R & F1 & P & R & F1 SVM & 0.8539 & 0.8122 & 0.8325 & 0.9366 & 0.8811 & 0.9080 & 0.9347 & 0.8810 & 0.9071 SGD & 0.8575 & 0.7329 & 0.7903 & 0.9104 & 0.8276 & 0.8670 & 0.8713 & 0.7951 & 0.8315 NB & 0.9353 & 0.7102 & 0.8074 & 0.8409 & 0.9048 & 0.8717 & 0.8049 & 0.9281 & 0.8621 RF & 0.8508 & 0.7524 & 0.7986 & 0.9182 & 0.7552 & 0.8288 & 0.8654 & 0.8210 & 0.8426 LR & 0.8872 & 0.6912 & 0.7770 & 0.7003 & 0.0725 & 0.1314 & 0.9751 & 0.5043 & 0.6648 CNN & 0.9159 & 0.9028 & 0.9085 * & 0.9370 & 0.9408 & 0.9367 * & 0.9359 & 0.9342 & 0.9335 * tabulartableSemi - supervised Learning Performancesec : semsupresTable tab : sslres presents the performance of the self - trained CNN across RBWH , RCH , and GCH ."
MPI,message passing interface,DEV-3226,sidewaystable[hp]adjustboxmax width = tabularc c c c c c c c c c c c 10r ( r)5 - 12 Cluster & Gather & File Access & Time & Serial & tabularc Comet : 24 Bridges : 24 SuperMIC : 20 tabular & tabularc Comet : 48 Bridges : 48 SuperMIC : 40 tabular & tabularc Comet : 72 Bridges : 60 SuperMIC : 80 tabular & tabularc Comet : 96 Bridges : 78 tabular & tabularc Comet : 144 Bridges : 84 SuperMIC : 160tabular & Comet : 192 & tabularc Comet : 384 SuperMIC : 320tabular Comet & MPI & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - & - & - Bridges & MPI & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - & - & - SuperMIC & MPI & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & - Comet & GA & Single & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - & - & - Comet & MPI & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - SuperMIC & MPI & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & - Comet & GA & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - SuperMIC & GA & Splitting & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & - Comet & MPI & PHDF5 & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular Bridges & MPI & PHDF5 & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & - SuperMIC & MPI & PHDF5 & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & tabularc tabular & - & tabularc tabular & - & tabularc tabular tabularadjustboxComparison of the compute and I / O scaling for different test cases and number of processes .
SM,scalar multiplication,DEV-3238,"ht]Improvements of different coordinate systems & Coordinate formula & Field [ HTML]FFFFFF & Affine & & ( , ) & [ HTML]FFFFFF & Traditional Projective & & ( , ) & [ HTML]FFFFFF & LD - Projective & & ( , ) & [ HTML]FFFFFF & & & & [ HTML]FFFFFF & -Projective & & ( , ) & [ HTML]FFFFFF & & Improved SM execution time & ( , ) & [ HTML]FFFFFF & Traditional Jacobian & 12 m ( PA ) and 4 m ( PD ) & ( , ) & [ HTML]FFFFFF & Chudnovsky Jacobian & 11 m ( PA ) and 5 m ( PD ) & ( , ) & [ HTML]FFFFFF & Modified Jacobian & 13 m ( PA ) and 4 m ( PD ) & ( , , , ) & ( 160,192,224 ) [ HTML]FFFFFF & Affine - Projective & & & [ HTML]FFFFFF & Affine - Jacobian & ) & & [ HTML]FFFFFF & & 11 m ( PA ) and 3 m ( PD ) & ( ) & [ HTML]FFFFFF & & 8 m ( PA ) and 4 m ( PD ) & & ( 224,256 ) [ HTML]FFFFFF & & 8 m ( PA ) and 4 m ( PD ) & & Efficiency Improvement Via Algorithm SimplificationThis section will show some different approaches to improving ECDSA performance ."
SM,speaker model,DEV-3320,lcccccc2*Model & 1cTeacher Forcing & 4cAutoregression & Human & Perplexity & BLEU & ROUGE-2 & DISTINCT-1/2 & NASL & EvaluationTV Series & & & & & & SM & 22.13 & 1.76 & 22.4 & 2.50/18.95 & 0.786 & 0.5566 0.0328SAM & 23.06 & 1.86 & 20.52 & 2.56/18.91 & 0.689 & 0.5375 0.0464 & 28.15 & 2.14 & 6.81 & 1.85 /6.93 & 1.135 & 0.5078 0.0382 & 30.94 & 2.41 & 14.03 & 0.66 /2.54 & 1.216 & 0.3663 0.0883 & 25.10 & 3.07 & 30.47 & 2.19 /19.02 & 1.218 & 0.6127 0.0498 & 28.19 & 2.76 & 14.68 & 0.70 /4.76 & 1.163 & 0.4284 0.0337*[t ] vs. on UDC .
FEC,forward error correction,DEV-3397,"table[!h ] Average SSIM , VQM , and network footprint center tabularlccccc & 1lSHIELD & 1lCORVETTE & 1lVaUEP & 1lVaEEP & 1lWithout FEC 6cUrban environment SSIM & 0,895 & 0,787 & 0,701 & 0,684 & 0,551 VQM & 1,459 & 2,441 & 4,034 & 4,323 & 6,688 Overhead & 17,333 & 21,778 & 46,660 & 65,984 & - 6cHighway environment SSIM & 0,911 & 0,809 & 0,744 & 0,729 & 0,627 VQM & 1,328 & 2,095 & 3,414 & 3,728 & 5,281 Overhead & 12,333 & 17,112 & 46,660 & 65,984 & - tabular tab : shield : Sumary center tableSummaryThis chapter described and assessed two proposed mechanisms to increase the video transmission resiliency over VANETs ."
CNN,convolutional neural network,DEV-3399,table[H ] tabularccccccc & 2c80 & 2c160 & 2c320 & GMBF & CNNf15 + 11+st & GMBF & CNNf15 + 11+st & GMBF & CNNf15 + 11+st 2 * 01 & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & 2*Average & & & & & & & & & & & & tabular Relative improvements over SRP - PHAT for the strategy invelasco2012-F ( columns GMBF ) ; and the CNN fine tuned with the sequences described in Table tab : fine - tuning - material ( columns CNNf15 + 11+st ) tab : baselineResults+ft15 + 11 + 1 + 2 + 3table
MF,matrix factorization,DEV-3400,"Here , we simply let the latent representation of patient be the sum of its features ' latent vectors : equationp_i = _ uf_i e_u^iequationSimilarly , the latent representation of doctor is also given by the sum of its features ' latent vectors : equationq_j = _ uf_j e_u^jequationThen , MF learns and , such that the predicted score for unobserved entries is given by the inner product of latent patient and doctor representations : equationy_ij = g(i , j p_i , q_j ) = g(p_iq_j ) equation where denotes the function that maps model parameters to the predicted score ."
CNN,convolutional neural network,DEV-3418,"tabular@cccccc@ & Layer & Filters & Filter size & Input & Output & conv3d & & & & & max3d & & & & & conv3d & & & & & max3d & & & & & conv3d & & & & & max3d & & & & & conv3d & & & & & max3d & & & & & conv3d & & & & & max3d & & & & tabular tab:3dcnn_modeltableThe parameters of all architectures we used here ( number of fully connected layers , fully connected size , CNN architectures , etc ) were experimentally found through an exhaustive series of tests ."
FA,fractional anisotropy,DEV-3508,table [ ] tabularlll 3cAmyloid Load ( PiB Positivity ) Set 1 & PiB Angular L / R & PiB Cingulum Ant L / R & PiB Cingulum Post L / R & PiB Frontal Med Orb L / R & PiB Precuneus L / R & PiB Temporal Sup L / R & PiB Temporal Mid L / R & PiB SupraMarginal L Set 2 & FA Cerebral peduncle R & FA Cerebral peduncle L & MD Corticospinal tract R & MD Corticospinal tract L & Trail - Making Test Part A Score & MD Cerebral peduncle R & PET Cingulum Post R & tabular Group difference across Amyloid Load ( PiB Positivity ) tab : wrapPIBtableC ) Graph Scan Statistics on slope differences across amyloid load positivity .
FEC,forward error correction,DEV-3795,The parameters are defined as follows : FEC - based : accounts for mechanisms that employ FEC ; ARQ - based : mark mechanisms that use ARQ ; QoE - sensitive data : this parameter demonstrates mechanisms that identity and/or considerate the video content to define the EC policy ; Video - aware : check mark is given to mechanisms that use any video characteristics to define the amount of redundancy and/or retransmission ; High - quality video : it is marked if the mechanisms are using videos equal or higher than 720p ( HD ready ) ; Network status : this parameter defines if the mechanisms use the information about the network healthy to define the redundant data ; UEP - enabled : means that different amounts of redundancy are being added to distinct portions of the video .
US,uncertainty sampling,DEV-3968,fig : AAAI_first_pageminipageminipage.48 0.85 tabular[b]cccccc & 3cProposed & 2cClassic & EDG & ext1 & ext2 & US ( + Div ) & Rnd / Div Intepretable & Gray & Gray & 2*No & 2*No & Gray Strategy & Gray-2*Yes & Gray-2*Yes & & & Gray-2*Yes Robustness & Gray & GrayL & GrayL & 2*Low & Gray to Noise & Gray-2*High & GrayL-2*Med & GrayL-2*Med & & Gray-2*High Output & GrayL & GrayL & 2*Prob & 2*Prob & Gray Required & GrayL-2*Pred & GrayL-2*Pred & & & Gray-2*None Validation & 2*Yes & Gray & Gray & Gray & Gray Required & & Gray-2*No & Gray-2*No & Gray-2*No & Gray-2*No Label Cost & GrayL & GrayL & Gray & Gray & 2*Low Reduction & GrayL-2*Med & GrayL-2*Med & Gray-2*High & Gray-2*High & tabular tableComparison between different sampling approaches .
FDA,fisher 's discriminant analysis,DEV-3980,"R1IR = [ 90.8 ] & & & & & & EO & & & & R1IR = [ 85.6 ] 2cTask & 9lEC : resting state with eyes closed , EO : resting state with eyes open , MI : motor imagery , ERP : event related potential 2c2*Classifier & 9lANN : artificial neural networks , FDA : Fisher discriminant analysis , KNN : k - nearest neighbours , LDA : linear discriminant analysis & & 9lMAP : maximum a posteriori , CC : cross correlation , L1 ( Manhattan ) distance , L2 ( Euclidean ) distance , cosine distance tabular table*Previous protocols sec : previous_protocolTable table : comparison summarises the state - of - the - art of the existing EEG biometrics applications based on multiple data acquisition days ."
MAT,multi - fingered adaptive tactile grasping,DEV-4158,"Significance of Calibration Noise Experimental ResultsWhile the experimental results demonstrate the robustness of MAT under various levels of calibration noises , it is important to point out that the calibration noise experiments reflect the same types of symptoms caused by other similarly common challenges in grasping , such as : suboptimality in the grasp pose generated by a learning or planning method partial observability of the 3D point cloud under a monocular camera setup inaccurate object pose prediction by an object pose estimation algorithm perceptual difficulties dealing with transparent or reflective objects performance degradation caused by low - fidelity sim - to - real transfer unexpected object pose disturbanceGrasping in Simulation under Calibration NoiseTable shows the grasp success rate of MAT compared to a strong vision baseline , at varying calibration noises ."
CNN,convolutional neural network,DEV-4459,"table[H ] tabularccccccc & 2c80 & 2c160 & 2c320 & CNNt15 & CNNf15 & CNNt15 & CNNf15 & CNNt15 & CNNf15 2 * 01 & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & 2*Average & & & & & & & & & & & & tabular Results for the CNN proposal , either trained from scratch with sequence 15 ( columns CNNt15 ) or fine tuned with sequence 15 ( columns CNNf15 ) ."
GCN,graph convolution networks,DEV-4487,"For evaluating structure fusion generalization , we compare structure fusion based graph convolutional networks ( SF - GCN ) , propagation fusion based graph convolutional networks ( PF - GCN ) and structure propagation fusion based graph convolutional networks ( SPF - GCN).In Table , we observe that the performance of SPF - GCN is better than that of other method , and the least improvement of SPF - GCN respectively is for Cora , for Citeseer and for PubMed , while the performance of SP is superior to that of PF - GCN , and the improvement of SF - GCN respectively is for Cora , for Citeseer and for PubMed Therefore , PF and SF both are benefit for further mining the structure information and the role of SF is more important than that of PF ."
LM,language model,DEV-4791,"W1 & W2 & W3 & W[1,2,3 ] & & Pre - trained & Google & 56.1 & 60.2 & 62.2 & 24.5 & 60.2 & Task - specific & Google & 53.8 & 55.3 & 56.1 & 55.0 Experiments on the type of model for inter - sentential relations Local Model ( Linear ) & Task - specific & Google & 24.0 & 22.6 & 32.7 & 21.0 Local Model ( Bilinear as the final layer ) & Task - specific & Google & 59.5 & 53.7 & 60.1 & 60.5 Local Model ( Bilinear ) & Task - specific & Google & 74.0 & 66.6 & 74.4 & 78.3 Local Model ( Bilinear ) & Task - specific & Google - Tasnim & 73.3 & 66.2 & 73.2 & 78.0 Local Model ( Bilinear ) & Task - specific & ELMo & 74.1 & 65.7 & 73.6 & 78.2 Local Model ( Bilinear ) & Task - specific & BERT & 76.6 & 66.3 & 76.5 & 83.5 Global Model ( LConv ) & Task - specific & Google & 57.1 & 53.3 & 56.5 & 59.6 Local + Global Model ( LConv ) & Task - specific & Google & 74.6 & 65.0 & 75.2 & 79.6 Local + Global Model ( LConv ) & Task - specific & ELMo & 76.0 & 64.6 & - & - Experiments on the type of model for sentence grammar ( + LM loss ) Local Model ( ELMo - style LM ) & Pre - trained & Google & 75.2 & 67.2 & 75.9 & 80.1 Local Model ( ELMo - style LM ) & Pre - trained & ELMo & 76.1 & 64.2 & 75.9 & 80.6 Full Model ( Our ) & Pre - trained & Google & ? ?"
CC,classifier chain,DEV-4803,"fig_gramar4figurefigure[!htbp ] Verbatim[frame = single , samepage = true ] < ALGS - PT > : : = < ALGS - PT1 > < ALGS - PT2 > < ALGS - PT3 > < ALGS - PT4 > < ALGS - PT1 > : : = BR CC LP # BR='Binary Relevance ' , CC='Classifier Chain ' # LC='Label Powerset'<ALGS - PT2 > : : = ( BRq CCq ) < dsr > < ComplexCC_Trellis > # BRq and CCq = ' quick versions for BR and CC ' FW RT < LP_based > # FW='Four - class pairWise ' , RT='Ranking - Threshold'<ALGS - PT3 > : : = BCC < dp_complete > # BCC='Bayesian Classifier Chain'<ALGS - PT4 > : : = PMCC < B > < ts > < ii > < chi_PMCC > < ps > < pof > # PMCC='Population of Monte - Carlo Classifier Chains'<dsr > : : = RANDOM - REAL(0.2 , 0.8 ) # dsr='down - sample ratio'<ComplexCC_Trellis > : : = PCC ( MCC < chi_MCC > < CT > ) < ii > < pof > ( CDN < CDT > ) < i_cdn_cdt > < ci > # PCC='Probabilistic Classifier Chains ' # MCC='Monte - Carlo Classifier Chains ' # CT='Classication Trellis ' # CDN='Conditional Dependency Networks ' # CDT='Conditional Dependency Trellis ' < chi_MCC>::= < chi_CT > 0 # chi_MCC='nmber of chain iterations for MCC'<ii > : : = RANDOM - INT(2 , 100 ) # ii='number of inference interations'<pof > : : = Accuracy Jaccard index Hamming score Exact match Jaccard distance Rank loss Hamming loss Zero One loss Harmonic score Log Loss lim : L Micro Recall One error Log Loss lim : D Micro Precision Macro Precision Macro Recall F1 micro averaged Avg precision F1 macro averaged by example F1 macro averaged by label AUPRC macro averaged AUROC macro averaged Levenshtein distance # pof='Payoff function'<CT > : : = < chi_CT > < w > < dp><dp > : : = C I Ib Ibf H Hbf X F None # dp='dependency type'<chi_CT > : : = RANDOM - INT(2 , 1500 ) # chi_CT='number of chain iterations for CT'<w > : : = 0 1 -1 < d > # w='width of the trellis'<d > : : = RANDOM - INT(1 , SQRT(L ) +1 ) # d='neighborhood density ' # Where L is the number of labels < CDT > : : = < w > < dp > # parameters defined earlier < i_cdn_cdt > : : = RANDOM - INT(101 , 1000 ) # i_cdn_cdt='total number of iterations'<ci > : : = RANDOM - INT(1 , 100 ) # ci='collection iterations'<LP_based > : : = ( PS PSt < RAkEL - based > ) < sv > < pv > # PS='Pruned Sets ' # PSt='Pruned Sets with Threshold'<sv > : : = RANDOM - INT(0 , 5 ) # sv='subsampling value ' < pv > : : = RANDOM - INT(1 , 5 ) # pv='pruning value ' < RAkEL - based > : : = ( RAkEL < sre > RAkELd ) < les > # RAkEL='RAndom k - labEL Pruned Sets ' # RAkELd='RAndom k - labEL Disjoint Pruned Sets'<sre > : : = RANDOM - INT(2 , min(2L , 100 ) ) # sre='number of subsets to run in an ensemble'<les > : : = RANDOM - INT(1 , L/2 ) # les='number of labels in each label subset ' # Where L is the number of labels < dp_complete > : : = < dp > LEAD # dp='complete dependency type for BCC ' < B > : : = RANDOM - REAL(0.01 , 0.99 ) # B='Beta factor for deacreasing the temperature'<ts > : : = 0 1 # ts='Temperature switch'<ps > : : = RANDOM - INT(1 , 50 ) # ps='population size'<chi_PMCC > : : = RANDOM - INT(51 , 1500 ) # chi_PMCC='number of chain iterations for PMCC'Verbatim 1 mm Defined Grammar - Part 5 : MLC Problem Transformation Methods ."
SNP,single nucleotide polymorphisms,DEV-4824,"Broadly speaking , our simulation study is founded around a core set of pure 2-way interaction SNP datasets similar to those previously benchmarked , but we expand beyond these to include groups of SNP datasets with ( 1 ) a variety of simple main effects , ( 2 ) 3-way interactions , ( 3 ) genetic heterogeneity , ( 4 ) continuous - valued features , ( 5 ) a mix of discrete and continuous features , ( 6 ) multi - class endpoints , ( 7 ) continuous endpoints , ( 8) missing data , and ( 9 ) imbalanced data ."
MRE,median recovery error,DEV-4827,"WARNING : use overfitting instead of only ' memorization ' which is more ambiguous , ' verbatim ' seems to strongToDo list : look at distribution of attributes in the latent spaceIf we have time : Do a figure with auto - encoder ( if we manage to sample the latent space , for instance by fitting a Gaussian to the encoded images in the latent space)Show a picture of a few examples of Eiffel tower or london bridge in LSUN that seem to be memorize but are just samples learn from templateadd appendix with extra experiments ( a lot of images of recovery)show ( at least on some images ) that VGG , perceptual loss , L1 or L1 on Laplacian pyramid gives the same results for recovery because GLO reported some mitigated results on this ( sometimes L2 is better , sometimes pyramid : in the end , they mixed the two metrics)give statistics about the recovery precision with LBFGS : for a given * generated * target image , the distribution of errors using a lot of random initializations ( to demonstrate that the problem is almost convex , or at least ' easy ' to optimize ) show convergence speed vs SGD or other optimization method ( because it is was is generaly used in the literature : this finding makes it possible to experiment more easily ) , for a given distorted target generated image , the average / median error vs the distorsion : : you did that on training image but it would be nice to show it first on generated images , to show robustnessdiscuss difference LBFGS vs SGD : LBFGS is much faster to converge , not prone to gradient step setting , but can suffer from instabilitiesadd experiments on Auto Encoder : visual recovery results , histograms , MRE values , and maybe FID if we have time ( sampling from the latent space)add a small paragraph to explain experimental settings , i.e. that we reproduce several architectures from the literature and trained again on splitting , ... tell somewhere the difference ( I think in the intro ) between over - fitting and verbatim memorization , for which we provide a solid definition : add more experiments on LSUN : show histograms with PG - GAN without GAP , show recovery failure resultsadd experiments on MNIST , CIFAR ?"
PSO,power system operations,DEV-4857,"PSO algorithm A typical PSO algorithm is given for completeness as discussed in as follows Set , , and parameters Initialize positions and velocities of each particle of the population Evaluate particles fitness i.e. , and find the index of the best particle Select and Set iteration count Update velocity and position of particles and and Evaluate the fitness of updated particles i.e. , and find the index of the best particle at this iteration Update Pbest of each particle of the population If then else Update Gbest of the population If then and set else If then go to step 12 else and go to step 6 Optimum solution is obtained as Fig ."
RDF,resource description framework,DEV-4963,table[htbp]RDFS Entailment rules fromHorst tab : entail2 tabularcllR & If RDF graph contains & Then ( ) ( 2 ) & & rdf : type & rdfs : domain & ( 3 ) & & rdf : type & rdfs : range & ( 7 ) & & & rdfs : subPropertyOf & ( 5 ) & rdfs : subPropertyOf & rdfs : subPropertyOf & rdfs : subPropertyOf & ( 9 ) & rdf : type & rdf : type & rdfs : subClassOf & ( 11 ) & rdfs : subClassOf & rdfs : subClassOf & rdfs : subClassOf & tabulartablefigure[thbp][Step 1 : rdfs : subClassOf ] entailex1a.jpg[Step2 : rdfs : subClassOf ] entailex1b.jpg[Combine results : rdfs : subClassOf ] entailex1c.jpgRule 11 's example .
CNN,convolutional neural network,DEV-5273,"table[H ] tabularcccccccccc & 3c80 & 3c160 & 3c320 & SRP & CNNt15 & CNNf15 & SRP & CNNt15 & CNNf15 & SRP & CNNt15 & CNNf15 2 * 01 & & & & & & & & & & & & & & & & & & 2 * 02 & & & & & & & & & & & & & & & & & & 2 * 03 & & & & & & & & & & & & & & & & & & 2*Average & & & & & & & & & & & & & & & & & & tabular Results for the SRP - PHAT strategy ( columns SRP ) and the CNN , either trained from scratch with sequence 15 ( columns CNNt15 ) or fine tuned with sequence 15 ( columns CNNf15 ) ."
BR,binary relevance,DEV-5317,"fig_gramar4figurefigure[!htbp ] Verbatim[frame = single , samepage = true ] < ALGS - PT > : : = < ALGS - PT1 > < ALGS - PT2 > < ALGS - PT3 > < ALGS - PT4 > < ALGS - PT1 > : : = BR CC LP # BR='Binary Relevance ' , CC='Classifier Chain ' # LC='Label Powerset'<ALGS - PT2 > : : = ( BRq CCq ) < dsr > < ComplexCC_Trellis > # BRq and CCq = ' quick versions for BR and CC ' FW RT < LP_based > # FW='Four - class pairWise ' , RT='Ranking - Threshold'<ALGS - PT3 > : : = BCC < dp_complete > # BCC='Bayesian Classifier Chain'<ALGS - PT4 > : : = PMCC < B > < ts > < ii > < chi_PMCC > < ps > < pof > # PMCC='Population of Monte - Carlo Classifier Chains'<dsr > : : = RANDOM - REAL(0.2 , 0.8 ) # dsr='down - sample ratio'<ComplexCC_Trellis > : : = PCC ( MCC < chi_MCC > < CT > ) < ii > < pof > ( CDN < CDT > ) < i_cdn_cdt > < ci > # PCC='Probabilistic Classifier Chains ' # MCC='Monte - Carlo Classifier Chains ' # CT='Classication Trellis ' # CDN='Conditional Dependency Networks ' # CDT='Conditional Dependency Trellis ' < chi_MCC>::= < chi_CT > 0 # chi_MCC='nmber of chain iterations for MCC'<ii > : : = RANDOM - INT(2 , 100 ) # ii='number of inference interations'<pof > : : = Accuracy Jaccard index Hamming score Exact match Jaccard distance Rank loss Hamming loss Zero One loss Harmonic score Log Loss lim : L Micro Recall One error Log Loss lim : D Micro Precision Macro Precision Macro Recall F1 micro averaged Avg precision F1 macro averaged by example F1 macro averaged by label AUPRC macro averaged AUROC macro averaged Levenshtein distance # pof='Payoff function'<CT > : : = < chi_CT > < w > < dp><dp > : : = C I Ib Ibf H Hbf X F None # dp='dependency type'<chi_CT > : : = RANDOM - INT(2 , 1500 ) # chi_CT='number of chain iterations for CT'<w > : : = 0 1 -1 < d > # w='width of the trellis'<d > : : = RANDOM - INT(1 , SQRT(L ) +1 ) # d='neighborhood density ' # Where L is the number of labels < CDT > : : = < w > < dp > # parameters defined earlier < i_cdn_cdt > : : = RANDOM - INT(101 , 1000 ) # i_cdn_cdt='total number of iterations'<ci > : : = RANDOM - INT(1 , 100 ) # ci='collection iterations'<LP_based > : : = ( PS PSt < RAkEL - based > ) < sv > < pv > # PS='Pruned Sets ' # PSt='Pruned Sets with Threshold'<sv > : : = RANDOM - INT(0 , 5 ) # sv='subsampling value ' < pv > : : = RANDOM - INT(1 , 5 ) # pv='pruning value ' < RAkEL - based > : : = ( RAkEL < sre > RAkELd ) < les > # RAkEL='RAndom k - labEL Pruned Sets ' # RAkELd='RAndom k - labEL Disjoint Pruned Sets'<sre > : : = RANDOM - INT(2 , min(2L , 100 ) ) # sre='number of subsets to run in an ensemble'<les > : : = RANDOM - INT(1 , L/2 ) # les='number of labels in each label subset ' # Where L is the number of labels < dp_complete > : : = < dp > LEAD # dp='complete dependency type for BCC ' < B > : : = RANDOM - REAL(0.01 , 0.99 ) # B='Beta factor for deacreasing the temperature'<ts > : : = 0 1 # ts='Temperature switch'<ps > : : = RANDOM - INT(1 , 50 ) # ps='population size'<chi_PMCC > : : = RANDOM - INT(51 , 1500 ) # chi_PMCC='number of chain iterations for PMCC'Verbatim 1 mm Defined Grammar - Part 5 : MLC Problem Transformation Methods ."
CIA,"confidentiality , integrity , and availability",DEV-5333,"Major abbreviations 8.7cmSlX mygray Abbreviation & mygray Description ACA - A & Alternative Ascending Clock Auction ACA - T & Traditional Ascending Clock Auction CA / CDF & Cryptographic Authority / Cumulative Distribution Function CIA & Confidentiality , Integrity and Availability triad CRN & Cognitive Ratio Network DoS / DDoS & Denial - of - Service / Distributed Denial - of - Service EBV&Encrypted Bit Vector FJ & Friendly Jamming HMAC&Hash Message Authentication Code KKT & Karush - Kuhn - Tucker LSA & Licensed Shared Access MANET & Mobile Ad hoc NETwork NUM & Network Utility Maximization OPE & Order Preserving Encryption PD / PU / SU&Primary Destination / Primary User / Secondary User SINR & Signal - to - Interference - plus - Noise Ratio SSDF & Spectrum Sensing Data Falsification TLC / TTP & Time Lapse Cryptography / Trusted Third Party VCG / GSP & Vickrey - Clarke - Groves / Generalized Second - Price auction Overview of wireless security issuesWireless networks play an extremely important role in many applications ."
ODE,ordinary differential equation,DEV-5336,"( C2 ) If P1 uses CRL1 with a rate of learning faster than P2 who learns with RL2 , then the ODE is given by(Explicit Solutions of Smooth BR Equation ) : Given P2 's trajectory and an initial condition the smooth best response equation in ( ) has a unique solution given by the vectorial functionwhere In particular , if P2 is a slow learner i.e. , constant in time , then the smooth best response equation of P1 converges to which goes to when ( Explicit Solutions of Replicator Equation ) : Given P2 's trajectory and an interior initial condition the replicator equation in ( ) has a unique solution given by the vectorial function , with a normalization factor In particular , if P2 is a slow learner , i.e. , constant in time , then the replicator equation of P1 converges toNote that these solutions are in the interior of the simplex for finite , but the trajectory can be arbitrarily close to the boundary when goes to infinity ."
TP,temporal pooler,DEV-5366,"tabular l l Variable & Description & Length of the past & Length of the lookbehind part & ( past + current step ) & Length of the future & Length of the whole sequence & Index of a layer & Index of an Expert in the layer & Set of cluster centers ( states ) & of an Expert & Dimension of , number of & cluster centers & Sequence of complete observations & Sequence of observations of & Expert in layer & Hidden state of the Expert in layer & Output vector of the Expert in layer & Number of sequences considered in a TP & Set of all providers of context to & an Expert & Likelihoods of seeing each context & element from each provider in each & position of each sequence tabulart : notationSelected notation ."
RF,random forest,DEV-5484,"tab : suprestabularllllllllll2 * & 3cRBWH & 3cRCH & 3cGCH & P & R & F1 & P & R & F1 & P & R & F1 SVM & 0.8539 & 0.8122 & 0.8325 & 0.9366 & 0.8811 & 0.9080 & 0.9347 & 0.8810 & 0.9071 SGD & 0.8575 & 0.7329 & 0.7903 & 0.9104 & 0.8276 & 0.8670 & 0.8713 & 0.7951 & 0.8315 NB & 0.9353 & 0.7102 & 0.8074 & 0.8409 & 0.9048 & 0.8717 & 0.8049 & 0.9281 & 0.8621 RF & 0.8508 & 0.7524 & 0.7986 & 0.9182 & 0.7552 & 0.8288 & 0.8654 & 0.8210 & 0.8426 LR & 0.8872 & 0.6912 & 0.7770 & 0.7003 & 0.0725 & 0.1314 & 0.9751 & 0.5043 & 0.6648 CNN & 0.9159 & 0.9028 & 0.9085 * & 0.9370 & 0.9408 & 0.9367 * & 0.9359 & 0.9342 & 0.9335 * tabulartableSemi - supervised Learning Performancesec : semsupresTable tab : sslres presents the performance of the self - trained CNN across RBWH , RCH , and GCH ."
MI,mutual information,DEV-5566,"The transmitters send power - normalized versions of the estimation error , i.e. , aswhere is a power scaling factor , , and is a modulation coefficient chosen asThe user 's correlation coefficients can be written as , In a symmetric Gaussian MAC i.e. , and for all , the sum - capacity is achievable using F - MEC coding strategy , if satisfies and is the unique solution ofFeedback Effect Analysis of the K - user Symmetric Gaussian MACIn the following , utilizing the derivative of the MI we show that how F - MEC code affects the information rate of a K - user symmetric Gaussian MAC ."
AV,acquaintance vaccination,DEV-5570,"List of SymbolsList of abbreviationslongtablecc Acronym & Descriptions ADN & Activity driven network modelling AV & Acquaintance vaccination APV & Absolute percentage variation CIP & co - location interaction parameters CN & Common neighbours DST network & Dense SPST network DDT network & Dense SPDT network DDT1 & Vaccinating neighbours in DDT network with direct linksDDT2 & Vaccinating neighbours in DDT network with any linksGDT & Generated SPDT network with 364 K nodes GST & Generated SPST network with 364 K nodes IMV & Individual movement based vaccination strategy IMVE & Individual movement based vaccination strategy with exact information IMVT & Individual movement based vaccination strategy with temporal information LST & SPDT network with the same number of links that of DDT network LST & SPST network with the same number of links that of DST network MLE & Maximum likelihood estimator OSN & Online social network PFU & Plaque - forming unit RV & Random vaccination DV & Degree vaccination RSE & Root squared error SPST & Same place same time transmission SPDT & Same place different time transmission SIR & Susceptible - infected - recovered SDT network & Sparse SPDT network of links having direct and indirect components SST network & Sparse SPST network of links having indirect component onlySPDT graph & graph based on SPDT diffusion longtableList of symbolslongtablecc Symbol & Descriptions A & Set of active copies of nodes in SPDT graph b & active particle decay rates from an area of interaction & activity potential of node & active periods of a node C & Particle concentration in interaction area C , C , C & scenario 1 , 2 and 3 d & Activation degree - number of SPDT links created during an activation E & Intake dose or exposure of infectious particles & Average volume fraction of room air introduced by exhaled breath f & distribution function F & Disease spreading force in the network at the current day of simulation & Average disease spreading force in the network & Graph & Dynamic graph g & Particle generation rate by an infected individual h & Activation frequency I & Number of infected individuals in the system & Number of infected individuals at a simulation day & Number of infected individuals in the system at the current time that disease prevalence & Number of infected individuals up to a simulation day L & links set N & Total number of individuals , nodes , users p & Pulmonary rate of susceptible individual & Infection probability for an intake dose & Probability of creating a link during an activation & Probability of breaking a created link Q & air exchange rate from an area q & Transition probability for changing inactive to active state R & disease reproduction ability of an infected individual r & Particle removal rate from interaction area r & Median of particles removal rates S & Number of susceptible individual T & Simulation period or disease observation period & Activation period or period host user or node stays at the interacted location & Link creation delay or delay neighbour user or node arrives at the interacted location & Stay duration of user or node stays at the interacted location V & Air volume of interaction area , , & waiting periods of a node Y & Labelling sets in graph X & updates z & Number of time step Z & set of nodes in the SPDT graph & power law exponent & Infection rate at the current day of simulation in the network for an infected individual & Indirect transmission period & central tendency & Scaling parameter of activation degree distribution & neighbour proportion & average volume fraction of room air that is exhaled by an susceptible individual & links presence function & state probability & nodes presence function & switching probability form active to inactive states & Infectiousness of infection particles & Fraction of dose or exposure reaches to the target infection site & Duration that virus is generated or infectious period of infected individual & inter - event time for node in activity driven networks & lower limit of active degree distribution & activation potential in ADN networks & particle accumulation rate & Transition probability for changing active to inactive state longtable"
RDF,resource description framework,DEV-5602,"However , there are also some differences between them : An RDF graph contains nodes of type resource ( whose label is an IRI ) and nodes of type Literal ( whose label is a value ) , whereas a PG allows a single type of node ; Each node or edge in an RDF graph contains just a single value ( i.e. a label ) , whereas each node or edge in a PG could contain multiple labels and properties respectively;An RDF graph supports multi - value properties , whereas a PG usually just support mono - value properties;An RDF graph allows to have edges between edges , a feature which is n't supported in a PG ( by definition);A node in an RDF graph could be associated with zero or more classes or resources , while a node in a PG usually has a single node type ."
LC,least confidence,DEV-5681,tab : measurescentertabletable*[t]tabularlcccccccccDsets & Chance & FTZ Ent - Ent & FTZ Ent - LC & MNB Ent - Ent & MNB Ent - LC & FTZ Ent - Ent & FTZ Ent - LC & MNB Ent - Ent & MNB Ent - LC SGN & & & & & & & & & DBP & & & & & & & & & YHA & & & & & & & & & YRP & & & & & & & & & YRF & & & & & & & & & AGN & & & & ? &
ML,machine learning,DEV-5686,"Rather , they should demand to the ML - DSS designers ( and their advocates ) evidence - based validations of their systems , and adopt them only once some further information has been given about , e.g. , the size of the diagnostic improvement detected , the trade - off between specificity ( avoiding false positive , i.e. , overtesting and overtreatment ) and sensitivity ( avoiding false negatives , i.e. , failing to treat and cure ) ; between the internal ( i.e. , bias ) and external ( i.e. , variance ) validity of the model ( regarding also the extent the ML - DSS could fit multimorbid cases , instead of being excessively specialized for one disease ) ; and between its prediction power and its interpretability , that is its scrutability by doctors and lay users to understand why the ML - DSS has suggested them a certain decision over possible others and make the "" hybrid "" agency of man - and - machine more accountable towards the colleagues , the patients and their dears ."
HD,hausdorff distance,DEV-5973,"C > p6.2emg>[RGB]195 , 195 , 195Ck > GrayC*[htbp ] The SE ( ) , AE ( ) and HD ( ) calculated using the results of different methods ( Dufour 's method , OCTRMIA3D and GDM ) and the ground truth manual segmentation , for the IS - OS ( ) surface in each of the 10 OCT volumesC > p6.2emg>[RGB]195 , 195 , 195Ck > GrayC*[htbp ] The OSE ( ) , OAE ( ) and OHD ( ) calculated using the results of different methods ( Dufour 's method , OCTRMIA3D and GDM ) and the ground truth manual segmentation , for overall retina surfaces in each of the 10 OCT volumes[h ! ]"
CNN,convolutional neural network,DEV-5998,"Compared to all the aforementioned works , we propose an easy to train CNN model , which do not require a lot of images in the training dataset , with combination of transfer learning ( Learning achieved by taking the convolutional base of a pre - trained network , running the new data of 4 traffic categories through it and training a new randomly initialized classifier ) and continuous learning ( Learning achieved by re - training the classifier with wrong predictions till operating period of the system ) capabilities on SOPC without the need of communicating the traffic images to the connected server for further analysis ."
FTE,foveal tilt effects,DEV-6107,"the edge maps at fine to medium scales with the overlayed Hough lines are shown in [ fig : B5]Figs to [ fig : B7]. We show the mean tilt angles measured in the DoG edge maps across multiple scales in [ appendix : AppxC]Appendix C.figuresectionC. Quantitative mean tiltsThe absolute mean tilts and the standard errors of detected tilt angles for the Cafe Wall variations tested are provided here in Figs [ fig:]Figs and [ fig : C2]. For the ' foveal tilt effect ' ( FTE , explained in [ sec:3.1.1]Sections and [ sec:3.1.2]Section ) , we used the near horizontal mean tilts at scale 4 , and reflected these values to [ fig:6]Fig ."
